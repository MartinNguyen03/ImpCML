{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using neural networks to predict on Kryptonite-N dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":4096:8\n",
      "expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "myseed = 180\n",
    "os.environ['PYTHONHASHSEED'] = str(myseed)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]= \":4096:8\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]= \"expandable_segments:True\"\n",
    "print(os.getenv(\"CUBLAS_WORKSPACE_CONFIG\"))\n",
    "print(os.getenv(\"PYTORCH_CUDA_ALLOC_CONF\"))\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence, plot_gaussian_process\n",
    "import time\n",
    "from codecarbon import EmissionsTracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {0}'.format(device))\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "\n",
    "def set_seeds(myseed):\n",
    "\n",
    "    random.seed(myseed)\n",
    "    np.random.seed(myseed)\n",
    "    torch.manual_seed(myseed)\n",
    "    torch.cuda.manual_seed(myseed)\n",
    "    torch.cuda.manual_seed_all(myseed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "set_seeds(myseed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, depth):\n",
    "        super().__init__()\n",
    "        # self.linear_layer_stack = nn.Sequential(\n",
    "        #     nn.Linear(input_size, hidden_size),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(hidden_size,hidden_size), \n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(hidden_size,1), \n",
    "        # )\n",
    "\n",
    "\n",
    "\n",
    "        modules = [torch.nn.Linear(input_size,hidden_size), torch.nn.LeakyReLU()]\n",
    "\n",
    "        for i in range(depth-1):\n",
    "            modules.append(torch.nn.Linear(hidden_size,hidden_size))\n",
    "            modules.append(torch.nn.LeakyReLU())\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.hidden_layer_stack = torch.nn.Sequential(\n",
    "            *modules,\n",
    "        )\n",
    "        \n",
    "        set_seeds(myseed)\n",
    "        self._initialize_weights()  \n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.hidden_layer_stack:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight)\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer_stack(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to validate model performance\n",
    "def validate_nn(model, X_val, y_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val.to(device))\n",
    "        val_outputs = torch.round(torch.sigmoid(val_outputs)).cpu().numpy()\n",
    "        accuracy = accuracy_score(y_val, val_outputs)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_nn(model, dataloader, criterion, optimizer, epochs, X_val, y_val):\n",
    "    start= time.perf_counter()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.unsqueeze(1).to(device)  # Move data to device\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}')\n",
    "        # Function\n",
    "        # eoe_time = time.perf_counter()\n",
    "        # if eoe_time-start > 150:\n",
    "        #     return epoch+1\n",
    "            \n",
    "        \n",
    "        \n",
    "        if (epoch+1)%10 == 0:\n",
    "            validation_acc = validate_nn(model,X_val, y_val)\n",
    "            print(f\"Validation Accuracy: {validation_acc}\")\n",
    "\n",
    "        # if epoch%10 == 0:\n",
    "        #     validate_nn(model, )\n",
    "\n",
    "    return epochs\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "Device: cuda\n",
      "(48000, 24)\n",
      "(250000, 24)\n",
      "(298000, 24)\n",
      "torch.Size([238400, 24])\n",
      "tensor([-0.0658,  1.1866, -0.0287,  0.0842,  0.9765,  0.9252,  1.0144, -0.0635,\n",
      "         1.1466,  0.0446,  0.0054,  0.0184, -0.0242,  0.0449,  1.0113, -0.0181,\n",
      "         0.8241,  1.0257,  0.9992,  1.0118,  0.0436,  0.0483, -0.1741,  1.0206])\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\"\n",
    "print('Device: {0}'.format(device))\n",
    "\n",
    "# setting kryptonite dataset no.\n",
    "n = 24\n",
    "\n",
    "# Reading and normalising dataset features for efficient convergence\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = np.load('Datasets/kryptonite-%s-X.npy'%(n))\n",
    "y = np.load('Datasets/kryptonite-%s-y.npy'%(n))\n",
    "\n",
    "# print(f\"X shape: {X.shape()}\")\n",
    "print(np.shape(X))\n",
    "\n",
    "if n>18:\n",
    "    X_add = np.load('Datasets/additional-kryptonite-%s-X.npy'%(n))\n",
    "    print(np.shape(X_add))\n",
    "    y_add = np.load('Datasets/additional-kryptonite-%s-y.npy'%(n))\n",
    "\n",
    "    X=  np.concatenate((X,X_add),axis = 0)\n",
    "    y=  np.concatenate((y,y_add),axis = 0)\n",
    "\n",
    "\n",
    "print(np.shape(X))\n",
    "\n",
    "\n",
    "# print((X[0]))\n",
    "\n",
    "# # Shuffle and split the data\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=myseed)  # 60% training\n",
    "\n",
    "# scaler.fit(X_train)\n",
    "# scaler.transform(X_train)\n",
    "# scaler.transform(X_temp)\n",
    "\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=myseed)  # 20% validation, 20% test\n",
    "\n",
    "set_seeds(myseed)\n",
    "\n",
    "# Shuffle and split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=myseed)  # 80% training-validation/20% testing\n",
    "\n",
    "scaler.fit(X_train)\n",
    "scaler.transform(X_train)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=myseed)  # 20% validation, 20% test\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "print((X_train.size()))\n",
    "# print((X_val[0]))\n",
    "print((X_test[0]))\n",
    "\n",
    "\n",
    "\n",
    "best_accuracy = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/86, Loss: 0.8378\n",
      "Epoch 2/86, Loss: 0.6933\n",
      "Epoch 3/86, Loss: 0.6932\n",
      "Epoch 4/86, Loss: 0.6933\n",
      "Epoch 5/86, Loss: 0.6933\n",
      "Epoch 6/86, Loss: 0.6932\n",
      "Epoch 7/86, Loss: 0.6931\n",
      "Epoch 8/86, Loss: 0.6930\n",
      "Epoch 9/86, Loss: 0.6930\n",
      "Epoch 10/86, Loss: 0.6928\n",
      "Validation Accuracy: 0.5014597315436241\n",
      "Epoch 11/86, Loss: 0.6929\n",
      "Epoch 12/86, Loss: 0.6927\n",
      "Epoch 13/86, Loss: 0.6926\n",
      "Epoch 14/86, Loss: 0.6924\n",
      "Epoch 15/86, Loss: 0.6922\n",
      "Epoch 16/86, Loss: 0.6922\n",
      "Epoch 17/86, Loss: 0.6920\n",
      "Epoch 18/86, Loss: 0.6920\n",
      "Epoch 19/86, Loss: 0.6918\n",
      "Epoch 20/86, Loss: 0.6918\n",
      "Validation Accuracy: 0.5026510067114094\n",
      "Epoch 21/86, Loss: 0.6918\n",
      "Epoch 22/86, Loss: 0.6916\n",
      "Epoch 23/86, Loss: 0.6915\n",
      "Epoch 24/86, Loss: 0.6913\n",
      "Epoch 25/86, Loss: 0.6913\n",
      "Epoch 26/86, Loss: 0.6912\n",
      "Epoch 27/86, Loss: 0.6911\n",
      "Epoch 28/86, Loss: 0.6910\n",
      "Epoch 29/86, Loss: 0.6908\n",
      "Epoch 30/86, Loss: 0.6909\n",
      "Validation Accuracy: 0.4979194630872483\n",
      "Epoch 31/86, Loss: 0.6906\n",
      "Epoch 32/86, Loss: 0.6907\n",
      "Epoch 33/86, Loss: 0.6905\n",
      "Epoch 34/86, Loss: 0.6901\n",
      "Epoch 35/86, Loss: 0.6900\n",
      "Epoch 36/86, Loss: 0.6897\n",
      "Epoch 37/86, Loss: 0.6895\n",
      "Epoch 38/86, Loss: 0.6893\n",
      "Epoch 39/86, Loss: 0.6890\n",
      "Epoch 40/86, Loss: 0.6889\n",
      "Validation Accuracy: 0.49634228187919466\n",
      "Epoch 41/86, Loss: 0.6885\n",
      "Epoch 42/86, Loss: 0.6879\n",
      "Epoch 43/86, Loss: 0.6880\n",
      "Epoch 44/86, Loss: 0.6877\n",
      "Epoch 45/86, Loss: 0.6873\n",
      "Epoch 46/86, Loss: 0.6870\n",
      "Epoch 47/86, Loss: 0.6865\n",
      "Epoch 48/86, Loss: 0.6860\n",
      "Epoch 49/86, Loss: 0.6857\n",
      "Epoch 50/86, Loss: 0.6855\n",
      "Validation Accuracy: 0.4968120805369127\n",
      "Epoch 51/86, Loss: 0.6854\n",
      "Epoch 52/86, Loss: 0.6850\n",
      "Epoch 53/86, Loss: 0.6843\n",
      "Epoch 54/86, Loss: 0.6839\n",
      "Epoch 55/86, Loss: 0.6838\n",
      "Epoch 56/86, Loss: 0.6831\n",
      "Epoch 57/86, Loss: 0.6830\n",
      "Epoch 58/86, Loss: 0.6824\n",
      "Epoch 59/86, Loss: 0.6825\n",
      "Epoch 60/86, Loss: 0.6816\n",
      "Validation Accuracy: 0.4974664429530201\n",
      "Epoch 61/86, Loss: 0.6811\n",
      "Epoch 62/86, Loss: 0.6808\n",
      "Epoch 63/86, Loss: 0.6803\n",
      "Epoch 64/86, Loss: 0.6798\n",
      "Epoch 65/86, Loss: 0.6795\n",
      "Epoch 66/86, Loss: 0.6787\n",
      "Epoch 67/86, Loss: 0.6788\n",
      "Epoch 68/86, Loss: 0.6776\n",
      "Epoch 69/86, Loss: 0.6773\n",
      "Epoch 70/86, Loss: 0.6768\n",
      "Validation Accuracy: 0.49894295302013425\n",
      "Epoch 71/86, Loss: 0.6764\n",
      "Epoch 72/86, Loss: 0.6759\n",
      "Epoch 73/86, Loss: 0.6751\n",
      "Epoch 74/86, Loss: 0.6748\n",
      "Epoch 75/86, Loss: 0.6743\n",
      "Epoch 76/86, Loss: 0.6738\n",
      "Epoch 77/86, Loss: 0.6731\n",
      "Epoch 78/86, Loss: 0.6723\n",
      "Epoch 79/86, Loss: 0.6713\n",
      "Epoch 80/86, Loss: 0.6713\n",
      "Validation Accuracy: 0.49914429530201343\n",
      "Epoch 81/86, Loss: 0.6707\n",
      "Epoch 82/86, Loss: 0.6697\n",
      "Epoch 83/86, Loss: 0.6692\n",
      "Epoch 84/86, Loss: 0.6686\n",
      "Epoch 85/86, Loss: 0.6679\n",
      "Epoch 86/86, Loss: 0.6671\n",
      "Test Accuracy with Best Hyperparameters: 0.5005\n"
     ]
    }
   ],
   "source": [
    "set_seeds(myseed)\n",
    "model = NeuralNetwork(input_size=n, hidden_size =800, depth=2).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.008349349990043525, weight_decay=1e-05)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=int(2000), shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# train_nn(model, train_loader, criterion, optimizer, epochs=best_params[4])\n",
    "train_nn(model, train_loader, criterion, optimizer, epochs=86, X_val = X_test, y_val=y_test)\n",
    "\n",
    "test_accuracy = validate_nn(model, X_test, y_test)\n",
    "print(f\"Test Accuracy with Best Hyperparameters: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5-fold cross val Optimal hyperparams and performance across seeds\n",
    "\n",
    "\n",
    "# n = 9: \n",
    "# [0.0006529817319234669, np.int64(589), np.int64(2), np.int64(2247), np.int64(500), 0.0009065011800271416]\n",
    "\n",
    "\n",
    "# n=12\n",
    "# [0.002922344328571065, np.int64(55), np.int64(2), np.int64(53), np.int64(299), 0.0004494118722718984]\n",
    "\n",
    "\n",
    "# n=15\n",
    "# [0.0008212520954386176, np.int64(514), np.int64(2), np.int64(84), np.int64(500), 0.0006296438079895955]\n",
    "\n",
    "# n=18:\n",
    "# [0.005000499108369632, np.int64(1000), 2.3181051861840816, np.int64(1778), np.int64(290), 0.00036096505804747496]\n",
    "\n",
    "\n",
    "# n=24:\n",
    "# [0.008349349990043525, np.int64(800), 2.7541886562632367, np.int64(2000), np.int64(86), 1e-05]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# n=9\n",
    "# seed = 6095\n",
    "# 80/20\n",
    "# Test Accuracy with Best Hyperparameters: 0.9551\n",
    "\n",
    "# 70/30\n",
    "\n",
    "# 90/10  \n",
    "# 60/40:\n",
    "# Test Accuracy with Best Hyperparameters: 0.9569\n",
    "\n",
    "\n",
    "\n",
    "# seed = 42\n",
    "# Test Accuracy with Best Hyperparameters: 0.9595\n",
    "\n",
    "# seed = 81\n",
    "# Test Accuracy with Best Hyperparameters: 0.9574\n",
    "\n",
    "# seed = 180\n",
    "# Test Accuracy with Best Hyperparameters: 0.9571\n",
    "\n",
    "# seed = 4\n",
    "# Test Accuracy with Best Hyperparameters: 0.9570\n",
    "\n",
    "\n",
    "# n=12\n",
    "# seed = 6095\n",
    "# Test Accuracy with Best Hyperparameters: 0.9324\n",
    "\n",
    "# seed = 42\n",
    "# Test Accuracy with Best Hyperparameters: 0.9457\n",
    "\n",
    "# seed = 81\n",
    "# Test Accuracy with Best Hyperparameters: 0.9484\n",
    "\n",
    "# seed = 180\n",
    "# Test Accuracy with Best Hyperparameters: 0.9315\n",
    "\n",
    "# seed = 4\n",
    "# Test Accuracy with Best Hyperparameters: 0.9505\n",
    "\n",
    "\n",
    "\n",
    "# n=18 Test Accuracy with Best Hyperparameters: 0.8527\n",
    "\n",
    "\n",
    "# n=12\n",
    "# Test Accuracy with Best Hyperparameters: 0.9356\n",
    "\n",
    "\n",
    "# n=15 \n",
    "\n",
    "# [0.0008212520954386176, np.int64(514), np.int64(2), np.int64(84), np.int64(500), 0.0006296438079895955]\n",
    "\n",
    "\n",
    "# seed = 6095\n",
    "# Test Accuracy with Best Hyperparameters: 0.9408\n",
    "\n",
    "# seed = 42\n",
    "# Test Accuracy with Best Hyperparameters: 0.9466\n",
    "\n",
    "# seed = 81\n",
    "# Test Accuracy with Best Hyperparameters: 0.9420\n",
    "\n",
    "# seed = 180\n",
    "# Test Accuracy with Best Hyperparameters: 0.9379\n",
    "\n",
    "# seed = 4\n",
    "# Test Accuracy with Best Hyperparameters: 0.9380\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# n=18 \n",
    "\n",
    "# [0.005000499108369632, np.int64(1000), 2.3181051861840816, np.int64(1778), np.int64(264), 0.00036096505804747496]\n",
    "\n",
    "# seed = 6095\n",
    "# Test Accuracy with Best Hyperparameters: 0.9058\n",
    "\n",
    "# seed = 42\n",
    "# 0.9289\n",
    "# seed = 81\n",
    "# 0.9012\n",
    "# seed = 180\n",
    "# 0.8724\n",
    "# seed = 4\n",
    "# 0.8688\n",
    "\n",
    "\n",
    "# n=24\n",
    "# Test Accuracy with Best Hyperparameters: 0.8631\n",
    "\n",
    "\n",
    "# seed = 42\n",
    "# 0.8568\n",
    "# seed = 81\n",
    "# 0.8681\n",
    "# seed = 180\n",
    "# 0.5005\n",
    "# seed = 4\n",
    "# 0.8638\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 24)\n",
      "(10000, 1)\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Array saved to hiddenlabels/y_predicted_24.npy\n"
     ]
    }
   ],
   "source": [
    "def hidden_nn(model, X_hidden):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        held_outputs = model(X_hidden.to(device))\n",
    "        held_outputs = torch.round(torch.sigmoid(held_outputs)).cpu().numpy()\n",
    "        \n",
    "    return held_outputs\n",
    "\n",
    "X_hidden = np.load('Datasets/hidden-kryptonite-%s-X.npy'%(n))\n",
    "scaler.transform(X_hidden)\n",
    "print(np.shape(X_hidden))\n",
    "X_hidden = torch.tensor(X_hidden, dtype=torch.float32)\n",
    "hidden_labels = hidden_nn(model, X_hidden)\n",
    "print(np.shape(hidden_labels))\n",
    "print(hidden_labels)\n",
    "\n",
    "\n",
    "directory = \"hiddenlabels/\"\n",
    "filename = f\"y_predicted_{n}.npy\"\n",
    "\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "np.save(file_path, hidden_labels)\n",
    "\n",
    "print(f\"Array saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_train, y_train, X_val, y_val, param_grid, krypto_n):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    for depth, hidden_size, learning_rate, batch_size, epochs in product(*param_grid.values()):\n",
    "\n",
    "\n",
    "        print(f\"Training with depth={depth}, hidden_size={hidden_size}, learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "        \n",
    "        model = NeuralNetwork(input_size=krypto_n, hidden_size=hidden_size, depth=depth).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(myseed)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "        \n",
    "        train_nn(model, train_loader, criterion, optimizer, epochs=epochs)\n",
    "        \n",
    "        accuracy = validate_nn(model, X_val, y_val)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {'depth':depth, 'hidden_size': hidden_size, 'learning_rate': learning_rate, \n",
    "                           'batch_size': batch_size, 'epochs': epochs}\n",
    "            best_model = model\n",
    "\n",
    "        print(f\"Current best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    \n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Validation Accuracy:\", best_accuracy)\n",
    "    print(f\"Krypto variant: {krypto_n}\")\n",
    "    \n",
    "    \n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_grid_search(X_train, y_train, X_val, y_val, krypto_n):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    combos = 0\n",
    "    max_combos = 10\n",
    "\n",
    "    while best_accuracy < 0.90:\n",
    "\n",
    "\n",
    "        # hidden_sizes = param_grid['hidden_size']\n",
    "        # learning_rates = param_grid~learning_rate']\n",
    "        # batch_sizes = param_grid['batch_size']\n",
    "        # epoch_list = param_grid['epochs']\n",
    "\n",
    "        hidden_size = np.random.randint(10,4000)\n",
    "        learning_rate = np.random.uniform(0.0003, 0.0045)\n",
    "        batch_size = np.random.randint(64,6000)\n",
    "        epochs = np.random.randint(15, 50)\n",
    "        depth = np.random.randint(2,3)\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Training with depth={depth}, hidden_size={hidden_size}, learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "        \n",
    "        model = NeuralNetwork(input_size=krypto_n, hidden_size=hidden_size, depth=depth).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(myseed)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "        \n",
    "        train_nn(model, train_loader, criterion, optimizer, epochs=epochs)\n",
    "        \n",
    "        accuracy = validate_nn(model, X_val, y_val)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {'depth': [depth], 'hidden_size': [hidden_size], 'learning_rate': [learning_rate], \n",
    "                           'batch_size': [batch_size], 'epochs': [epochs]}\n",
    "            best_model = model\n",
    "\n",
    "        print(f\"Current best accuracy: {best_accuracy:.4f}\")\n",
    "        combos += 1\n",
    "\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Validation Accuracy:\", best_accuracy)\n",
    "    print(f\"Krypto variant: {krypto_n}\")\n",
    "    print(combos)\n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.04003923969404982\n",
      "hidden size=168\n",
      "depth=2.0\n",
      "batch_size=10407\n",
      "epochs=56\n",
      "decay=7.059680419915504e-07\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/56, Loss: 1.1497\n",
      "Epoch 2/56, Loss: 0.6945\n",
      "Epoch 3/56, Loss: 0.6934\n",
      "Epoch 4/56, Loss: 0.6932\n",
      "Epoch 5/56, Loss: 0.6932\n",
      "Epoch 6/56, Loss: 0.6932\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 113\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# min_accuracy = min(fold_accuracies)\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# print(f\"Min Validation Accuracy: {min_accuracy:.4f} for hyperparameters:\\n\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth=2\\nbatch_size={batch_size}\\nepochs={epochs}\\n\\n\\n\")    \u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mavg_accuracy\n\u001b[1;32m--> 113\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgp_minimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmyseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m plot_convergence(result)\n\u001b[0;32m    117\u001b[0m best_params \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\skopt\\optimizer\\gp.py:281\u001b[0m, in \u001b[0;36mgp_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     base_estimator \u001b[38;5;241m=\u001b[39m cook_estimator(\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGP\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    276\u001b[0m         space\u001b[38;5;241m=\u001b[39mspace,\n\u001b[0;32m    277\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrng\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax),\n\u001b[0;32m    278\u001b[0m         noise\u001b[38;5;241m=\u001b[39mnoise,\n\u001b[0;32m    279\u001b[0m     )\n\u001b[1;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43macq_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43macq_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macq_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_random_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_random_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_initial_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_initial_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_point_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_point_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43my0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\skopt\\optimizer\\base.py:332\u001b[0m, in \u001b[0;36mbase_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_calls):\n\u001b[0;32m    331\u001b[0m     next_x \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mask()\n\u001b[1;32m--> 332\u001b[0m     next_y \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m     result \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mtell(next_x, next_y)\n\u001b[0;32m    334\u001b[0m     result\u001b[38;5;241m.\u001b[39mspecs \u001b[38;5;241m=\u001b[39m specs\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\skopt\\utils.py:779\u001b[0m, in \u001b[0;36muse_named_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    776\u001b[0m arg_dict \u001b[38;5;241m=\u001b[39m {dim\u001b[38;5;241m.\u001b[39mname: value \u001b[38;5;28;01mfor\u001b[39;00m dim, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dimensions, x)}\n\u001b[0;32m    778\u001b[0m \u001b[38;5;66;03m# Call the wrapped objective function with the named arguments.\u001b[39;00m\n\u001b[1;32m--> 779\u001b[0m objective_value \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objective_value\n",
      "Cell \u001b[1;32mIn[13], line 79\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(learning_rate, hidden_size, depth, batch_size, epochs, decay)\u001b[0m\n\u001b[0;32m     76\u001b[0m g\u001b[38;5;241m.\u001b[39mmanual_seed(myseed) \n\u001b[0;32m     77\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(batch_size), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, worker_init_fn\u001b[38;5;241m=\u001b[39mseed_worker, generator\u001b[38;5;241m=\u001b[39mg)\n\u001b[1;32m---> 79\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_val_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# epochs = train_nn(model, train_loader, criterion, optimizer, epochs=epochs)\u001b[39;00m\n\u001b[0;32m     83\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mtrain_nn\u001b[1;34m(model, dataloader, criterion, optimizer, epochs, X_val, y_val)\u001b[0m\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 17\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    210\u001b[0m     state_steps: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m     adam(\n\u001b[0;32m    224\u001b[0m         params_with_grad,\n\u001b[0;32m    225\u001b[0m         grads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:127\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[1;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         has_complex \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m         params_with_grad\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "\n",
    "param_space = [\n",
    "    Real(1e-5, 1e-1, prior=\"log-uniform\", name=\"learning_rate\"), \n",
    "    Integer(100, 2000, prior = \"log-uniform\",name=\"hidden_size\"),              \n",
    "    Real(2,2.5, prior=\"log-uniform\" ,name=\"depth\"),                           \n",
    "    Integer(2000, 30000, prior = \"log-uniform\", name=\"batch_size\"),                   \n",
    "    Integer(10, 180,prior = \"log-uniform\", name=\"epochs\"),\n",
    "    Real(0.0000001,0.1, prior =\"log-uniform\", name=\"decay\")\n",
    "]\n",
    "\n",
    "# n=15 paramspace\n",
    "# param_space = [\n",
    "#     Real(1e-5, 1e-2, prior=\"log-uniform\", name=\"learning_rate\"),  \n",
    "#     Integer(2, 6000, prior = \"log-uniform\",name=\"hidden_size\"),                       \n",
    "#     Integer(2, 4, name=\"depth\"),                               \n",
    "#     Integer(16, 10000, prior = \"log-uniform\", name=\"batch_size\"),                         \n",
    "#     Integer(1, 500, name=\"epochs\"),                             \n",
    "# ]\n",
    "\n",
    "# best validation performance at around:\n",
    "# lr = 0.0002\n",
    "# hidden_size = 889-1200\n",
    "# depth = 2\n",
    "# batch_size = 40\n",
    "# epochs = 500\n",
    "\n",
    "\n",
    "set_seeds(myseed)\n",
    "\n",
    "krypto_n = n\n",
    "k_folds = 5\n",
    "\n",
    "@use_named_args(param_space)\n",
    "\n",
    "def objective(learning_rate, hidden_size, depth, batch_size, epochs, decay):\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=myseed)\n",
    "    fold_accuracies = []\n",
    "\n",
    "    print(f\"Training with hyperparameters:\\n\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth={np.floor(depth)}\\nbatch_size={batch_size}\\nepochs={epochs}\\ndecay={decay}\\n\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        # fold_start = time.perf_counter()\n",
    "        print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "\n",
    "\n",
    "        model = NeuralNetwork(input_size=krypto_n, hidden_size=hidden_size, depth=int(np.floor(depth))).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(myseed) \n",
    "        train_loader = DataLoader(train_dataset, batch_size=int(batch_size), shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "        \n",
    "        epochs = train_nn(model, train_loader, criterion, optimizer, epochs=epochs, X_val =X_val_fold, y_val = y_val_fold)\n",
    "        # epochs = train_nn(model, train_loader, criterion, optimizer, epochs=epochs)\n",
    "       \n",
    "       \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        print(f\"\\nTrained in {epochs} epochs\\n\")\n",
    "        \n",
    "        # fold_end = time.perf_counter()\n",
    "\n",
    "        accuracy = validate_nn(model, X_val_fold, y_val_fold)\n",
    "        print(f\"Validation Accuracy for Fold {fold + 1}: {accuracy:.4f}\")\n",
    "        print(f\"\\nwith hyperparameters:\\n\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth={int(np.floor(depth))}\\nbatch_size={batch_size}\\nepochs={epochs}\\ndecay={decay}\\n\\n\\n\")\n",
    "        if accuracy < 0.74:\n",
    "            return -accuracy\n",
    "        \n",
    "        # fold_duration = fold_end - fold_start\n",
    "        # if fold_duration > 80:\n",
    "            # return 1\n",
    "        \n",
    "        fold_accuracies.append(accuracy)\n",
    "    \n",
    "    avg_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "    print(f\"Average Validation Accuracy: {avg_accuracy:.4f} for hyperparameters:\\n\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth={np.floor(depth)}\\nbatch_size={batch_size}\\nepochs={epochs}\\ndecay={decay}\\n\\n\\n\")\n",
    "\n",
    "\n",
    "    # min_accuracy = min(fold_accuracies)\n",
    "    # print(f\"Min Validation Accuracy: {min_accuracy:.4f} for hyperparameters:\\n\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth=2\\nbatch_size={batch_size}\\nepochs={epochs}\\n\\n\\n\")    \n",
    "\n",
    "    return -avg_accuracy\n",
    "\n",
    "\n",
    "\n",
    "result = gp_minimize(objective, param_space, n_calls=60, random_state=myseed)\n",
    "\n",
    "plot_convergence(result)\n",
    "\n",
    "best_params = result.x\n",
    "\n",
    "# emissions: float = tracker.stop()\n",
    "# print(emissions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/86, Loss: 0.8068\n",
      "Epoch 2/86, Loss: 0.6932\n",
      "Epoch 3/86, Loss: 0.6932\n",
      "Epoch 4/86, Loss: 0.6932\n",
      "Epoch 5/86, Loss: 0.6931\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(best_params[\u001b[38;5;241m3\u001b[39m]), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# train_nn(model, train_loader, criterion, optimizer, epochs=best_params[4])\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m86\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m validate_nn(model, X_test, y_test)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy with Best Hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mtrain_nn\u001b[1;34m(model, dataloader, criterion, optimizer, epochs, X_val, y_val)\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      8\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Move data to device\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_seeds(myseed)\n",
    "\n",
    "model = NeuralNetwork(input_size=krypto_n, hidden_size=best_params[1], depth=int(best_params[2])).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params[0], weight_decay=best_params[5])\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=int(best_params[3]), shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# train_nn(model, train_loader, criterion, optimizer, epochs=best_params[4])\n",
    "train_nn(model, train_loader, criterion, optimizer, epochs=best_params[4], X_val = X_test, y_val=y_test)\n",
    "\n",
    "test_accuracy = validate_nn(model, X_test, y_test)\n",
    "print(f\"Test Accuracy with Best Hyperparameters: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# n=9\n",
    "# seed = 6095\n",
    "# 80/20\n",
    "# Test Accuracy with Best Hyperparameters: 0.9551\n",
    "\n",
    "# 70/30\n",
    "\n",
    "# 90/10  \n",
    "\n",
    "\n",
    "\n",
    "# seed = 42\n",
    "# Test Accuracy with Best Hyperparameters: 0.9595\n",
    "\n",
    "# seed = 81\n",
    "# Test Accuracy with Best Hyperparameters: 0.9574\n",
    "\n",
    "# seed = 180\n",
    "# Test Accuracy with Best Hyperparameters: 0.9571\n",
    "\n",
    "# seed = 4\n",
    "# Test Accuracy with Best Hyperparameters: 0.9570\n",
    "\n",
    "\n",
    "# n=12\n",
    "# seed = 6095\n",
    "# Test Accuracy with Best Hyperparameters: 0.9324\n",
    "\n",
    "# seed = 42\n",
    "# Test Accuracy with Best Hyperparameters: 0.9457\n",
    "\n",
    "# seed = 81\n",
    "# Test Accuracy with Best Hyperparameters: 0.9484\n",
    "\n",
    "# seed = 180\n",
    "# Test Accuracy with Best Hyperparameters: 0.9315\n",
    "\n",
    "# seed = 4\n",
    "# Test Accuracy with Best Hyperparameters: 0.9505\n",
    "\n",
    "\n",
    "\n",
    "# n=18 Test Accuracy with Best Hyperparameters: 0.8527\n",
    "\n",
    "\n",
    "# n=12\n",
    "# Test Accuracy with Best Hyperparameters: 0.9356\n",
    "\n",
    "\n",
    "# n=15 \n",
    "\n",
    "# [0.0008212520954386176, np.int64(514), np.int64(2), np.int64(84), np.int64(500), 0.0006296438079895955]\n",
    "\n",
    "\n",
    "# seed = 6095\n",
    "# Test Accuracy with Best Hyperparameters: 0.9408\n",
    "\n",
    "# seed = 42\n",
    "# Test Accuracy with Best Hyperparameters: 0.9466\n",
    "\n",
    "# seed = 81\n",
    "# Test Accuracy with Best Hyperparameters: 0.9420\n",
    "\n",
    "# seed = 180\n",
    "# Test Accuracy with Best Hyperparameters: 0.9379\n",
    "\n",
    "# seed = 4\n",
    "# Test Accuracy with Best Hyperparameters: 0.9380\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# n=18 \n",
    "\n",
    "# [0.005000499108369632, np.int64(1000), 2.3181051861840816, np.int64(1778), np.int64(264), 0.00036096505804747496]\n",
    "\n",
    "# seed = 6095\n",
    "# Test Accuracy with Best Hyperparameters: 0.9058\n",
    "\n",
    "# seed = 42\n",
    "\n",
    "# seed = 81\n",
    "\n",
    "# seed = 180\n",
    "\n",
    "# seed = 4\n",
    "\n",
    "\n",
    "\n",
    "# n=24\n",
    "# Test Accuracy with Best Hyperparameters: 0.8631\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.008349349990043525, np.int64(800), 2.7541886562632367, np.int64(2000), np.int64(86), 1e-05]\n"
     ]
    }
   ],
   "source": [
    "print(best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
