{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using neural networks to predict on Kryptonite-N dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":4096:8\n",
      "expandable_segments:True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, KFold\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\sklearn\\__init__.py:84\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     )\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m     ]\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\sklearn\\utils\\_joblib.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m     _warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# joblib imports may raise DeprecationWarning on certain Python\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# versions\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m         Memory,\n\u001b[0;32m     12\u001b[0m         Parallel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         register_parallel_backend,\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     26\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_parallel_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m ]\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\joblib\\__init__.py:129\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_pickle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_compressor\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delayed\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cpu_count\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\joblib\\parallel.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logger, short_format_time\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m memstr_to_bytes\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parallel_backends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (FallbackToBackend, MultiprocessingBackend,\n\u001b[0;32m     32\u001b[0m                                  ThreadingBackend, SequentialBackend,\n\u001b[0;32m     33\u001b[0m                                  LokyBackend)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eval_expr, _Sentinel\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Make sure that those two classes are part of the public joblib.parallel API\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# so that 3rd party backend implementers can import them from here.\u001b[39;00m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\joblib\\_parallel_backends.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ABCMeta, abstractmethod\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     _TracebackCapturingWrapper,\n\u001b[0;32m     14\u001b[0m     _retrieve_traceback_capturing_wrapped_call\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiprocessing_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mp\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\joblib\\_utils.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiprocessing_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mp\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloky\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ExceptionWithTraceback\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# supported operators\u001b[39;00m\n\u001b[0;32m     15\u001b[0m operators \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m     ast\u001b[38;5;241m.\u001b[39mAdd: op\u001b[38;5;241m.\u001b[39madd,\n\u001b[0;32m     17\u001b[0m     ast\u001b[38;5;241m.\u001b[39mSub: op\u001b[38;5;241m.\u001b[39msub,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     ast\u001b[38;5;241m.\u001b[39mUSub: op\u001b[38;5;241m.\u001b[39mneg,\n\u001b[0;32m     24\u001b[0m }\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\__init__.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreduction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_loky_pickler\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreusable_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_reusable_executor\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloudpickle_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wrap_non_picklable_objects\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BrokenProcessPool, ProcessPoolExecutor\n\u001b[0;32m     25\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_reusable_executor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu_count\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_loky_pickler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     41\u001b[0m ]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1524\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1498\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1597\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "myseed = 6095\n",
    "os.environ['PYTHONHASHSEED'] = str(myseed)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]= \":4096:8\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]= \"expandable_segments:True\"\n",
    "print(os.getenv(\"CUBLAS_WORKSPACE_CONFIG\"))\n",
    "print(os.getenv(\"PYTORCH_CUDA_ALLOC_CONF\"))\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence, plot_gaussian_process\n",
    "import time\n",
    "from codecarbon import EmissionsTracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {0}'.format(device))\n",
    "\n",
    "\n",
    "def set_seeds(myseed):\n",
    "\n",
    "    random.seed(myseed)\n",
    "    np.random.seed(myseed)\n",
    "    torch.manual_seed(myseed)\n",
    "    torch.cuda.manual_seed(myseed)\n",
    "    torch.cuda.manual_seed_all(myseed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "set_seeds(myseed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_features_vs_label(X, y):\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    assert y.shape == (n_samples,)\n",
    "    assert set(np.unique(y)).issubset({0, 1})\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.scatter(X[y == 0, i], [0] * sum(y == 0), color='blue', label='Label 0', alpha=0.6)\n",
    "        plt.scatter(X[y == 1, i], [1] * sum(y == 1), color='red', label='Label 1', alpha=0.6)\n",
    "        \n",
    "        plt.xlabel(f'Feature {i+1}')\n",
    "        plt.ylabel('Label')\n",
    "        plt.title(f'Feature {i+1} vs. Label')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "n = 15\n",
    "X = np.load('Datasets/kryptonite-%s-X.npy'%(n))\n",
    "y = np.load('Datasets/kryptonite-%s-y.npy'%(n))\n",
    "\n",
    "# plot_features_vs_label(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, depth):\n",
    "        super().__init__()\n",
    "        # self.linear_layer_stack = nn.Sequential(\n",
    "        #     nn.Linear(input_size, hidden_size),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(hidden_size,hidden_size), \n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(hidden_size,1), \n",
    "        # )\n",
    "\n",
    "\n",
    "\n",
    "        modules = [torch.nn.Linear(input_size,hidden_size), torch.nn.LeakyReLU()]\n",
    "\n",
    "        for i in range(depth-1):\n",
    "            modules.append(torch.nn.Linear(hidden_size,hidden_size))\n",
    "            modules.append(torch.nn.LeakyReLU())\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.hidden_layer_stack = torch.nn.Sequential(\n",
    "            *modules,\n",
    "        )\n",
    "        \n",
    "        set_seeds(myseed)\n",
    "        self._initialize_weights()  \n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.hidden_layer_stack:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight)\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer_stack(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_nn(model, X_val, y_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val.to(device))\n",
    "        val_outputs = torch.round(torch.sigmoid(val_outputs)).cpu().numpy()\n",
    "        accuracy = accuracy_score(y_val, val_outputs)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# train_size = len(X_train)\n",
    "# def train_nn(model, dataloader, criterion, optimizer, epochs):\n",
    "\n",
    "def train_nn(model, dataloader, criterion, optimizer, epochs, X_val, y_val):\n",
    "    start= time.perf_counter()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.unsqueeze(1).to(device)  # Move data to device\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}')\n",
    "        # eoe_time = time.perf_counter()\n",
    "        # if eoe_time-start > 150:\n",
    "        #     return epoch+1\n",
    "            \n",
    "        \n",
    "        \n",
    "        if (epoch+1)%10 == 0:\n",
    "            validation_acc = validate_nn(model,X_val, y_val)\n",
    "            print(f\"Validation Accuracy: {validation_acc}\")\n",
    "\n",
    "        # if epoch%10 == 0:\n",
    "        #     validate_nn(model, )\n",
    "\n",
    "    return epochs\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_train, y_train, X_val, y_val, param_grid, krypto_n):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    for depth, hidden_size, learning_rate, batch_size, epochs in product(*param_grid.values()):\n",
    "\n",
    "\n",
    "        print(f\"Training with depth={depth}, hidden_size={hidden_size}, learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "        \n",
    "        model = NeuralNetwork(input_size=krypto_n, hidden_size=hidden_size, depth=depth).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(myseed)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "        \n",
    "        train_nn(model, train_loader, criterion, optimizer, epochs=epochs)\n",
    "        \n",
    "        accuracy = validate_nn(model, X_val, y_val)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {'depth':depth, 'hidden_size': hidden_size, 'learning_rate': learning_rate, \n",
    "                           'batch_size': batch_size, 'epochs': epochs}\n",
    "            best_model = model\n",
    "\n",
    "        print(f\"Current best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    \n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Validation Accuracy:\", best_accuracy)\n",
    "    print(f\"Krypto variant: {krypto_n}\")\n",
    "    \n",
    "    \n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_grid_search(X_train, y_train, X_val, y_val, krypto_n):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    combos = 0\n",
    "    max_combos = 10\n",
    "\n",
    "    while best_accuracy < 0.90:\n",
    "\n",
    "\n",
    "        # hidden_sizes = param_grid['hidden_size']\n",
    "        # learning_rates = param_grid~learning_rate']\n",
    "        # batch_sizes = param_grid['batch_size']\n",
    "        # epoch_list = param_grid['epochs']\n",
    "\n",
    "        hidden_size = np.random.randint(10,4000)\n",
    "        learning_rate = np.random.uniform(0.0003, 0.0045)\n",
    "        batch_size = np.random.randint(64,6000)\n",
    "        epochs = np.random.randint(15, 50)\n",
    "        depth = np.random.randint(2,3)\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Training with depth={depth}, hidden_size={hidden_size}, learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "        \n",
    "        model = NeuralNetwork(input_size=krypto_n, hidden_size=hidden_size, depth=depth).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(myseed)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "        \n",
    "        train_nn(model, train_loader, criterion, optimizer, epochs=epochs)\n",
    "        \n",
    "        accuracy = validate_nn(model, X_val, y_val)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {'depth': [depth], 'hidden_size': [hidden_size], 'learning_rate': [learning_rate], \n",
    "                           'batch_size': [batch_size], 'epochs': [epochs]}\n",
    "            best_model = model\n",
    "\n",
    "        print(f\"Current best accuracy: {best_accuracy:.4f}\")\n",
    "        combos += 1\n",
    "\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Validation Accuracy:\", best_accuracy)\n",
    "    print(f\"Krypto variant: {krypto_n}\")\n",
    "    print(combos)\n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "Device: cuda\n",
      "(60000, 30)\n",
      "(250000, 30)\n",
      "(310000, 30)\n",
      "torch.Size([248000, 30])\n",
      "tensor([ 0.0209,  0.9877,  0.0376,  0.8977,  0.0126,  0.0118,  0.0183,  0.0038,\n",
      "        -0.0080,  0.8416,  0.9639,  0.7767,  0.0628,  1.0071,  1.1124,  0.0173,\n",
      "         1.0892, -0.0105,  0.0932,  0.9871,  0.9691,  1.0487,  0.0365,  0.0269,\n",
      "         0.2208,  0.9698,  0.9534, -0.2391,  0.8032,  1.1243])\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\"\n",
    "print('Device: {0}'.format(device))\n",
    "\n",
    "# setting kryptonite dataset no.\n",
    "n = 30\n",
    "\n",
    "# Reading and normalising dataset features for efficient convergence\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = np.load('Datasets/kryptonite-%s-X.npy'%(n))\n",
    "y = np.load('Datasets/kryptonite-%s-y.npy'%(n))\n",
    "\n",
    "# print(f\"X shape: {X.shape()}\")\n",
    "print(np.shape(X))\n",
    "\n",
    "if n>18:\n",
    "    X_add = np.load('Datasets/additional-kryptonite-%s-X.npy'%(n))\n",
    "    print(np.shape(X_add))\n",
    "    y_add = np.load('Datasets/additional-kryptonite-%s-y.npy'%(n))\n",
    "\n",
    "    X=  np.concatenate((X,X_add),axis = 0)\n",
    "    y=  np.concatenate((y,y_add),axis = 0)\n",
    "\n",
    "\n",
    "print(np.shape(X))\n",
    "\n",
    "\n",
    "# print((X[0]))\n",
    "\n",
    "# # Shuffle and split the data\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=myseed)  # 60% training\n",
    "\n",
    "# scaler.fit(X_train)\n",
    "# scaler.transform(X_train)\n",
    "# scaler.transform(X_temp)\n",
    "\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=myseed)  # 20% validation, 20% test\n",
    "\n",
    "set_seeds(myseed)\n",
    "\n",
    "# Shuffle and split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=myseed)  # 80% training-validation/20% testing\n",
    "\n",
    "scaler.fit(X_train)\n",
    "scaler.transform(X_train)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=myseed)  # 20% validation, 20% test\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "print((X_train.size()))\n",
    "# print((X_val[0]))\n",
    "print((X_test[0]))\n",
    "\n",
    "\n",
    "\n",
    "best_accuracy = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.0015341702952119374\n",
      "hidden size=851\n",
      "depth=2.0\n",
      "batch_size=28553\n",
      "epochs=26\n",
      "decay=6.5814914921113845e-06\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/26, Loss: 1.1054\n",
      "Epoch 2/26, Loss: 0.7529\n",
      "Epoch 3/26, Loss: 0.7021\n",
      "Epoch 4/26, Loss: 0.7046\n",
      "Epoch 5/26, Loss: 0.6943\n",
      "Epoch 6/26, Loss: 0.6950\n",
      "Epoch 7/26, Loss: 0.6936\n",
      "Epoch 8/26, Loss: 0.6933\n",
      "Epoch 9/26, Loss: 0.6933\n",
      "Epoch 10/26, Loss: 0.6931\n",
      "Validation Accuracy: 0.4994556451612903\n",
      "Epoch 11/26, Loss: 0.6931\n",
      "Epoch 12/26, Loss: 0.6930\n",
      "Epoch 13/26, Loss: 0.6929\n",
      "Epoch 14/26, Loss: 0.6929\n",
      "Epoch 15/26, Loss: 0.6928\n",
      "Epoch 16/26, Loss: 0.6928\n",
      "Epoch 17/26, Loss: 0.6927\n",
      "Epoch 18/26, Loss: 0.6927\n",
      "Epoch 19/26, Loss: 0.6926\n",
      "Epoch 20/26, Loss: 0.6925\n",
      "Validation Accuracy: 0.4993951612903226\n",
      "Epoch 21/26, Loss: 0.6924\n",
      "Epoch 22/26, Loss: 0.6924\n",
      "Epoch 23/26, Loss: 0.6922\n",
      "Epoch 24/26, Loss: 0.6921\n",
      "Epoch 25/26, Loss: 0.6920\n",
      "Epoch 26/26, Loss: 0.6918\n",
      "\n",
      "Trained in 26 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.5011\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=0.0015341702952119374\n",
      "hidden size=851\n",
      "depth=2\n",
      "batch_size=28553\n",
      "epochs=26\n",
      "decay=6.5814914921113845e-06\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=3.551802167432569e-05\n",
      "hidden size=1759\n",
      "depth=2.0\n",
      "batch_size=29192\n",
      "epochs=33\n",
      "decay=0.0006763821583822122\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/33, Loss: 0.7010\n",
      "Epoch 2/33, Loss: 0.6975\n",
      "Epoch 3/33, Loss: 0.6956\n",
      "Epoch 4/33, Loss: 0.6947\n",
      "Epoch 5/33, Loss: 0.6945\n",
      "Epoch 6/33, Loss: 0.6942\n",
      "Epoch 7/33, Loss: 0.6940\n",
      "Epoch 8/33, Loss: 0.6938\n",
      "Epoch 9/33, Loss: 0.6937\n",
      "Epoch 10/33, Loss: 0.6936\n",
      "Validation Accuracy: 0.49858870967741936\n",
      "Epoch 11/33, Loss: 0.6935\n",
      "Epoch 12/33, Loss: 0.6933\n",
      "Epoch 13/33, Loss: 0.6932\n",
      "Epoch 14/33, Loss: 0.6931\n",
      "Epoch 15/33, Loss: 0.6930\n",
      "Epoch 16/33, Loss: 0.6929\n",
      "Epoch 17/33, Loss: 0.6928\n",
      "Epoch 18/33, Loss: 0.6927\n",
      "Epoch 19/33, Loss: 0.6926\n",
      "Epoch 20/33, Loss: 0.6924\n",
      "Validation Accuracy: 0.4996975806451613\n",
      "Epoch 21/33, Loss: 0.6924\n",
      "Epoch 22/33, Loss: 0.6923\n",
      "Epoch 23/33, Loss: 0.6923\n",
      "Epoch 24/33, Loss: 0.6921\n",
      "Epoch 25/33, Loss: 0.6920\n",
      "Epoch 26/33, Loss: 0.6919\n",
      "Epoch 27/33, Loss: 0.6919\n",
      "Epoch 28/33, Loss: 0.6918\n",
      "Epoch 29/33, Loss: 0.6918\n",
      "Epoch 30/33, Loss: 0.6916\n",
      "Validation Accuracy: 0.5000403225806451\n",
      "Epoch 31/33, Loss: 0.6916\n",
      "Epoch 32/33, Loss: 0.6914\n",
      "Epoch 33/33, Loss: 0.6914\n",
      "\n",
      "Trained in 33 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.5021\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=3.551802167432569e-05\n",
      "hidden size=1759\n",
      "depth=2\n",
      "batch_size=29192\n",
      "epochs=33\n",
      "decay=0.0006763821583822122\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=2.025891496271218e-05\n",
      "hidden size=991\n",
      "depth=2.0\n",
      "batch_size=5674\n",
      "epochs=46\n",
      "decay=0.03922161431938617\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/46, Loss: 0.7015\n",
      "Epoch 2/46, Loss: 0.6986\n",
      "Epoch 3/46, Loss: 0.6976\n",
      "Epoch 4/46, Loss: 0.6969\n",
      "Epoch 5/46, Loss: 0.6964\n",
      "Epoch 6/46, Loss: 0.6959\n",
      "Epoch 7/46, Loss: 0.6956\n",
      "Epoch 8/46, Loss: 0.6953\n",
      "Epoch 9/46, Loss: 0.6951\n",
      "Epoch 10/46, Loss: 0.6949\n",
      "Validation Accuracy: 0.4986491935483871\n",
      "Epoch 11/46, Loss: 0.6947\n",
      "Epoch 12/46, Loss: 0.6946\n",
      "Epoch 13/46, Loss: 0.6945\n",
      "Epoch 14/46, Loss: 0.6944\n",
      "Epoch 15/46, Loss: 0.6943\n",
      "Epoch 16/46, Loss: 0.6942\n",
      "Epoch 17/46, Loss: 0.6941\n",
      "Epoch 18/46, Loss: 0.6940\n",
      "Epoch 19/46, Loss: 0.6940\n",
      "Epoch 20/46, Loss: 0.6939\n",
      "Validation Accuracy: 0.49933467741935483\n",
      "Epoch 21/46, Loss: 0.6938\n",
      "Epoch 22/46, Loss: 0.6938\n",
      "Epoch 23/46, Loss: 0.6937\n",
      "Epoch 24/46, Loss: 0.6937\n",
      "Epoch 25/46, Loss: 0.6937\n",
      "Epoch 26/46, Loss: 0.6936\n",
      "Epoch 27/46, Loss: 0.6936\n",
      "Epoch 28/46, Loss: 0.6935\n",
      "Epoch 29/46, Loss: 0.6935\n",
      "Epoch 30/46, Loss: 0.6935\n",
      "Validation Accuracy: 0.4997782258064516\n",
      "Epoch 31/46, Loss: 0.6935\n",
      "Epoch 32/46, Loss: 0.6934\n",
      "Epoch 33/46, Loss: 0.6934\n",
      "Epoch 34/46, Loss: 0.6934\n",
      "Epoch 35/46, Loss: 0.6934\n",
      "Epoch 36/46, Loss: 0.6933\n",
      "Epoch 37/46, Loss: 0.6933\n",
      "Epoch 38/46, Loss: 0.6933\n",
      "Epoch 39/46, Loss: 0.6933\n",
      "Epoch 40/46, Loss: 0.6933\n",
      "Validation Accuracy: 0.5008064516129033\n",
      "Epoch 41/46, Loss: 0.6932\n",
      "Epoch 42/46, Loss: 0.6932\n",
      "Epoch 43/46, Loss: 0.6932\n",
      "Epoch 44/46, Loss: 0.6932\n",
      "Epoch 45/46, Loss: 0.6932\n",
      "Epoch 46/46, Loss: 0.6932\n",
      "\n",
      "Trained in 46 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.5008\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=2.025891496271218e-05\n",
      "hidden size=991\n",
      "depth=2\n",
      "batch_size=5674\n",
      "epochs=46\n",
      "decay=0.03922161431938617\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.0001002440615545309\n",
      "hidden size=549\n",
      "depth=2.0\n",
      "batch_size=3476\n",
      "epochs=119\n",
      "decay=1.1014963169863628e-07\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/119, Loss: 0.6961\n",
      "Epoch 2/119, Loss: 0.6941\n",
      "Epoch 3/119, Loss: 0.6934\n",
      "Epoch 4/119, Loss: 0.6929\n",
      "Epoch 5/119, Loss: 0.6925\n",
      "Epoch 6/119, Loss: 0.6921\n",
      "Epoch 7/119, Loss: 0.6917\n",
      "Epoch 8/119, Loss: 0.6916\n",
      "Epoch 9/119, Loss: 0.6911\n",
      "Epoch 10/119, Loss: 0.6907\n",
      "Validation Accuracy: 0.5005241935483871\n",
      "Epoch 11/119, Loss: 0.6906\n",
      "Epoch 12/119, Loss: 0.6903\n",
      "Epoch 13/119, Loss: 0.6899\n",
      "Epoch 14/119, Loss: 0.6895\n",
      "Epoch 15/119, Loss: 0.6892\n",
      "Epoch 16/119, Loss: 0.6889\n",
      "Epoch 17/119, Loss: 0.6886\n",
      "Epoch 18/119, Loss: 0.6883\n",
      "Epoch 19/119, Loss: 0.6877\n",
      "Epoch 20/119, Loss: 0.6877\n",
      "Validation Accuracy: 0.4996774193548387\n",
      "Epoch 21/119, Loss: 0.6872\n",
      "Epoch 22/119, Loss: 0.6867\n",
      "Epoch 23/119, Loss: 0.6862\n",
      "Epoch 24/119, Loss: 0.6860\n",
      "Epoch 25/119, Loss: 0.6855\n",
      "Epoch 26/119, Loss: 0.6849\n",
      "Epoch 27/119, Loss: 0.6849\n",
      "Epoch 28/119, Loss: 0.6842\n",
      "Epoch 29/119, Loss: 0.6837\n",
      "Epoch 30/119, Loss: 0.6831\n",
      "Validation Accuracy: 0.5006048387096774\n",
      "Epoch 31/119, Loss: 0.6825\n",
      "Epoch 32/119, Loss: 0.6821\n",
      "Epoch 33/119, Loss: 0.6814\n",
      "Epoch 34/119, Loss: 0.6810\n",
      "Epoch 35/119, Loss: 0.6809\n",
      "Epoch 36/119, Loss: 0.6804\n",
      "Epoch 37/119, Loss: 0.6795\n",
      "Epoch 38/119, Loss: 0.6789\n",
      "Epoch 39/119, Loss: 0.6783\n",
      "Epoch 40/119, Loss: 0.6780\n",
      "Validation Accuracy: 0.5000201612903226\n",
      "Epoch 41/119, Loss: 0.6776\n",
      "Epoch 42/119, Loss: 0.6769\n",
      "Epoch 43/119, Loss: 0.6760\n",
      "Epoch 44/119, Loss: 0.6756\n",
      "Epoch 45/119, Loss: 0.6750\n",
      "Epoch 46/119, Loss: 0.6749\n",
      "Epoch 47/119, Loss: 0.6742\n",
      "Epoch 48/119, Loss: 0.6731\n",
      "Epoch 49/119, Loss: 0.6725\n",
      "Epoch 50/119, Loss: 0.6719\n",
      "Validation Accuracy: 0.5013709677419355\n",
      "Epoch 51/119, Loss: 0.6714\n",
      "Epoch 52/119, Loss: 0.6716\n",
      "Epoch 53/119, Loss: 0.6704\n",
      "Epoch 54/119, Loss: 0.6697\n",
      "Epoch 55/119, Loss: 0.6692\n",
      "Epoch 56/119, Loss: 0.6685\n",
      "Epoch 57/119, Loss: 0.6677\n",
      "Epoch 58/119, Loss: 0.6674\n",
      "Epoch 59/119, Loss: 0.6670\n",
      "Epoch 60/119, Loss: 0.6661\n",
      "Validation Accuracy: 0.4986895161290323\n",
      "Epoch 61/119, Loss: 0.6657\n",
      "Epoch 62/119, Loss: 0.6650\n",
      "Epoch 63/119, Loss: 0.6645\n",
      "Epoch 64/119, Loss: 0.6647\n",
      "Epoch 65/119, Loss: 0.6629\n",
      "Epoch 66/119, Loss: 0.6622\n",
      "Epoch 67/119, Loss: 0.6620\n",
      "Epoch 68/119, Loss: 0.6614\n",
      "Epoch 69/119, Loss: 0.6612\n",
      "Epoch 70/119, Loss: 0.6602\n",
      "Validation Accuracy: 0.5009072580645161\n",
      "Epoch 71/119, Loss: 0.6593\n",
      "Epoch 72/119, Loss: 0.6591\n",
      "Epoch 73/119, Loss: 0.6582\n",
      "Epoch 74/119, Loss: 0.6579\n",
      "Epoch 75/119, Loss: 0.6572\n",
      "Epoch 76/119, Loss: 0.6570\n",
      "Epoch 77/119, Loss: 0.6561\n",
      "Epoch 78/119, Loss: 0.6554\n",
      "Epoch 79/119, Loss: 0.6548\n",
      "Epoch 80/119, Loss: 0.6543\n",
      "Validation Accuracy: 0.49993951612903226\n",
      "Epoch 81/119, Loss: 0.6537\n",
      "Epoch 82/119, Loss: 0.6529\n",
      "Epoch 83/119, Loss: 0.6528\n",
      "Epoch 84/119, Loss: 0.6516\n",
      "Epoch 85/119, Loss: 0.6510\n",
      "Epoch 86/119, Loss: 0.6506\n",
      "Epoch 87/119, Loss: 0.6501\n",
      "Epoch 88/119, Loss: 0.6495\n",
      "Epoch 89/119, Loss: 0.6491\n",
      "Epoch 90/119, Loss: 0.6482\n",
      "Validation Accuracy: 0.5011895161290323\n",
      "Epoch 91/119, Loss: 0.6471\n",
      "Epoch 92/119, Loss: 0.6474\n",
      "Epoch 93/119, Loss: 0.6468\n",
      "Epoch 94/119, Loss: 0.6456\n",
      "Epoch 95/119, Loss: 0.6461\n",
      "Epoch 96/119, Loss: 0.6445\n",
      "Epoch 97/119, Loss: 0.6442\n",
      "Epoch 98/119, Loss: 0.6431\n",
      "Epoch 99/119, Loss: 0.6433\n",
      "Epoch 100/119, Loss: 0.6421\n",
      "Validation Accuracy: 0.5010282258064516\n",
      "Epoch 101/119, Loss: 0.6418\n",
      "Epoch 102/119, Loss: 0.6415\n",
      "Epoch 103/119, Loss: 0.6403\n",
      "Epoch 104/119, Loss: 0.6396\n",
      "Epoch 105/119, Loss: 0.6392\n",
      "Epoch 106/119, Loss: 0.6384\n",
      "Epoch 107/119, Loss: 0.6381\n",
      "Epoch 108/119, Loss: 0.6373\n",
      "Epoch 109/119, Loss: 0.6372\n",
      "Epoch 110/119, Loss: 0.6361\n",
      "Validation Accuracy: 0.49943548387096776\n",
      "Epoch 111/119, Loss: 0.6358\n",
      "Epoch 112/119, Loss: 0.6355\n",
      "Epoch 113/119, Loss: 0.6345\n",
      "Epoch 114/119, Loss: 0.6337\n",
      "Epoch 115/119, Loss: 0.6329\n",
      "Epoch 116/119, Loss: 0.6326\n",
      "Epoch 117/119, Loss: 0.6318\n",
      "Epoch 118/119, Loss: 0.6313\n",
      "Epoch 119/119, Loss: 0.6309\n",
      "\n",
      "Trained in 119 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.4993\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=0.0001002440615545309\n",
      "hidden size=549\n",
      "depth=2\n",
      "batch_size=3476\n",
      "epochs=119\n",
      "decay=1.1014963169863628e-07\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=1.770720197227023e-05\n",
      "hidden size=1338\n",
      "depth=2.0\n",
      "batch_size=2384\n",
      "epochs=129\n",
      "decay=0.09398236602192603\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/129, Loss: 0.6978\n",
      "Epoch 2/129, Loss: 0.6963\n",
      "Epoch 3/129, Loss: 0.6954\n",
      "Epoch 4/129, Loss: 0.6949\n",
      "Epoch 5/129, Loss: 0.6945\n",
      "Epoch 6/129, Loss: 0.6943\n",
      "Epoch 7/129, Loss: 0.6940\n",
      "Epoch 8/129, Loss: 0.6939\n",
      "Epoch 9/129, Loss: 0.6937\n",
      "Epoch 10/129, Loss: 0.6936\n",
      "Validation Accuracy: 0.49997983870967744\n",
      "Epoch 11/129, Loss: 0.6935\n",
      "Epoch 12/129, Loss: 0.6935\n",
      "Epoch 13/129, Loss: 0.6934\n",
      "Epoch 14/129, Loss: 0.6934\n",
      "Epoch 15/129, Loss: 0.6933\n",
      "Epoch 16/129, Loss: 0.6933\n",
      "Epoch 17/129, Loss: 0.6932\n",
      "Epoch 18/129, Loss: 0.6932\n",
      "Epoch 19/129, Loss: 0.6932\n",
      "Epoch 20/129, Loss: 0.6932\n",
      "Validation Accuracy: 0.5009475806451613\n",
      "Epoch 21/129, Loss: 0.6931\n",
      "Epoch 22/129, Loss: 0.6932\n",
      "Epoch 23/129, Loss: 0.6931\n",
      "Epoch 24/129, Loss: 0.6931\n",
      "Epoch 25/129, Loss: 0.6931\n",
      "Epoch 26/129, Loss: 0.6931\n",
      "Epoch 27/129, Loss: 0.6931\n",
      "Epoch 28/129, Loss: 0.6931\n",
      "Epoch 29/129, Loss: 0.6931\n",
      "Epoch 30/129, Loss: 0.6931\n",
      "Validation Accuracy: 0.5018346774193548\n",
      "Epoch 31/129, Loss: 0.6931\n",
      "Epoch 32/129, Loss: 0.6931\n",
      "Epoch 33/129, Loss: 0.6931\n",
      "Epoch 34/129, Loss: 0.6931\n",
      "Epoch 35/129, Loss: 0.6931\n",
      "Epoch 36/129, Loss: 0.6931\n",
      "Epoch 37/129, Loss: 0.6931\n",
      "Epoch 38/129, Loss: 0.6931\n",
      "Epoch 39/129, Loss: 0.6931\n",
      "Epoch 40/129, Loss: 0.6931\n",
      "Validation Accuracy: 0.4999193548387097\n",
      "Epoch 41/129, Loss: 0.6931\n",
      "Epoch 42/129, Loss: 0.6931\n",
      "Epoch 43/129, Loss: 0.6931\n",
      "Epoch 44/129, Loss: 0.6931\n",
      "Epoch 45/129, Loss: 0.6931\n",
      "Epoch 46/129, Loss: 0.6931\n",
      "Epoch 47/129, Loss: 0.6931\n",
      "Epoch 48/129, Loss: 0.6931\n",
      "Epoch 49/129, Loss: 0.6931\n",
      "Epoch 50/129, Loss: 0.6931\n",
      "Validation Accuracy: 0.4980241935483871\n",
      "Epoch 51/129, Loss: 0.6931\n",
      "Epoch 52/129, Loss: 0.6931\n",
      "Epoch 53/129, Loss: 0.6931\n",
      "Epoch 54/129, Loss: 0.6931\n",
      "Epoch 55/129, Loss: 0.6931\n",
      "Epoch 56/129, Loss: 0.6931\n",
      "Epoch 57/129, Loss: 0.6931\n",
      "Epoch 58/129, Loss: 0.6931\n",
      "Epoch 59/129, Loss: 0.6931\n",
      "Epoch 60/129, Loss: 0.6931\n",
      "Validation Accuracy: 0.5\n",
      "Epoch 61/129, Loss: 0.6931\n",
      "Epoch 62/129, Loss: 0.6931\n",
      "Epoch 63/129, Loss: 0.6931\n",
      "Epoch 64/129, Loss: 0.6931\n",
      "Epoch 65/129, Loss: 0.6931\n",
      "Epoch 66/129, Loss: 0.6931\n",
      "Epoch 67/129, Loss: 0.6931\n",
      "Epoch 68/129, Loss: 0.6931\n",
      "Epoch 69/129, Loss: 0.6931\n",
      "Epoch 70/129, Loss: 0.6931\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 71/129, Loss: 0.6931\n",
      "Epoch 72/129, Loss: 0.6931\n",
      "Epoch 73/129, Loss: 0.6931\n",
      "Epoch 74/129, Loss: 0.6931\n",
      "Epoch 75/129, Loss: 0.6931\n",
      "Epoch 76/129, Loss: 0.6931\n",
      "Epoch 77/129, Loss: 0.6931\n",
      "Epoch 78/129, Loss: 0.6931\n",
      "Epoch 79/129, Loss: 0.6931\n",
      "Epoch 80/129, Loss: 0.6931\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 81/129, Loss: 0.6931\n",
      "Epoch 82/129, Loss: 0.6931\n",
      "Epoch 83/129, Loss: 0.6931\n",
      "Epoch 84/129, Loss: 0.6931\n",
      "Epoch 85/129, Loss: 0.6931\n",
      "Epoch 86/129, Loss: 0.6931\n",
      "Epoch 87/129, Loss: 0.6931\n",
      "Epoch 88/129, Loss: 0.6931\n",
      "Epoch 89/129, Loss: 0.6931\n",
      "Epoch 90/129, Loss: 0.6931\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 91/129, Loss: 0.6931\n",
      "Epoch 92/129, Loss: 0.6931\n",
      "Epoch 93/129, Loss: 0.6931\n",
      "Epoch 94/129, Loss: 0.6931\n",
      "Epoch 95/129, Loss: 0.6931\n",
      "Epoch 96/129, Loss: 0.6931\n",
      "Epoch 97/129, Loss: 0.6931\n",
      "Epoch 98/129, Loss: 0.6931\n",
      "Epoch 99/129, Loss: 0.6931\n",
      "Epoch 100/129, Loss: 0.6931\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 101/129, Loss: 0.6931\n",
      "Epoch 102/129, Loss: 0.6931\n",
      "Epoch 103/129, Loss: 0.6931\n",
      "Epoch 104/129, Loss: 0.6931\n",
      "Epoch 105/129, Loss: 0.6931\n",
      "Epoch 106/129, Loss: 0.6931\n",
      "Epoch 107/129, Loss: 0.6931\n",
      "Epoch 108/129, Loss: 0.6931\n",
      "Epoch 109/129, Loss: 0.6931\n",
      "Epoch 110/129, Loss: 0.6931\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 111/129, Loss: 0.6931\n",
      "Epoch 112/129, Loss: 0.6931\n",
      "Epoch 113/129, Loss: 0.6931\n",
      "Epoch 114/129, Loss: 0.6931\n",
      "Epoch 115/129, Loss: 0.6931\n",
      "Epoch 116/129, Loss: 0.6931\n",
      "Epoch 117/129, Loss: 0.6931\n",
      "Epoch 118/129, Loss: 0.6931\n",
      "Epoch 119/129, Loss: 0.6931\n",
      "Epoch 120/129, Loss: 0.6931\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 121/129, Loss: 0.6931\n",
      "Epoch 122/129, Loss: 0.6931\n",
      "Epoch 123/129, Loss: 0.6931\n",
      "Epoch 124/129, Loss: 0.6931\n",
      "Epoch 125/129, Loss: 0.6931\n",
      "Epoch 126/129, Loss: 0.6931\n",
      "Epoch 127/129, Loss: 0.6931\n",
      "Epoch 128/129, Loss: 0.6931\n",
      "Epoch 129/129, Loss: 0.6931\n",
      "\n",
      "Trained in 129 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.4998\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=1.770720197227023e-05\n",
      "hidden size=1338\n",
      "depth=2\n",
      "batch_size=2384\n",
      "epochs=129\n",
      "decay=0.09398236602192603\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.0010376209666348077\n",
      "hidden size=1326\n",
      "depth=2.0\n",
      "batch_size=66311\n",
      "epochs=145\n",
      "decay=5.3262807979890573e-05\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/145, Loss: 1.0934\n",
      "Epoch 2/145, Loss: 0.9615\n",
      "Epoch 3/145, Loss: 0.8445\n",
      "Epoch 4/145, Loss: 0.7159\n",
      "Epoch 5/145, Loss: 0.7587\n",
      "Epoch 6/145, Loss: 0.7058\n",
      "Epoch 7/145, Loss: 0.7091\n",
      "Epoch 8/145, Loss: 0.7126\n",
      "Epoch 9/145, Loss: 0.6955\n",
      "Epoch 10/145, Loss: 0.6981\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 11/145, Loss: 0.7005\n",
      "Epoch 12/145, Loss: 0.6951\n",
      "Epoch 13/145, Loss: 0.6939\n",
      "Epoch 14/145, Loss: 0.6955\n",
      "Epoch 15/145, Loss: 0.6946\n",
      "Epoch 16/145, Loss: 0.6932\n",
      "Epoch 17/145, Loss: 0.6935\n",
      "Epoch 18/145, Loss: 0.6938\n",
      "Epoch 19/145, Loss: 0.6933\n",
      "Epoch 20/145, Loss: 0.6930\n",
      "Validation Accuracy: 0.49848790322580644\n",
      "Epoch 21/145, Loss: 0.6932\n",
      "Epoch 22/145, Loss: 0.6931\n",
      "Epoch 23/145, Loss: 0.6929\n",
      "Epoch 24/145, Loss: 0.6929\n",
      "Epoch 25/145, Loss: 0.6929\n",
      "Epoch 26/145, Loss: 0.6928\n",
      "Epoch 27/145, Loss: 0.6928\n",
      "Epoch 28/145, Loss: 0.6928\n",
      "Epoch 29/145, Loss: 0.6927\n",
      "Epoch 30/145, Loss: 0.6927\n",
      "Validation Accuracy: 0.49921370967741935\n",
      "Epoch 31/145, Loss: 0.6927\n",
      "Epoch 32/145, Loss: 0.6926\n",
      "Epoch 33/145, Loss: 0.6926\n",
      "Epoch 34/145, Loss: 0.6926\n",
      "Epoch 35/145, Loss: 0.6925\n",
      "Epoch 36/145, Loss: 0.6925\n",
      "Epoch 37/145, Loss: 0.6924\n",
      "Epoch 38/145, Loss: 0.6924\n",
      "Epoch 39/145, Loss: 0.6924\n",
      "Epoch 40/145, Loss: 0.6923\n",
      "Validation Accuracy: 0.500383064516129\n",
      "Epoch 41/145, Loss: 0.6923\n",
      "Epoch 42/145, Loss: 0.6922\n",
      "Epoch 43/145, Loss: 0.6922\n",
      "Epoch 44/145, Loss: 0.6921\n",
      "Epoch 45/145, Loss: 0.6921\n",
      "Epoch 46/145, Loss: 0.6920\n",
      "Epoch 47/145, Loss: 0.6919\n",
      "Epoch 48/145, Loss: 0.6918\n",
      "Epoch 49/145, Loss: 0.6918\n",
      "Epoch 50/145, Loss: 0.6917\n",
      "Validation Accuracy: 0.5002217741935484\n",
      "Epoch 51/145, Loss: 0.6915\n",
      "Epoch 52/145, Loss: 0.6914\n",
      "Epoch 53/145, Loss: 0.6913\n",
      "Epoch 54/145, Loss: 0.6912\n",
      "Epoch 55/145, Loss: 0.6911\n",
      "Epoch 56/145, Loss: 0.6909\n",
      "Epoch 57/145, Loss: 0.6908\n",
      "Epoch 58/145, Loss: 0.6907\n",
      "Epoch 59/145, Loss: 0.6907\n",
      "Epoch 60/145, Loss: 0.6905\n",
      "Validation Accuracy: 0.5007862903225806\n",
      "Epoch 61/145, Loss: 0.6904\n",
      "Epoch 62/145, Loss: 0.6902\n",
      "Epoch 63/145, Loss: 0.6901\n",
      "Epoch 64/145, Loss: 0.6899\n",
      "Epoch 65/145, Loss: 0.6897\n",
      "Epoch 66/145, Loss: 0.6895\n",
      "Epoch 67/145, Loss: 0.6893\n",
      "Epoch 68/145, Loss: 0.6892\n",
      "Epoch 69/145, Loss: 0.6891\n",
      "Epoch 70/145, Loss: 0.6891\n",
      "Validation Accuracy: 0.5013508064516129\n",
      "Epoch 71/145, Loss: 0.6888\n",
      "Epoch 72/145, Loss: 0.6885\n",
      "Epoch 73/145, Loss: 0.6883\n",
      "Epoch 74/145, Loss: 0.6882\n",
      "Epoch 75/145, Loss: 0.6879\n",
      "Epoch 76/145, Loss: 0.6876\n",
      "Epoch 77/145, Loss: 0.6876\n",
      "Epoch 78/145, Loss: 0.6875\n",
      "Epoch 79/145, Loss: 0.6872\n",
      "Epoch 80/145, Loss: 0.6867\n",
      "Validation Accuracy: 0.5016935483870968\n",
      "Epoch 81/145, Loss: 0.6865\n",
      "Epoch 82/145, Loss: 0.6861\n",
      "Epoch 83/145, Loss: 0.6860\n",
      "Epoch 84/145, Loss: 0.6857\n",
      "Epoch 85/145, Loss: 0.6854\n",
      "Epoch 86/145, Loss: 0.6851\n",
      "Epoch 87/145, Loss: 0.6849\n",
      "Epoch 88/145, Loss: 0.6855\n",
      "Epoch 89/145, Loss: 0.6846\n",
      "Epoch 90/145, Loss: 0.6842\n",
      "Validation Accuracy: 0.501008064516129\n",
      "Epoch 91/145, Loss: 0.6844\n",
      "Epoch 92/145, Loss: 0.6842\n",
      "Epoch 93/145, Loss: 0.6837\n",
      "Epoch 94/145, Loss: 0.6831\n",
      "Epoch 95/145, Loss: 0.6832\n",
      "Epoch 96/145, Loss: 0.6825\n",
      "Epoch 97/145, Loss: 0.6823\n",
      "Epoch 98/145, Loss: 0.6819\n",
      "Epoch 99/145, Loss: 0.6816\n",
      "Epoch 100/145, Loss: 0.6813\n",
      "Validation Accuracy: 0.5009072580645161\n",
      "Epoch 101/145, Loss: 0.6811\n",
      "Epoch 102/145, Loss: 0.6809\n",
      "Epoch 103/145, Loss: 0.6809\n",
      "Epoch 104/145, Loss: 0.6802\n",
      "Epoch 105/145, Loss: 0.6799\n",
      "Epoch 106/145, Loss: 0.6802\n",
      "Epoch 107/145, Loss: 0.6793\n",
      "Epoch 108/145, Loss: 0.6797\n",
      "Epoch 109/145, Loss: 0.6793\n",
      "Epoch 110/145, Loss: 0.6803\n",
      "Validation Accuracy: 0.5024395161290323\n",
      "Epoch 111/145, Loss: 0.6789\n",
      "Epoch 112/145, Loss: 0.6783\n",
      "Epoch 113/145, Loss: 0.6779\n",
      "Epoch 114/145, Loss: 0.6777\n",
      "Epoch 115/145, Loss: 0.6776\n",
      "Epoch 116/145, Loss: 0.6773\n",
      "Epoch 117/145, Loss: 0.6774\n",
      "Epoch 118/145, Loss: 0.6768\n",
      "Epoch 119/145, Loss: 0.6762\n",
      "Epoch 120/145, Loss: 0.6757\n",
      "Validation Accuracy: 0.5025604838709677\n",
      "Epoch 121/145, Loss: 0.6755\n",
      "Epoch 122/145, Loss: 0.6752\n",
      "Epoch 123/145, Loss: 0.6752\n",
      "Epoch 124/145, Loss: 0.6748\n",
      "Epoch 125/145, Loss: 0.6742\n",
      "Epoch 126/145, Loss: 0.6750\n",
      "Epoch 127/145, Loss: 0.6742\n",
      "Epoch 128/145, Loss: 0.6741\n",
      "Epoch 129/145, Loss: 0.6736\n",
      "Epoch 130/145, Loss: 0.6735\n",
      "Validation Accuracy: 0.5009677419354839\n",
      "Epoch 131/145, Loss: 0.6725\n",
      "Epoch 132/145, Loss: 0.6725\n",
      "Epoch 133/145, Loss: 0.6718\n",
      "Epoch 134/145, Loss: 0.6720\n",
      "Epoch 135/145, Loss: 0.6742\n",
      "Epoch 136/145, Loss: 0.6713\n",
      "Epoch 137/145, Loss: 0.6708\n",
      "Epoch 138/145, Loss: 0.6708\n",
      "Epoch 139/145, Loss: 0.6701\n",
      "Epoch 140/145, Loss: 0.6731\n",
      "Validation Accuracy: 0.5024798387096774\n",
      "Epoch 141/145, Loss: 0.6715\n",
      "Epoch 142/145, Loss: 0.6709\n",
      "Epoch 143/145, Loss: 0.6710\n",
      "Epoch 144/145, Loss: 0.6701\n",
      "Epoch 145/145, Loss: 0.6699\n",
      "\n",
      "Trained in 145 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.5007\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=0.0010376209666348077\n",
      "hidden size=1326\n",
      "depth=2\n",
      "batch_size=66311\n",
      "epochs=145\n",
      "decay=5.3262807979890573e-05\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.0003381199155346545\n",
      "hidden size=1059\n",
      "depth=2.0\n",
      "batch_size=4020\n",
      "epochs=24\n",
      "decay=0.00010135188854665643\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/24, Loss: 0.7054\n",
      "Epoch 2/24, Loss: 0.6935\n",
      "Epoch 3/24, Loss: 0.6925\n",
      "Epoch 4/24, Loss: 0.6918\n",
      "Epoch 5/24, Loss: 0.6910\n",
      "Epoch 6/24, Loss: 0.6906\n",
      "Epoch 7/24, Loss: 0.6897\n",
      "Epoch 8/24, Loss: 0.6894\n",
      "Epoch 9/24, Loss: 0.6886\n",
      "Epoch 10/24, Loss: 0.6879\n",
      "Validation Accuracy: 0.502258064516129\n",
      "Epoch 11/24, Loss: 0.6869\n",
      "Epoch 12/24, Loss: 0.6862\n",
      "Epoch 13/24, Loss: 0.6855\n",
      "Epoch 14/24, Loss: 0.6848\n",
      "Epoch 15/24, Loss: 0.6841\n",
      "Epoch 16/24, Loss: 0.6840\n",
      "Epoch 17/24, Loss: 0.6821\n",
      "Epoch 18/24, Loss: 0.6812\n",
      "Epoch 19/24, Loss: 0.6804\n",
      "Epoch 20/24, Loss: 0.6788\n",
      "Validation Accuracy: 0.504133064516129\n",
      "Epoch 21/24, Loss: 0.6785\n",
      "Epoch 22/24, Loss: 0.6772\n",
      "Epoch 23/24, Loss: 0.6763\n",
      "Epoch 24/24, Loss: 0.6753\n",
      "\n",
      "Trained in 24 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.5010\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=0.0003381199155346545\n",
      "hidden size=1059\n",
      "depth=2\n",
      "batch_size=4020\n",
      "epochs=24\n",
      "decay=0.00010135188854665643\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=1.9250661815429116e-05\n",
      "hidden size=696\n",
      "depth=2.0\n",
      "batch_size=2786\n",
      "epochs=24\n",
      "decay=1.1063027350433357e-05\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/24, Loss: 0.6958\n",
      "Epoch 2/24, Loss: 0.6943\n",
      "Epoch 3/24, Loss: 0.6938\n",
      "Epoch 4/24, Loss: 0.6936\n",
      "Epoch 5/24, Loss: 0.6932\n",
      "Epoch 6/24, Loss: 0.6931\n",
      "Epoch 7/24, Loss: 0.6930\n",
      "Epoch 8/24, Loss: 0.6928\n",
      "Epoch 9/24, Loss: 0.6926\n",
      "Epoch 10/24, Loss: 0.6924\n",
      "Validation Accuracy: 0.4994758064516129\n",
      "Epoch 11/24, Loss: 0.6923\n",
      "Epoch 12/24, Loss: 0.6921\n",
      "Epoch 13/24, Loss: 0.6920\n",
      "Epoch 14/24, Loss: 0.6918\n",
      "Epoch 15/24, Loss: 0.6917\n",
      "Epoch 16/24, Loss: 0.6916\n",
      "Epoch 17/24, Loss: 0.6915\n",
      "Epoch 18/24, Loss: 0.6913\n",
      "Epoch 19/24, Loss: 0.6912\n",
      "Epoch 20/24, Loss: 0.6911\n",
      "Validation Accuracy: 0.5002822580645161\n",
      "Epoch 21/24, Loss: 0.6910\n",
      "Epoch 22/24, Loss: 0.6909\n",
      "Epoch 23/24, Loss: 0.6907\n",
      "Epoch 24/24, Loss: 0.6906\n",
      "\n",
      "Trained in 24 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.4994\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=1.9250661815429116e-05\n",
      "hidden size=696\n",
      "depth=2\n",
      "batch_size=2786\n",
      "epochs=24\n",
      "decay=1.1063027350433357e-05\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.0022614406984891996\n",
      "hidden size=894\n",
      "depth=2.0\n",
      "batch_size=13453\n",
      "epochs=25\n",
      "decay=2.81592059727656e-06\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/25, Loss: 0.9732\n",
      "Epoch 2/25, Loss: 0.6977\n",
      "Epoch 3/25, Loss: 0.6934\n",
      "Epoch 4/25, Loss: 0.6932\n",
      "Epoch 5/25, Loss: 0.6931\n",
      "Epoch 6/25, Loss: 0.6931\n",
      "Epoch 7/25, Loss: 0.6930\n",
      "Epoch 8/25, Loss: 0.6929\n",
      "Epoch 9/25, Loss: 0.6928\n",
      "Epoch 10/25, Loss: 0.6927\n",
      "Validation Accuracy: 0.5001814516129032\n",
      "Epoch 11/25, Loss: 0.6925\n",
      "Epoch 12/25, Loss: 0.6923\n",
      "Epoch 13/25, Loss: 0.6919\n",
      "Epoch 14/25, Loss: 0.6916\n",
      "Epoch 15/25, Loss: 0.6913\n",
      "Epoch 16/25, Loss: 0.6907\n",
      "Epoch 17/25, Loss: 0.6904\n",
      "Epoch 18/25, Loss: 0.6898\n",
      "Epoch 19/25, Loss: 0.6896\n",
      "Epoch 20/25, Loss: 0.6887\n",
      "Validation Accuracy: 0.5029637096774193\n",
      "Epoch 21/25, Loss: 0.6878\n",
      "Epoch 22/25, Loss: 0.6871\n",
      "Epoch 23/25, Loss: 0.6860\n",
      "Epoch 24/25, Loss: 0.6851\n",
      "Epoch 25/25, Loss: 0.6842\n",
      "\n",
      "Trained in 25 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.5008\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=0.0022614406984891996\n",
      "hidden size=894\n",
      "depth=2\n",
      "batch_size=13453\n",
      "epochs=25\n",
      "decay=2.81592059727656e-06\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.049588893009069906\n",
      "hidden size=562\n",
      "depth=2.0\n",
      "batch_size=16954\n",
      "epochs=19\n",
      "decay=1.712350706866566e-07\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/19, Loss: 12.2432\n",
      "Epoch 2/19, Loss: 0.9336\n",
      "Epoch 3/19, Loss: 0.7478\n",
      "Epoch 4/19, Loss: 0.7122\n",
      "Epoch 5/19, Loss: 0.7001\n",
      "Epoch 6/19, Loss: 0.6956\n",
      "Epoch 7/19, Loss: 0.6947\n",
      "Epoch 8/19, Loss: 0.6942\n",
      "Epoch 9/19, Loss: 0.6939\n",
      "Epoch 10/19, Loss: 0.6937\n",
      "Validation Accuracy: 0.4975\n",
      "Epoch 11/19, Loss: 0.6935\n",
      "Epoch 12/19, Loss: 0.6936\n",
      "Epoch 13/19, Loss: 0.6934\n",
      "Epoch 14/19, Loss: 0.6932\n",
      "Epoch 15/19, Loss: 0.6937\n",
      "Epoch 16/19, Loss: 0.6935\n",
      "Epoch 17/19, Loss: 0.6931\n",
      "Epoch 18/19, Loss: 0.6933\n",
      "Epoch 19/19, Loss: 0.6937\n",
      "\n",
      "Trained in 19 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.5006\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=0.049588893009069906\n",
      "hidden size=562\n",
      "depth=2\n",
      "batch_size=16954\n",
      "epochs=19\n",
      "decay=1.712350706866566e-07\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.009607717384962942\n",
      "hidden size=1770\n",
      "depth=2.0\n",
      "batch_size=88280\n",
      "epochs=10\n",
      "decay=2.21568434110821e-06\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/10, Loss: 22.6786\n",
      "Epoch 2/10, Loss: 3.1927\n",
      "Epoch 3/10, Loss: 1.1637\n",
      "Epoch 4/10, Loss: 0.9735\n",
      "Epoch 5/10, Loss: 0.7431\n",
      "Epoch 6/10, Loss: 0.7056\n",
      "Epoch 7/10, Loss: 0.6998\n",
      "Epoch 8/10, Loss: 0.7060\n",
      "Epoch 9/10, Loss: 0.6952\n",
      "Epoch 10/10, Loss: 0.6945\n",
      "Validation Accuracy: 0.49993951612903226\n",
      "\n",
      "Trained in 10 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.4999\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=0.009607717384962942\n",
      "hidden size=1770\n",
      "depth=2\n",
      "batch_size=88280\n",
      "epochs=10\n",
      "decay=2.21568434110821e-06\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=2.2115611431023553e-05\n",
      "hidden size=524\n",
      "depth=2.0\n",
      "batch_size=97312\n",
      "epochs=21\n",
      "decay=0.08866823183334108\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/21, Loss: 0.7023\n",
      "Epoch 2/21, Loss: 0.7018\n",
      "Epoch 3/21, Loss: 0.7019\n",
      "Epoch 4/21, Loss: 0.7032\n",
      "Epoch 5/21, Loss: 0.7016\n",
      "Epoch 6/21, Loss: 0.7003\n",
      "Epoch 7/21, Loss: 0.7016\n",
      "Epoch 8/21, Loss: 0.7011\n",
      "Epoch 9/21, Loss: 0.7005\n",
      "Epoch 10/21, Loss: 0.7002\n",
      "Validation Accuracy: 0.49850806451612906\n",
      "Epoch 11/21, Loss: 0.7011\n",
      "Epoch 12/21, Loss: 0.7004\n",
      "Epoch 13/21, Loss: 0.7000\n",
      "Epoch 14/21, Loss: 0.7009\n",
      "Epoch 15/21, Loss: 0.7013\n",
      "Epoch 16/21, Loss: 0.6994\n",
      "Epoch 17/21, Loss: 0.7002\n",
      "Epoch 18/21, Loss: 0.7012\n",
      "Epoch 19/21, Loss: 0.6993\n",
      "Epoch 20/21, Loss: 0.7002\n",
      "Validation Accuracy: 0.49766129032258066\n",
      "Epoch 21/21, Loss: 0.6992\n",
      "\n",
      "Trained in 21 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.4975\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=2.2115611431023553e-05\n",
      "hidden size=524\n",
      "depth=2\n",
      "batch_size=97312\n",
      "epochs=21\n",
      "decay=0.08866823183334108\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.00012214836646817046\n",
      "hidden size=2000\n",
      "depth=2.0\n",
      "batch_size=7093\n",
      "epochs=10\n",
      "decay=3.130288820940582e-07\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/10, Loss: 0.6991\n",
      "Epoch 2/10, Loss: 0.6934\n",
      "Epoch 3/10, Loss: 0.6919\n",
      "Epoch 4/10, Loss: 0.6909\n",
      "Epoch 5/10, Loss: 0.6901\n",
      "Epoch 6/10, Loss: 0.6891\n",
      "Epoch 7/10, Loss: 0.6880\n",
      "Epoch 8/10, Loss: 0.6869\n",
      "Epoch 9/10, Loss: 0.6859\n",
      "Epoch 10/10, Loss: 0.6859\n",
      "Validation Accuracy: 0.5004233870967741\n",
      "\n",
      "Trained in 10 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.5004\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=0.00012214836646817046\n",
      "hidden size=2000\n",
      "depth=2\n",
      "batch_size=7093\n",
      "epochs=10\n",
      "decay=3.130288820940582e-07\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.002034627937704769\n",
      "hidden size=1943\n",
      "depth=2.0\n",
      "batch_size=7320\n",
      "epochs=48\n",
      "decay=1e-07\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/48, Loss: 1.1255\n",
      "Epoch 2/48, Loss: 0.6933\n",
      "Epoch 3/48, Loss: 0.6931\n",
      "Epoch 4/48, Loss: 0.6931\n",
      "Epoch 5/48, Loss: 0.6930\n",
      "Epoch 6/48, Loss: 0.6930\n",
      "Epoch 7/48, Loss: 0.6929\n",
      "Epoch 8/48, Loss: 0.6927\n",
      "Epoch 9/48, Loss: 0.6923\n",
      "Epoch 10/48, Loss: 0.6918\n",
      "Validation Accuracy: 0.4967540322580645\n",
      "Epoch 11/48, Loss: 0.6918\n",
      "Epoch 12/48, Loss: 0.6914\n",
      "Epoch 13/48, Loss: 0.6907\n",
      "Epoch 14/48, Loss: 0.6899\n",
      "Epoch 15/48, Loss: 0.6896\n",
      "Epoch 16/48, Loss: 0.6886\n",
      "Epoch 17/48, Loss: 0.6879\n",
      "Epoch 18/48, Loss: 0.6871\n",
      "Epoch 19/48, Loss: 0.6856\n",
      "Epoch 20/48, Loss: 0.6847\n",
      "Validation Accuracy: 0.5009274193548388\n",
      "Epoch 21/48, Loss: 0.6837\n",
      "Epoch 22/48, Loss: 0.6815\n",
      "Epoch 23/48, Loss: 0.6793\n",
      "Epoch 24/48, Loss: 0.6769\n",
      "Epoch 25/48, Loss: 0.6771\n",
      "Epoch 26/48, Loss: 0.6724\n",
      "Epoch 27/48, Loss: 0.6700\n",
      "Epoch 28/48, Loss: 0.6669\n",
      "Epoch 29/48, Loss: 0.6641\n",
      "Epoch 30/48, Loss: 0.6629\n",
      "Validation Accuracy: 0.5031854838709677\n",
      "Epoch 31/48, Loss: 0.6593\n",
      "Epoch 32/48, Loss: 0.6562\n",
      "Epoch 33/48, Loss: 0.6571\n",
      "Epoch 34/48, Loss: 0.6491\n",
      "Epoch 35/48, Loss: 0.6492\n",
      "Epoch 36/48, Loss: 0.6420\n",
      "Epoch 37/48, Loss: 0.6421\n",
      "Epoch 38/48, Loss: 0.6359\n",
      "Epoch 39/48, Loss: 0.6299\n",
      "Epoch 40/48, Loss: 0.6275\n",
      "Validation Accuracy: 0.5018346774193548\n",
      "Epoch 41/48, Loss: 0.6213\n",
      "Epoch 42/48, Loss: 0.6232\n",
      "Epoch 43/48, Loss: 0.6143\n",
      "Epoch 44/48, Loss: 0.6079\n",
      "Epoch 45/48, Loss: 0.6036\n",
      "Epoch 46/48, Loss: 0.5953\n",
      "Epoch 47/48, Loss: 0.5922\n",
      "Epoch 48/48, Loss: 0.5857\n",
      "\n",
      "Trained in 48 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.5037\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=0.002034627937704769\n",
      "hidden size=1943\n",
      "depth=2\n",
      "batch_size=7320\n",
      "epochs=48\n",
      "decay=1e-07\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.06055804206098276\n",
      "hidden size=562\n",
      "depth=2.0\n",
      "batch_size=6845\n",
      "epochs=164\n",
      "decay=0.09800199322399919\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/164, Loss: 1.3335\n",
      "Epoch 2/164, Loss: 0.7399\n",
      "Epoch 3/164, Loss: 0.6932\n",
      "Epoch 4/164, Loss: 0.6932\n",
      "Epoch 5/164, Loss: 0.6932\n",
      "Epoch 6/164, Loss: 0.6932\n",
      "Epoch 7/164, Loss: 0.6931\n",
      "Epoch 8/164, Loss: 0.6932\n",
      "Epoch 9/164, Loss: 0.6931\n",
      "Epoch 10/164, Loss: 0.6931\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 11/164, Loss: 0.6932\n",
      "Epoch 12/164, Loss: 0.6932\n",
      "Epoch 13/164, Loss: 0.6931\n",
      "Epoch 14/164, Loss: 0.6932\n",
      "Epoch 15/164, Loss: 0.6932\n",
      "Epoch 16/164, Loss: 0.6932\n",
      "Epoch 17/164, Loss: 0.6932\n",
      "Epoch 18/164, Loss: 0.6932\n",
      "Epoch 19/164, Loss: 0.6931\n",
      "Epoch 20/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 21/164, Loss: 0.6931\n",
      "Epoch 22/164, Loss: 0.6932\n",
      "Epoch 23/164, Loss: 0.6931\n",
      "Epoch 24/164, Loss: 0.6932\n",
      "Epoch 25/164, Loss: 0.6932\n",
      "Epoch 26/164, Loss: 0.6932\n",
      "Epoch 27/164, Loss: 0.6932\n",
      "Epoch 28/164, Loss: 0.6931\n",
      "Epoch 29/164, Loss: 0.6932\n",
      "Epoch 30/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 31/164, Loss: 0.6932\n",
      "Epoch 32/164, Loss: 0.6932\n",
      "Epoch 33/164, Loss: 0.6931\n",
      "Epoch 34/164, Loss: 0.6932\n",
      "Epoch 35/164, Loss: 0.6932\n",
      "Epoch 36/164, Loss: 0.6932\n",
      "Epoch 37/164, Loss: 0.6932\n",
      "Epoch 38/164, Loss: 0.6932\n",
      "Epoch 39/164, Loss: 0.6932\n",
      "Epoch 40/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 41/164, Loss: 0.6931\n",
      "Epoch 42/164, Loss: 0.6932\n",
      "Epoch 43/164, Loss: 0.6932\n",
      "Epoch 44/164, Loss: 0.6932\n",
      "Epoch 45/164, Loss: 0.6932\n",
      "Epoch 46/164, Loss: 0.6932\n",
      "Epoch 47/164, Loss: 0.6932\n",
      "Epoch 48/164, Loss: 0.6932\n",
      "Epoch 49/164, Loss: 0.6931\n",
      "Epoch 50/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 51/164, Loss: 0.6932\n",
      "Epoch 52/164, Loss: 0.6932\n",
      "Epoch 53/164, Loss: 0.6932\n",
      "Epoch 54/164, Loss: 0.6932\n",
      "Epoch 55/164, Loss: 0.6932\n",
      "Epoch 56/164, Loss: 0.6932\n",
      "Epoch 57/164, Loss: 0.6932\n",
      "Epoch 58/164, Loss: 0.6932\n",
      "Epoch 59/164, Loss: 0.6932\n",
      "Epoch 60/164, Loss: 0.6931\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 61/164, Loss: 0.6932\n",
      "Epoch 62/164, Loss: 0.6932\n",
      "Epoch 63/164, Loss: 0.6932\n",
      "Epoch 64/164, Loss: 0.6932\n",
      "Epoch 65/164, Loss: 0.6932\n",
      "Epoch 66/164, Loss: 0.6932\n",
      "Epoch 67/164, Loss: 0.6932\n",
      "Epoch 68/164, Loss: 0.6932\n",
      "Epoch 69/164, Loss: 0.6932\n",
      "Epoch 70/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 71/164, Loss: 0.6932\n",
      "Epoch 72/164, Loss: 0.6932\n",
      "Epoch 73/164, Loss: 0.6932\n",
      "Epoch 74/164, Loss: 0.6932\n",
      "Epoch 75/164, Loss: 0.6932\n",
      "Epoch 76/164, Loss: 0.6932\n",
      "Epoch 77/164, Loss: 0.6931\n",
      "Epoch 78/164, Loss: 0.6931\n",
      "Epoch 79/164, Loss: 0.6932\n",
      "Epoch 80/164, Loss: 0.6931\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 81/164, Loss: 0.6932\n",
      "Epoch 82/164, Loss: 0.6932\n",
      "Epoch 83/164, Loss: 0.6932\n",
      "Epoch 84/164, Loss: 0.6932\n",
      "Epoch 85/164, Loss: 0.6932\n",
      "Epoch 86/164, Loss: 0.6932\n",
      "Epoch 87/164, Loss: 0.6932\n",
      "Epoch 88/164, Loss: 0.6932\n",
      "Epoch 89/164, Loss: 0.6931\n",
      "Epoch 90/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 91/164, Loss: 0.6932\n",
      "Epoch 92/164, Loss: 0.6932\n",
      "Epoch 93/164, Loss: 0.6932\n",
      "Epoch 94/164, Loss: 0.6932\n",
      "Epoch 95/164, Loss: 0.6932\n",
      "Epoch 96/164, Loss: 0.6932\n",
      "Epoch 97/164, Loss: 0.6932\n",
      "Epoch 98/164, Loss: 0.6932\n",
      "Epoch 99/164, Loss: 0.6932\n",
      "Epoch 100/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 101/164, Loss: 0.6932\n",
      "Epoch 102/164, Loss: 0.6932\n",
      "Epoch 103/164, Loss: 0.6932\n",
      "Epoch 104/164, Loss: 0.6932\n",
      "Epoch 105/164, Loss: 0.6932\n",
      "Epoch 106/164, Loss: 0.6932\n",
      "Epoch 107/164, Loss: 0.6932\n",
      "Epoch 108/164, Loss: 0.6932\n",
      "Epoch 109/164, Loss: 0.6932\n",
      "Epoch 110/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 111/164, Loss: 0.6932\n",
      "Epoch 112/164, Loss: 0.6932\n",
      "Epoch 113/164, Loss: 0.6932\n",
      "Epoch 114/164, Loss: 0.6932\n",
      "Epoch 115/164, Loss: 0.6932\n",
      "Epoch 116/164, Loss: 0.6932\n",
      "Epoch 117/164, Loss: 0.6932\n",
      "Epoch 118/164, Loss: 0.6932\n",
      "Epoch 119/164, Loss: 0.6932\n",
      "Epoch 120/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 121/164, Loss: 0.6932\n",
      "Epoch 122/164, Loss: 0.6932\n",
      "Epoch 123/164, Loss: 0.6932\n",
      "Epoch 124/164, Loss: 0.6932\n",
      "Epoch 125/164, Loss: 0.6932\n",
      "Epoch 126/164, Loss: 0.6932\n",
      "Epoch 127/164, Loss: 0.6932\n",
      "Epoch 128/164, Loss: 0.6933\n",
      "Epoch 129/164, Loss: 0.6932\n",
      "Epoch 130/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 131/164, Loss: 0.6932\n",
      "Epoch 132/164, Loss: 0.6932\n",
      "Epoch 133/164, Loss: 0.6932\n",
      "Epoch 134/164, Loss: 0.6932\n",
      "Epoch 135/164, Loss: 0.6933\n",
      "Epoch 136/164, Loss: 0.6932\n",
      "Epoch 137/164, Loss: 0.6932\n",
      "Epoch 138/164, Loss: 0.6932\n",
      "Epoch 139/164, Loss: 0.6932\n",
      "Epoch 140/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 141/164, Loss: 0.6932\n",
      "Epoch 142/164, Loss: 0.6932\n",
      "Epoch 143/164, Loss: 0.6932\n",
      "Epoch 144/164, Loss: 0.6932\n",
      "Epoch 145/164, Loss: 0.6932\n",
      "Epoch 146/164, Loss: 0.6932\n",
      "Epoch 147/164, Loss: 0.6932\n",
      "Epoch 148/164, Loss: 0.6932\n",
      "Epoch 149/164, Loss: 0.6932\n",
      "Epoch 150/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 151/164, Loss: 0.6933\n",
      "Epoch 152/164, Loss: 0.6933\n",
      "Epoch 153/164, Loss: 0.6932\n",
      "Epoch 154/164, Loss: 0.6932\n",
      "Epoch 155/164, Loss: 0.6932\n",
      "Epoch 156/164, Loss: 0.6932\n",
      "Epoch 157/164, Loss: 0.6932\n",
      "Epoch 158/164, Loss: 0.6932\n",
      "Epoch 159/164, Loss: 0.6932\n",
      "Epoch 160/164, Loss: 0.6932\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 161/164, Loss: 0.6932\n",
      "Epoch 162/164, Loss: 0.6932\n",
      "Epoch 163/164, Loss: 0.6932\n",
      "Epoch 164/164, Loss: 0.6932\n",
      "\n",
      "Trained in 164 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.4998\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=0.06055804206098276\n",
      "hidden size=562\n",
      "depth=2\n",
      "batch_size=6845\n",
      "epochs=164\n",
      "decay=0.09800199322399919\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=0.07842482054864003\n",
      "hidden size=1911\n",
      "depth=2.0\n",
      "batch_size=18514\n",
      "epochs=67\n",
      "decay=0.05861933390551577\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/67, Loss: 64.9443\n",
      "Epoch 2/67, Loss: 2.6179\n",
      "Epoch 3/67, Loss: 4.2762\n",
      "Epoch 4/67, Loss: 2.1467\n",
      "Epoch 5/67, Loss: 1.6415\n",
      "Epoch 6/67, Loss: 1.7149\n",
      "Epoch 7/67, Loss: 1.2004\n",
      "Epoch 8/67, Loss: 1.5774\n",
      "Epoch 9/67, Loss: 1.9756\n",
      "Epoch 10/67, Loss: 1.2766\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 11/67, Loss: 1.7310\n",
      "Epoch 12/67, Loss: 1.5118\n",
      "Epoch 13/67, Loss: 1.3725\n",
      "Epoch 14/67, Loss: 1.6696\n",
      "Epoch 15/67, Loss: 1.1548\n",
      "Epoch 16/67, Loss: 1.2761\n",
      "Epoch 17/67, Loss: 1.3945\n",
      "Epoch 18/67, Loss: 1.2585\n",
      "Epoch 19/67, Loss: 2.1227\n",
      "Epoch 20/67, Loss: 1.2534\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 21/67, Loss: 2.4013\n",
      "Epoch 22/67, Loss: 1.7114\n",
      "Epoch 23/67, Loss: 1.9463\n",
      "Epoch 24/67, Loss: 1.8105\n",
      "Epoch 25/67, Loss: 2.1058\n",
      "Epoch 26/67, Loss: 2.3319\n",
      "Epoch 27/67, Loss: 2.7655\n",
      "Epoch 28/67, Loss: 2.9860\n",
      "Epoch 29/67, Loss: 2.1197\n",
      "Epoch 30/67, Loss: 2.7542\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 31/67, Loss: 2.6215\n",
      "Epoch 32/67, Loss: 2.9464\n",
      "Epoch 33/67, Loss: 2.5312\n",
      "Epoch 34/67, Loss: 4.3084\n",
      "Epoch 35/67, Loss: 4.3526\n",
      "Epoch 36/67, Loss: 3.2184\n",
      "Epoch 37/67, Loss: 3.8307\n",
      "Epoch 38/67, Loss: 3.2753\n",
      "Epoch 39/67, Loss: 3.4113\n",
      "Epoch 40/67, Loss: 4.6546\n",
      "Validation Accuracy: 0.500241935483871\n",
      "Epoch 41/67, Loss: 3.3660\n",
      "Epoch 42/67, Loss: 3.7851\n",
      "Epoch 43/67, Loss: 4.6755\n",
      "Epoch 44/67, Loss: 2.4285\n",
      "Epoch 45/67, Loss: 1.9583\n",
      "Epoch 46/67, Loss: 2.2768\n",
      "Epoch 47/67, Loss: 1.8445\n",
      "Epoch 48/67, Loss: 2.9090\n",
      "Epoch 49/67, Loss: 2.1760\n",
      "Epoch 50/67, Loss: 3.3636\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 51/67, Loss: 2.5701\n",
      "Epoch 52/67, Loss: 2.8886\n",
      "Epoch 53/67, Loss: 2.4109\n",
      "Epoch 54/67, Loss: 4.7814\n",
      "Epoch 55/67, Loss: 3.4024\n",
      "Epoch 56/67, Loss: 5.0745\n",
      "Epoch 57/67, Loss: 2.0812\n",
      "Epoch 58/67, Loss: 7.7558\n",
      "Epoch 59/67, Loss: 2.9521\n",
      "Epoch 60/67, Loss: 2.8914\n",
      "Validation Accuracy: 0.49975806451612903\n",
      "Epoch 61/67, Loss: 5.7440\n",
      "Epoch 62/67, Loss: 2.2910\n",
      "Epoch 63/67, Loss: 3.7476\n",
      "Epoch 64/67, Loss: 3.0734\n",
      "Epoch 65/67, Loss: 2.9675\n",
      "Epoch 66/67, Loss: 1.8557\n",
      "Epoch 67/67, Loss: 2.1917\n",
      "\n",
      "Trained in 67 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.5002\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=0.07842482054864003\n",
      "hidden size=1911\n",
      "depth=2\n",
      "batch_size=18514\n",
      "epochs=67\n",
      "decay=0.05861933390551577\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=2.1299614440869092e-05\n",
      "hidden size=1797\n",
      "depth=2.0\n",
      "batch_size=2635\n",
      "epochs=10\n",
      "decay=0.07894041845604488\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/10, Loss: 0.6985\n",
      "Epoch 2/10, Loss: 0.6963\n",
      "Epoch 3/10, Loss: 0.6954\n",
      "Epoch 4/10, Loss: 0.6948\n",
      "Epoch 5/10, Loss: 0.6945\n",
      "Epoch 6/10, Loss: 0.6943\n",
      "Epoch 7/10, Loss: 0.6941\n",
      "Epoch 8/10, Loss: 0.6939\n",
      "Epoch 9/10, Loss: 0.6938\n",
      "Epoch 10/10, Loss: 0.6937\n",
      "Validation Accuracy: 0.5023790322580645\n",
      "\n",
      "Trained in 10 epochs\n",
      "\n",
      "Validation Accuracy for Fold 1: 0.5024\n",
      "\n",
      "with hyperparameters:\n",
      "\n",
      "learning rate=2.1299614440869092e-05\n",
      "hidden size=1797\n",
      "depth=2\n",
      "batch_size=2635\n",
      "epochs=10\n",
      "decay=0.07894041845604488\n",
      "\n",
      "\n",
      "\n",
      "Training with hyperparameters:\n",
      "\n",
      "learning rate=1.2617309790622315e-05\n",
      "hidden size=2000\n",
      "depth=3.0\n",
      "batch_size=87788\n",
      "epochs=76\n",
      "decay=1e-07\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 670.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 4.09 GiB is allocated by PyTorch, and 698.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 113\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# min_accuracy = min(fold_accuracies)\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# print(f\"Min Validation Accuracy: {min_accuracy:.4f} for hyperparameters:\\n\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth=2\\nbatch_size={batch_size}\\nepochs={epochs}\\n\\n\\n\")    \u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mavg_accuracy\n\u001b[1;32m--> 113\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgp_minimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmyseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m plot_convergence(result)\n\u001b[0;32m    117\u001b[0m best_params \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\skopt\\optimizer\\gp.py:281\u001b[0m, in \u001b[0;36mgp_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     base_estimator \u001b[38;5;241m=\u001b[39m cook_estimator(\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGP\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    276\u001b[0m         space\u001b[38;5;241m=\u001b[39mspace,\n\u001b[0;32m    277\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrng\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax),\n\u001b[0;32m    278\u001b[0m         noise\u001b[38;5;241m=\u001b[39mnoise,\n\u001b[0;32m    279\u001b[0m     )\n\u001b[1;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase_minimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43macq_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macq_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43macq_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macq_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_random_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_random_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_initial_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_initial_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_point_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_point_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_restarts_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43my0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\skopt\\optimizer\\base.py:332\u001b[0m, in \u001b[0;36mbase_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_calls):\n\u001b[0;32m    331\u001b[0m     next_x \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mask()\n\u001b[1;32m--> 332\u001b[0m     next_y \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m     result \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mtell(next_x, next_y)\n\u001b[0;32m    334\u001b[0m     result\u001b[38;5;241m.\u001b[39mspecs \u001b[38;5;241m=\u001b[39m specs\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\skopt\\utils.py:779\u001b[0m, in \u001b[0;36muse_named_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    776\u001b[0m arg_dict \u001b[38;5;241m=\u001b[39m {dim\u001b[38;5;241m.\u001b[39mname: value \u001b[38;5;28;01mfor\u001b[39;00m dim, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dimensions, x)}\n\u001b[0;32m    778\u001b[0m \u001b[38;5;66;03m# Call the wrapped objective function with the named arguments.\u001b[39;00m\n\u001b[1;32m--> 779\u001b[0m objective_value \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objective_value\n",
      "Cell \u001b[1;32mIn[11], line 79\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(learning_rate, hidden_size, depth, batch_size, epochs, decay)\u001b[0m\n\u001b[0;32m     76\u001b[0m g\u001b[38;5;241m.\u001b[39mmanual_seed(myseed) \n\u001b[0;32m     77\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(batch_size), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, worker_init_fn\u001b[38;5;241m=\u001b[39mseed_worker, generator\u001b[38;5;241m=\u001b[39mg)\n\u001b[1;32m---> 79\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_val_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# epochs = train_nn(model, train_loader, criterion, optimizer, epochs=epochs)\u001b[39;00m\n\u001b[0;32m     83\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mtrain_nn\u001b[1;34m(model, dataloader, criterion, optimizer, epochs, X_val, y_val)\u001b[0m\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     19\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 670.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 4.09 GiB is allocated by PyTorch, and 698.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "\n",
    "param_space = [\n",
    "    Real(1e-5, 1e-1, prior=\"log-uniform\", name=\"learning_rate\"), \n",
    "    Integer(500, 2000, prior = \"log-uniform\",name=\"hidden_size\"),              \n",
    "    Real(2,2.5, prior=\"log-uniform\" ,name=\"depth\"),                           \n",
    "    Integer(2000, 30000, prior = \"log-uniform\", name=\"batch_size\"),                   \n",
    "    Integer(10, 180,prior = \"log-uniform\", name=\"epochs\"),\n",
    "    Real(0.0000001,0.1, prior =\"log-uniform\", name=\"decay\")\n",
    "]\n",
    "\n",
    "# n=15 paramspace\n",
    "# param_space = [\n",
    "#     Real(1e-5, 1e-2, prior=\"log-uniform\", name=\"learning_rate\"),  \n",
    "#     Integer(2, 6000, prior = \"log-uniform\",name=\"hidden_size\"),                       \n",
    "#     Integer(2, 4, name=\"depth\"),                               \n",
    "#     Integer(16, 10000, prior = \"log-uniform\", name=\"batch_size\"),                         \n",
    "#     Integer(1, 500, name=\"epochs\"),                             \n",
    "# ]\n",
    "\n",
    "# best validation performance at around:\n",
    "# lr = 0.0002\n",
    "# hidden_size = 889-1200\n",
    "# depth = 2\n",
    "# batch_size = 40\n",
    "# epochs = 500\n",
    "\n",
    "\n",
    "set_seeds(myseed)\n",
    "\n",
    "krypto_n = n\n",
    "k_folds = 5\n",
    "\n",
    "@use_named_args(param_space)\n",
    "# def objective(learning_rate, hidden_size, depth, batch_size, epochs):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     model = NeuralNetwork(input_size=krypto_n, hidden_size=hidden_size, depth=depth).to(device)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     train_dataset = TensorDataset(X_train, y_train)\n",
    "#     g = torch.Generator()\n",
    "#     g.manual_seed(myseed) \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=int(batch_size), shuffle=True, worker_init_fn=seed_worker, generator=g, pin_memory=True)\n",
    "\n",
    "#     print(f\"Training with hyperparameters:\\n\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth={depth}\\nbatch_size={batch_size}\\nepochs={epochs}\\n\")\n",
    "#     train_nn(model, train_loader, criterion, optimizer, epochs=epochs, X_val= X_val, y_val = y_val)\n",
    "    \n",
    "#     accuracy = validate_nn(model, X_val, y_val)\n",
    "\n",
    "#     print(f\"\\nfor hyperparameters:\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth={depth}\\nbatch_size={batch_size}\\nepochs={epochs}\\n\\n################        Final Validation Accuracy:     {accuracy:.4f}          ##################\\n\")\n",
    "#     return -accuracy\n",
    "\n",
    "def objective(learning_rate, hidden_size, depth, batch_size, epochs, decay):\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=myseed)\n",
    "    fold_accuracies = []\n",
    "\n",
    "    print(f\"Training with hyperparameters:\\n\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth={np.floor(depth)}\\nbatch_size={batch_size}\\nepochs={epochs}\\ndecay={decay}\\n\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        # fold_start = time.perf_counter()\n",
    "        print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "\n",
    "\n",
    "        model = NeuralNetwork(input_size=krypto_n, hidden_size=hidden_size, depth=int(np.floor(depth))).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(myseed) \n",
    "        train_loader = DataLoader(train_dataset, batch_size=int(batch_size), shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "        \n",
    "        epochs = train_nn(model, train_loader, criterion, optimizer, epochs=epochs, X_val =X_val_fold, y_val = y_val_fold)\n",
    "        # epochs = train_nn(model, train_loader, criterion, optimizer, epochs=epochs)\n",
    "       \n",
    "       \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        print(f\"\\nTrained in {epochs} epochs\\n\")\n",
    "        \n",
    "        # fold_end = time.perf_counter()\n",
    "\n",
    "        accuracy = validate_nn(model, X_val_fold, y_val_fold)\n",
    "        print(f\"Validation Accuracy for Fold {fold + 1}: {accuracy:.4f}\")\n",
    "        print(f\"\\nwith hyperparameters:\\n\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth={int(np.floor(depth))}\\nbatch_size={batch_size}\\nepochs={epochs}\\ndecay={decay}\\n\\n\\n\")\n",
    "        if accuracy < 0.74:\n",
    "            return -accuracy\n",
    "        \n",
    "        # fold_duration = fold_end - fold_start\n",
    "        # if fold_duration > 80:\n",
    "            # return 1\n",
    "        \n",
    "        fold_accuracies.append(accuracy)\n",
    "    \n",
    "    avg_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "    print(f\"Average Validation Accuracy: {avg_accuracy:.4f} for hyperparameters:\\n\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth={np.floor(depth)}\\nbatch_size={batch_size}\\nepochs={epochs}\\ndecay={decay}\\n\\n\\n\")\n",
    "\n",
    "\n",
    "    # min_accuracy = min(fold_accuracies)\n",
    "    # print(f\"Min Validation Accuracy: {min_accuracy:.4f} for hyperparameters:\\n\\nlearning rate={learning_rate}\\nhidden size={hidden_size}\\ndepth=2\\nbatch_size={batch_size}\\nepochs={epochs}\\n\\n\\n\")    \n",
    "\n",
    "    return -avg_accuracy\n",
    "\n",
    "\n",
    "\n",
    "result = gp_minimize(objective, param_space, n_calls=60, random_state=myseed)\n",
    "\n",
    "plot_convergence(result)\n",
    "\n",
    "best_params = result.x\n",
    "\n",
    "# emissions: float = tracker.stop()\n",
    "# print(emissions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/86, Loss: 0.8068\n",
      "Epoch 2/86, Loss: 0.6932\n",
      "Epoch 3/86, Loss: 0.6932\n",
      "Epoch 4/86, Loss: 0.6932\n",
      "Epoch 5/86, Loss: 0.6931\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(best_params[\u001b[38;5;241m3\u001b[39m]), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# train_nn(model, train_loader, criterion, optimizer, epochs=best_params[4])\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m86\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m validate_nn(model, X_test, y_test)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy with Best Hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mtrain_nn\u001b[1;34m(model, dataloader, criterion, optimizer, epochs, X_val, y_val)\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      8\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Move data to device\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32me:\\materials\\Year 4\\autumn\\MML\\ImpCML\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_seeds(myseed)\n",
    "\n",
    "model = NeuralNetwork(input_size=krypto_n, hidden_size=best_params[1], depth=int(best_params[2])).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params[0], weight_decay=best_params[5])\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=int(best_params[3]), shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# train_nn(model, train_loader, criterion, optimizer, epochs=best_params[4])\n",
    "train_nn(model, train_loader, criterion, optimizer, epochs=best_params[4], X_val = X_test, y_val=y_test)\n",
    "\n",
    "test_accuracy = validate_nn(model, X_test, y_test)\n",
    "print(f\"Test Accuracy with Best Hyperparameters: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# set_seeds(myseed)\n",
    "# model = NeuralNetwork(input_size=n, hidden_size =514, depth=2).to(device)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0008212520954386176, weight_decay=0.0006296438079895955)\n",
    "\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=int(84), shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# # train_nn(model, train_loader, criterion, optimizer, epochs=best_params[4])\n",
    "# train_nn(model, train_loader, criterion, optimizer, epochs=500, X_val = X_test, y_val=y_test)\n",
    "\n",
    "# test_accuracy = validate_nn(model, X_test, y_test)\n",
    "# print(f\"Test Accuracy with Best Hyperparameters: {test_accuracy:.4f}\")\n",
    "\n",
    "# n=9\n",
    "# seed = 6095\n",
    "# 80/20\n",
    "# Test Accuracy with Best Hyperparameters: 0.9551\n",
    "\n",
    "# 70/30\n",
    "\n",
    "# 90/10  \n",
    "\n",
    "\n",
    "\n",
    "# seed = 42\n",
    "# Test Accuracy with Best Hyperparameters: 0.9595\n",
    "\n",
    "# seed = 81\n",
    "# Test Accuracy with Best Hyperparameters: 0.9574\n",
    "\n",
    "# seed = 180\n",
    "# Test Accuracy with Best Hyperparameters: 0.9571\n",
    "\n",
    "# seed = 4\n",
    "# Test Accuracy with Best Hyperparameters: 0.9570\n",
    "\n",
    "\n",
    "# n=12\n",
    "# seed = 6095\n",
    "# Test Accuracy with Best Hyperparameters: 0.9324\n",
    "\n",
    "# seed = 42\n",
    "# Test Accuracy with Best Hyperparameters: 0.9457\n",
    "\n",
    "# seed = 81\n",
    "# Test Accuracy with Best Hyperparameters: 0.9484\n",
    "\n",
    "# seed = 180\n",
    "# Test Accuracy with Best Hyperparameters: 0.9315\n",
    "\n",
    "# seed = 4\n",
    "# Test Accuracy with Best Hyperparameters: 0.9505\n",
    "\n",
    "\n",
    "\n",
    "# n=18 Test Accuracy with Best Hyperparameters: 0.8527\n",
    "\n",
    "\n",
    "# n=12\n",
    "# Test Accuracy with Best Hyperparameters: 0.9356\n",
    "\n",
    "\n",
    "# n=15 \n",
    "\n",
    "# [0.0008212520954386176, np.int64(514), np.int64(2), np.int64(84), np.int64(500), 0.0006296438079895955]\n",
    "\n",
    "\n",
    "# seed = 6095\n",
    "# Test Accuracy with Best Hyperparameters: 0.9408\n",
    "\n",
    "# seed = 42\n",
    "# Test Accuracy with Best Hyperparameters: 0.9466\n",
    "\n",
    "# seed = 81\n",
    "# Test Accuracy with Best Hyperparameters: 0.9420\n",
    "\n",
    "# seed = 180\n",
    "# Test Accuracy with Best Hyperparameters: 0.9379\n",
    "\n",
    "# seed = 4\n",
    "# Test Accuracy with Best Hyperparameters: 0.9380\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# n=18 \n",
    "\n",
    "# [0.005000499108369632, np.int64(1000), 2.3181051861840816, np.int64(1778), np.int64(264), 0.00036096505804747496]\n",
    "\n",
    "# seed = 6095\n",
    "# Test Accuracy with Best Hyperparameters: 0.9058\n",
    "\n",
    "# seed = 42\n",
    "\n",
    "# seed = 81\n",
    "\n",
    "# seed = 180\n",
    "\n",
    "# seed = 4\n",
    "\n",
    "\n",
    "\n",
    "# n=24\n",
    "# Test Accuracy with Best Hyperparameters: 0.8631\n",
    "\n",
    "\n",
    "\n",
    "# n=30 \n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.008349349990043525, np.int64(800), 2.7541886562632367, np.int64(2000), np.int64(86), 1e-05]\n"
     ]
    }
   ],
   "source": [
    "print(best_params)\n",
    "# plot_gaussian_process(result)\n",
    "\n",
    "\n",
    "# n = 15: best params = [0.0008422357289338771, np.int64(429), np.int64(2), np.int64(748), np.int64(79)]\n",
    "# n = 18: [0.0003161682862801257, np.int64(829), np.int64(2), np.int64(132), np.int64(500)]\n",
    "\n",
    "\n",
    "# 5-fold cross val\n",
    "# n = 9: \n",
    "# [6.892793602028987e-05, np.int64(160), np.int64(5), np.int64(238), np.int64(85)]\n",
    "\n",
    "# [0.0006529817319234669, np.int64(589), np.int64(2), np.int64(2247), np.int64(500), 0.0009065011800271416]\n",
    "\n",
    "\n",
    "\n",
    "# n=12\n",
    "# [9.850645640011949e-05, np.int64(413), np.int64(3), np.int64(975), np.int64(320)]\n",
    "\n",
    "# [0.002922344328571065, np.int64(55), np.int64(2), np.int64(53), np.int64(299), 0.0004494118722718984]\n",
    "\n",
    "\n",
    "# n=15\n",
    "# [0.0003341615530109006, np.int64(889), np.int64(2), np.int64(61), np.int64(382)]\n",
    "# [0.0004385205351420517, np.int64(966), np.int64(2), np.int64(334), np.int64(350), 0.0011896621969900364]\n",
    "# [0.0004331825561706844, np.int64(3000), np.int64(2), np.int64(55), np.int64(350), 0.0005157392861889768]\n",
    "# [5.123356347896232e-05, np.int64(2129), np.int64(2), np.int64(171), np.int64(350), 0.00042419048245948736]\n",
    "# [0.0013701186872623389, np.int64(3000), np.int64(2), np.int64(56), np.int64(231), 0.00010044878532667275]\n",
    "\n",
    "# [0.0008212520954386176, np.int64(514), np.int64(2), np.int64(84), np.int64(500), 0.0006296438079895955]\n",
    "\n",
    "# n=18:\n",
    "# [0.005000499108369632, np.int64(1000), 2.3181051861840816, np.int64(1778), np.int64(290), 0.00036096505804747496]\n",
    "\n",
    "\n",
    "# n=24:\n",
    "# [0.008349349990043525, np.int64(800), 2.7541886562632367, np.int64(2000), np.int64(86), 1e-05]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# param_grid = {\n",
    "#     'depth': [2,4,8,16],\n",
    "#     'hidden_size': [8, 16, 24, 30], \n",
    "#     'learning_rate': [0.001, 0.005, 0.01, 0.05 , 0.1], \n",
    "#     'batch_size': [16, 32, 64, 96],     \n",
    "#     'epochs': [5, 10, 20, 30]              \n",
    "# }\n",
    "\n",
    "# Dictionary of hyperparameter values to search through\n",
    "\n",
    "# param_grid =  {'depth': [2], 'hidden_size': [1295], 'learning_rate': [0.002355056560959035], 'batch_size': [2469], 'epochs': [38]}\n",
    "\n",
    "\n",
    "# # print(\"Random Search\")\n",
    "# # best_model, best_params = rand_grid_search(X_train, y_train, X_val, y_val, n)\n",
    "\n",
    "# set_seeds(myseed)\n",
    "\n",
    "# print(\"Reproducibility check\")\n",
    "# best_model, best_params = grid_search(X_train, y_train, X_val, y_val, param_grid, n)\n",
    "\n",
    "\n",
    "# Current Hyperparameters for each Kryptonite Variant:\n",
    "\n",
    "# Best Parameters: {'depth': 3, 'hidden_size': 38, 'learning_rate': 0.00141, 'batch_size': 140, 'epochs': 35}\n",
    "# Best Validation Accuracy: 0.9527777777777777\n",
    "# Best Validation Accuracy: 0.955\n",
    "# Best Validation Accuracy: 0.9546296296296296\n",
    "# Best Validation Accuracy: 0.9512962962962963\n",
    "# Best Validation Accuracy: 0.9555555555555556\n",
    "# Best Validation Accuracy: 0.9535185185185185\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test Accuracy = 0.9566666666666667\n",
    "\n",
    "# Krypto variant: 9\n",
    "\n",
    "\n",
    "# Best Parameters: {'depth': 2, 'hidden_size': 84, 'learning_rate': 0.002226459276380165, 'batch_size': 155, 'epochs': 146}\n",
    "# Best Validation Accuracy: 0.9452777777777778\n",
    "# Test Accuracy = 0.9497222222222222\n",
    "# Krypto variant: 12\n",
    "\n",
    "\n",
    "# Best Parameters: {'depth': 2, 'hidden_size': 54, 'learning_rate': 0.002077090842646019, 'batch_size': 147, 'epochs': 165}\n",
    "# Best Validation Accuracy: 0.9342222222222222\n",
    "# Test Accuracy = 0.9304444444444444\n",
    "# Krypto variant: 15\n",
    "\n",
    "# Current best accuracy: 0.9048\n",
    "# Best Parameters: {'depth': 2, 'hidden_size': 97, 'learning_rate': 0.002937306209994177, 'batch_size': 100, 'epochs': 72}\n",
    "# Best Validation Accuracy: 0.9048148148148148\n",
    "# Test Accuracy = 0.9057407407407407\n",
    "# Krypto variant: 18\n",
    "\n",
    "# Training with depth=2, hidden_size=84, learning_rate=0.0008952525037794653, batch_size=155, epochs=146\n",
    "# Validation Accuracy: 0.9427\n",
    "# Krypto variant: 15\n",
    "\n",
    "# Best Parameters: {'depth': 2, 'hidden_size': 150, 'learning_rate': 0.0007625120811532073, 'batch_size': 443, 'epochs': 45}\n",
    "# Best Validation Accuracy: 0.86016\n",
    "# Test Accuracy = 0.8596933333333333\n",
    "# Krypto variant: 24\n",
    "\n",
    "# Best Parameters: {'depth': 2, 'hidden_size': 388, 'learning_rate': 0.000522646472592812, 'batch_size': 485, 'epochs': 156}\n",
    "# Best Validation Accuracy: 0.9032266666666666\n",
    "# Best Validation Accuracy: 0.8842933333333334\n",
    "# Best Validation Accuracy: 0.81512\n",
    "# Best Validation Accuracy: 0.8572266666666667\n",
    "# Best Validation Accuracy: 0.81512\n",
    "# Best Validation Accuracy: 0.81512\n",
    "# Best Validation Accuracy: 0.9123066666666667\n",
    "# Best Validation Accuracy: 0.84404\n",
    "\n",
    "\n",
    "# Krypto variant: 24\n",
    "\n",
    "# NEW DATA:\n",
    "# Training with depth=2, hidden_size=685, learning_rate=0.0026145410706330304, batch_size=4500, epochs=51\n",
    "# Validation Accuracy: 0.8118\n",
    "\n",
    "\n",
    "\n",
    "# Best Validation Accuracy: 0.5041834451901566\n",
    "# Best Validation Accuracy: 0.7748322147651007\n",
    "# Best Validation Accuracy: 0.8106263982102908\n",
    "# Best Validation Accuracy: 0.7934451901565995\n",
    "# Best Validation Accuracy: 0.8178635346756152\n",
    "# Best Validation Accuracy: 0.7923266219239373\n",
    "# Best Validation Accuracy: 0.7840268456375838\n",
    "# Best Validation Accuracy: 0.7840268456375838\n",
    "# Best Validation Accuracy: 0.7840268456375838\n",
    "# Best Validation Accuracy: 0.7840268456375838\n",
    " \n",
    "# Training with depth=2, hidden_size=1805, learning_rate=0.0026249175734212675, batch_size=4445, epochs=44\n",
    "\n",
    "# 0.8185\n",
    "\n",
    "\n",
    "# Best Validation Accuracy: 0.7513758389261745\n",
    "# Best Validation Accuracy: 0.7513758389261745\n",
    "\n",
    "\n",
    "\n",
    "# Best Parameters: {'depth': 2, 'hidden_size': 951, 'learning_rate': 0.0018208178202594196, 'batch_size': 2989, 'epochs': 30}\n",
    "# Best Validation Accuracy: 0.7789597315436242\n",
    "\n",
    "# Best Parameters: {'depth': 2, 'hidden_size': 951, 'learning_rate': 0.0018208178202594196, 'batch_size': 2989, 'epochs': 30}\n",
    "# Best Validation Accuracy: 0.8215883668903803\n",
    "\n",
    "# Best Parameters: {'depth': 2, 'hidden_size': 951, 'learning_rate': 0.0018208178202594196, 'batch_size': 2989, 'epochs': 30}\n",
    "# Best Validation Accuracy: 0.8215883668903803\n",
    "\n",
    "# Best Parameters: {'depth': 2, 'hidden_size': 951, 'learning_rate': 0.0018208178202594196, 'batch_size': 2989, 'epochs': 30}\n",
    "# Best Validation Accuracy: 0.8215883668903803\n",
    "\n",
    "# Best Parameters: {'depth': 2, 'hidden_size': 951, 'learning_rate': 0.0018208178202594196, 'batch_size': 2989, 'epochs': 30}\n",
    "# Best Validation Accuracy: 0.789317673378076\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# Training with depth=2, hidden_size=1295, learning_rate=0.002355056560959035, batch_size=2469, epochs=38\n",
    "# Current best accuracy: 0.8386\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, test_outputs)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[1;32m---> 10\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m test_nn(\u001b[43mbest_model\u001b[49m, X_test, y_test)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "def test_nn(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device))\n",
    "        test_outputs = torch.round(torch.sigmoid(test_outputs)).cpu().numpy()\n",
    "        accuracy = accuracy_score(y_test, test_outputs)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "test_accuracy = test_nn(best_model, X_test, y_test)\n",
    "print(f\"Test Accuracy = {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
