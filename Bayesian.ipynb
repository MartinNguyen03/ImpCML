{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchbnn as bnn  # torchbnn library for BNN layers\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BayesianNN(nn.Module):\n",
    "    def __init__(self, input_dim, prior_mu, prior_sigma, layer1_units, layer2_units):\n",
    "        super(BayesianNN, self).__init__()\n",
    "        # Define prior parameters\n",
    "        prior_mu = 0.0  # Mean of the prior distribution\n",
    "        prior_sigma = 0.1  # Standard deviation of the prior distribution\n",
    "        \n",
    "        # Initialize Bayesian layers with the specified priors\n",
    "        self.fc1 = bnn.BayesLinear(prior_mu, prior_sigma, in_features=input_dim, out_features=layer1_units)\n",
    "        self.fc2 = bnn.BayesLinear(prior_mu, prior_sigma, in_features=layer1_units, out_features=layer2_units)\n",
    "        self.fc3 = bnn.BayesLinear(prior_mu, prior_sigma, in_features=layer2_units, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bayesian_nn(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)  # Shape: (batch_size, 1)\n",
    "        \n",
    "        # Ensure y_batch has the same shape as y_pred\n",
    "        y_batch = y_batch.unsqueeze(1)  # Convert y_batch shape to (batch_size, 1) if needed\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)  # Now both are of shape (batch_size, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation function\n",
    "# def evaluate_bayesian_nn(model, loader):\n",
    "#     model.eval()\n",
    "#     all_preds = []\n",
    "#     with torch.no_grad():\n",
    "#         for X_batch, _ in loader:\n",
    "#             y_pred = model(X_batch)\n",
    "#             print(\"y_pred shape: \", y_pred.shape)\n",
    "#             if y_pred.dim() > 1:  # Check if y_pred is not a scalar\n",
    "#                 all_preds.extend(y_pred.round().squeeze().cpu().numpy())\n",
    "#             else:\n",
    "#                 all_preds.append(y_pred.round().cpu().numpy())  # For scalar, append directly\n",
    "\n",
    "#     return np.array(all_preds)\n",
    "def evaluate_bayesian_nn(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            # Round predictions and move to CPU for compatibility\n",
    "            y_pred = y_pred.round().cpu().numpy()\n",
    "\n",
    "            # If y_pred is a scalar, wrap it in a list, otherwise ensure it's a list-like object\n",
    "            if np.isscalar(y_pred):\n",
    "                y_pred = [y_pred]  # Wrap scalar in a list\n",
    "            elif len(y_pred.shape) == 1:  # If it's already a 1D array, no need to squeeze\n",
    "                y_pred = y_pred.tolist()  # Convert it to a list directly\n",
    "\n",
    "            # Extend the list of predictions\n",
    "            all_preds.extend(y_pred)\n",
    "\n",
    "    return np.array(all_preds)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run BNN for each value of n\n",
    "def run_bayesian_nn(n, hyper_params):\n",
    "    # Load data\n",
    "    X = np.load(f'Datasets/kryptonite-{n}-X.npy')\n",
    "    y = np.load(f'Datasets/kryptonite-{n}-y.npy')\n",
    "    \n",
    "    # Split data into training, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Convert data to PyTorch tensors and create DataLoaders\n",
    "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32).clone().detach(), torch.tensor(y_train, dtype=torch.float32).clone().detach()\n",
    "    X_val, y_val = torch.tensor(X_val, dtype=torch.float32).clone().detach(), torch.tensor(y_val, dtype=torch.float32).clone().detach()\n",
    "    X_test, y_test = torch.tensor(X_test, dtype=torch.float32).clone().detach(), torch.tensor(y_test, dtype=torch.float32).clone().detach()\n",
    "    \n",
    "    # Convert data to PyTorch tensors and create DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=hyper_params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=hyper_params['batch_size'])\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = BayesianNN(input_dim=input_dim, prior_mu=hyper_params['prior_mu'], prior_sigma=hyper_params['prior_sigma'], layer1_units=hyper_params['layer1_units'], layer2_units=hyper_params['layer2_units'])\n",
    "\n",
    "    \n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hyper_params['learning_rate'])\n",
    "    \n",
    "    # Train the model and track training loss\n",
    "    training_losses = []\n",
    "    for epoch in range(hyper_params['epochs']):\n",
    "        train_loss = train_bayesian_nn(model, train_loader, criterion, optimizer)\n",
    "        training_losses.append(train_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validate the model\n",
    "    y_val_pred = evaluate_bayesian_nn(model, val_loader)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"Validation Accuracy for n={n}: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Test the model\n",
    "    y_test_pred = evaluate_bayesian_nn(model, test_loader)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"Test Accuracy for n={n}: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 45.2703\n",
      "Function value obtained: -0.4954\n",
      "Current minimum: -0.4954\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 3.1798\n",
      "Function value obtained: -0.4987\n",
      "Current minimum: -0.4987\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 25.9858\n",
      "Function value obtained: -0.5896\n",
      "Current minimum: -0.5896\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 46.7780\n",
      "Function value obtained: -0.9357\n",
      "Current minimum: -0.9357\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 22.9944\n",
      "Function value obtained: -0.5517\n",
      "Current minimum: -0.9357\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 186.2069\n",
      "Function value obtained: -0.5009\n",
      "Current minimum: -0.9357\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 34.8078\n",
      "Function value obtained: -0.9448\n",
      "Current minimum: -0.9448\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 13.3042\n",
      "Function value obtained: -0.9467\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 33.6050\n",
      "Function value obtained: -0.5124\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 29.6442\n",
      "Function value obtained: -0.4991\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 28.2203\n",
      "Function value obtained: -0.9333\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.0110\n",
      "Function value obtained: -0.4946\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.1894\n",
      "Function value obtained: -0.4889\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 149.6608\n",
      "Function value obtained: -0.5033\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 31.4747\n",
      "Function value obtained: -0.9367\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 10.7813\n",
      "Function value obtained: -0.5035\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "Iteration No: 17 ended. Search finished for the next optimal point.\n",
      "Time taken: 27.3250\n",
      "Function value obtained: -0.9428\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 18 started. Searching for the next optimal point.\n",
      "Iteration No: 18 ended. Search finished for the next optimal point.\n",
      "Time taken: 20.7043\n",
      "Function value obtained: -0.9391\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 19 started. Searching for the next optimal point.\n",
      "Iteration No: 19 ended. Search finished for the next optimal point.\n",
      "Time taken: 30.2469\n",
      "Function value obtained: -0.9393\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 20 started. Searching for the next optimal point.\n",
      "Iteration No: 20 ended. Search finished for the next optimal point.\n",
      "Time taken: 15.2950\n",
      "Function value obtained: -0.9474\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 51.8106\n",
      "Function value obtained: -0.9387\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 19.3265\n",
      "Function value obtained: -0.5048\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 18.3100\n",
      "Function value obtained: -0.9461\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 48.2447\n",
      "Function value obtained: -0.9444\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 39.3027\n",
      "Function value obtained: -0.4963\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 49.1896\n",
      "Function value obtained: -0.9237\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 28.1799\n",
      "Function value obtained: -0.9448\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 22.6809\n",
      "Function value obtained: -0.9369\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 28.2021\n",
      "Function value obtained: -0.5009\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 136.7941\n",
      "Function value obtained: -0.9369\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 31 started. Searching for the next optimal point.\n",
      "Iteration No: 31 ended. Search finished for the next optimal point.\n",
      "Time taken: 17.1607\n",
      "Function value obtained: -0.9396\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 32 started. Searching for the next optimal point.\n",
      "Iteration No: 32 ended. Search finished for the next optimal point.\n",
      "Time taken: 107.2140\n",
      "Function value obtained: -0.8533\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 33 started. Searching for the next optimal point.\n",
      "Iteration No: 33 ended. Search finished for the next optimal point.\n",
      "Time taken: 4.4381\n",
      "Function value obtained: -0.8909\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 34 started. Searching for the next optimal point.\n",
      "Iteration No: 34 ended. Search finished for the next optimal point.\n",
      "Time taken: 23.4330\n",
      "Function value obtained: -0.9352\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 35 started. Searching for the next optimal point.\n",
      "Iteration No: 35 ended. Search finished for the next optimal point.\n",
      "Time taken: 20.9804\n",
      "Function value obtained: -0.9409\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 36 started. Searching for the next optimal point.\n",
      "Iteration No: 36 ended. Search finished for the next optimal point.\n",
      "Time taken: 34.7749\n",
      "Function value obtained: -0.9356\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 37 started. Searching for the next optimal point.\n",
      "Iteration No: 37 ended. Search finished for the next optimal point.\n",
      "Time taken: 35.0728\n",
      "Function value obtained: -0.9361\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 38 started. Searching for the next optimal point.\n",
      "Iteration No: 38 ended. Search finished for the next optimal point.\n",
      "Time taken: 33.5791\n",
      "Function value obtained: -0.9257\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 39 started. Searching for the next optimal point.\n",
      "Iteration No: 39 ended. Search finished for the next optimal point.\n",
      "Time taken: 22.8665\n",
      "Function value obtained: -0.9393\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 40 started. Searching for the next optimal point.\n",
      "Iteration No: 40 ended. Search finished for the next optimal point.\n",
      "Time taken: 33.1200\n",
      "Function value obtained: -0.9309\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 41 started. Searching for the next optimal point.\n",
      "Iteration No: 41 ended. Search finished for the next optimal point.\n",
      "Time taken: 40.1554\n",
      "Function value obtained: -0.9081\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 42 started. Searching for the next optimal point.\n",
      "Iteration No: 42 ended. Search finished for the next optimal point.\n",
      "Time taken: 4.1893\n",
      "Function value obtained: -0.4972\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 43 started. Searching for the next optimal point.\n",
      "Iteration No: 43 ended. Search finished for the next optimal point.\n",
      "Time taken: 6.5796\n",
      "Function value obtained: -0.8578\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 44 started. Searching for the next optimal point.\n",
      "Iteration No: 44 ended. Search finished for the next optimal point.\n",
      "Time taken: 23.2279\n",
      "Function value obtained: -0.9443\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 45 started. Searching for the next optimal point.\n",
      "Iteration No: 45 ended. Search finished for the next optimal point.\n",
      "Time taken: 25.0765\n",
      "Function value obtained: -0.9450\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 46 started. Searching for the next optimal point.\n",
      "Iteration No: 46 ended. Search finished for the next optimal point.\n",
      "Time taken: 18.9683\n",
      "Function value obtained: -0.9419\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 47 started. Searching for the next optimal point.\n",
      "Iteration No: 47 ended. Search finished for the next optimal point.\n",
      "Time taken: 24.0303\n",
      "Function value obtained: -0.9424\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 48 started. Searching for the next optimal point.\n",
      "Iteration No: 48 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.9828\n",
      "Function value obtained: -0.9415\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 49 started. Searching for the next optimal point.\n",
      "Iteration No: 49 ended. Search finished for the next optimal point.\n",
      "Time taken: 33.9795\n",
      "Function value obtained: -0.9311\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 50 started. Searching for the next optimal point.\n",
      "Iteration No: 50 ended. Search finished for the next optimal point.\n",
      "Time taken: 15.5962\n",
      "Function value obtained: -0.9456\n",
      "Current minimum: -0.9474\n",
      "Best hyperparameters found:\n",
      "learning_rate: 0.003992368422485689\n",
      "batch_size: 121\n",
      "layer1_units: 73\n",
      "layer2_units: 54\n",
      "prior_mu: 0.5103986684212475\n",
      "prior_sigma: 0.29950393862614066\n",
      "epochs: 81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "space = [\n",
    "    Real(1e-5, 1e-1, name='learning_rate'),  # Learning rate\n",
    "    Integer(16, 128, name='batch_size'),     # Batch size\n",
    "    Integer(32, 128, name='layer1_units'),   # Number of units in layer 1\n",
    "    Integer(16, 64, name='layer2_units'),    # Number of units in layer 2\n",
    "    Real(0.0, 1.0, name='prior_mu'),         # Prior mean\n",
    "    Real(0.01, 0.5, name='prior_sigma'),     # Prior sigma\n",
    "    Integer(5, 200, name='epochs')         # Number of epochs\n",
    "]\n",
    "\n",
    "\n",
    "# Objective function to optimize using Gaussian Process\n",
    "@use_named_args(space)\n",
    "def objective_function(learning_rate, batch_size, layer1_units, layer2_units, prior_mu, prior_sigma, epochs, n=9):\n",
    "    # Load the data for the specific dataset\n",
    "    X = np.load(f'Datasets/kryptonite-{n}-X.npy')\n",
    "    y = np.load(f'Datasets/kryptonite-{n}-y.npy')\n",
    "    \n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)\n",
    "    X_val, _, y_val, _ = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Convert to torch tensors and create DataLoader instances\n",
    "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    batch_size = int(max(16, batch_size))\n",
    "    # DataLoader for training and validation\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    # Initialize the model with the given hyperparameters\n",
    "    model = BayesianNN(input_dim=X_train.shape[1], \n",
    "                       prior_mu=prior_mu, prior_sigma=prior_sigma, \n",
    "                       layer1_units=layer1_units, layer2_units=layer2_units)\n",
    "    \n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training the model for a fixed number of epochs\n",
    "    for epoch in range(epochs):  # Use 10 epochs for each optimization trial\n",
    "        train_bayesian_nn(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    y_val_pred = evaluate_bayesian_nn(model, val_loader)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Return the negative validation accuracy to minimize (since gp_minimize tries to minimize the objective)\n",
    "    return -val_accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Run Bayesian Optimization with Gaussian Process\n",
    "results = gp_minimize(objective_function, space, n_calls=50, random_state=42, verbose=True)\n",
    "\n",
    "# Print the best found hyperparameters\n",
    "print(\"Best hyperparameters found:\")\n",
    "for param, value in zip(space, results.x):\n",
    "    print(f\"{param.name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'use_named_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      2\u001b[0m thresholds \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.95\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m12\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.925\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m45\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.70\u001b[39m\n\u001b[0;32m     10\u001b[0m }\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Objective function to optimize using Gaussian Process\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;129m@use_named_args\u001b[39m(space)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_function_all\u001b[39m(learning_rate, batch_size, layer1_units, layer2_units, prior_mu, prior_sigma, epochs):\n\u001b[0;32m     14\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Loop over different n values\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'use_named_args' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the search space for hyperparameters\n",
    "thresholds = {\n",
    "    '9': 0.95,\n",
    "    '12': 0.925,\n",
    "    '15': 0.90,\n",
    "    '18': 0.875,\n",
    "    '24': 0.80,\n",
    "    '30': 0.75,\n",
    "    '45': 0.70\n",
    "}\n",
    "# Objective function to optimize using Gaussian Process\n",
    "@use_named_args(space)\n",
    "def objective_function_all(learning_rate, batch_size, layer1_units, layer2_units, prior_mu, prior_sigma, epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Loop over different n values\n",
    "    for n_str, threshold in thresholds.items():\n",
    "        n = int(n_str)  # Convert n to integer\n",
    "        \n",
    "        # Load the data for the specific dataset\n",
    "        X = np.load(f'Datasets/kryptonite-{n}-X.npy')\n",
    "        y = np.load(f'Datasets/kryptonite-{n}-y.npy')\n",
    "        \n",
    "        # Split the dataset into training, validation, and test sets\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)\n",
    "        X_val, _, y_val, _ = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Convert to torch tensors and create DataLoader instances\n",
    "        X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "        X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "        \n",
    "        batch_size = int(max(5, batch_size))\n",
    "        # DataLoader for training and validation\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "        \n",
    "        # Initialize the model with the given hyperparameters\n",
    "        model = BayesianNN(input_dim=X_train.shape[1], \n",
    "                           prior_mu=prior_mu, prior_sigma=prior_sigma, \n",
    "                           layer1_units=layer1_units, layer2_units=layer2_units)\n",
    "        \n",
    "        criterion = nn.BCELoss()  # Binary Cross-Entropy loss\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Training the model for a fixed number of epochs\n",
    "        for epoch in range(epochs):  # Use specified epochs for each optimization trial\n",
    "            train_bayesian_nn(model, train_loader, criterion, optimizer)\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        y_val_pred = evaluate_bayesian_nn(model, val_loader)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        \n",
    "        # Calculate the loss for this n: the absolute deviation from the target threshold\n",
    "        loss_n = abs(val_accuracy - threshold)\n",
    "        total_loss += loss_n  # Add the loss for this n to the total loss\n",
    "    \n",
    "    # Return the total loss (to be minimized)\n",
    "    return total_loss\n",
    "\n",
    "# Run Bayesian Optimization with Gaussian Process\n",
    "results = gp_minimize(objective_function_all, space, n_calls=50, random_state=42, verbose=True)\n",
    "\n",
    "# Print the best found hyperparameters\n",
    "print(\"Best hyperparameters found:\")\n",
    "for param, value in zip(space, results.x):\n",
    "    print(f\"{param.name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.7020\n",
      "Epoch 2, Train Loss: 0.6968\n",
      "Epoch 3, Train Loss: 0.6949\n",
      "Epoch 4, Train Loss: 0.6948\n",
      "Epoch 5, Train Loss: 0.6938\n",
      "Epoch 6, Train Loss: 0.6945\n",
      "Epoch 7, Train Loss: 0.6948\n",
      "Epoch 8, Train Loss: 0.6929\n",
      "Epoch 9, Train Loss: 0.6943\n",
      "Epoch 10, Train Loss: 0.6927\n",
      "Epoch 11, Train Loss: 0.6930\n",
      "Epoch 12, Train Loss: 0.6929\n",
      "Epoch 13, Train Loss: 0.6921\n",
      "Epoch 14, Train Loss: 0.6906\n",
      "Epoch 15, Train Loss: 0.6872\n",
      "Epoch 16, Train Loss: 0.6793\n",
      "Epoch 17, Train Loss: 0.6500\n",
      "Epoch 18, Train Loss: 0.5991\n",
      "Epoch 19, Train Loss: 0.5236\n",
      "Epoch 20, Train Loss: 0.4401\n",
      "Epoch 21, Train Loss: 0.3864\n",
      "Epoch 22, Train Loss: 0.3364\n",
      "Epoch 23, Train Loss: 0.3120\n",
      "Epoch 24, Train Loss: 0.2797\n",
      "Epoch 25, Train Loss: 0.2726\n",
      "Epoch 26, Train Loss: 0.2469\n",
      "Epoch 27, Train Loss: 0.2418\n",
      "Epoch 28, Train Loss: 0.2408\n",
      "Epoch 29, Train Loss: 0.2230\n",
      "Epoch 30, Train Loss: 0.2213\n",
      "Epoch 31, Train Loss: 0.2159\n",
      "Epoch 32, Train Loss: 0.2139\n",
      "Epoch 33, Train Loss: 0.2090\n",
      "Epoch 34, Train Loss: 0.2091\n",
      "Epoch 35, Train Loss: 0.2084\n",
      "Epoch 36, Train Loss: 0.2056\n",
      "Epoch 37, Train Loss: 0.2022\n",
      "Epoch 38, Train Loss: 0.1996\n",
      "Epoch 39, Train Loss: 0.1978\n",
      "Epoch 40, Train Loss: 0.1984\n",
      "Epoch 41, Train Loss: 0.1943\n",
      "Epoch 42, Train Loss: 0.1939\n",
      "Epoch 43, Train Loss: 0.1898\n",
      "Epoch 44, Train Loss: 0.1950\n",
      "Epoch 45, Train Loss: 0.1904\n",
      "Epoch 46, Train Loss: 0.1882\n",
      "Epoch 47, Train Loss: 0.1866\n",
      "Epoch 48, Train Loss: 0.1866\n",
      "Epoch 49, Train Loss: 0.1884\n",
      "Epoch 50, Train Loss: 0.1834\n",
      "Epoch 51, Train Loss: 0.1832\n",
      "Epoch 52, Train Loss: 0.1826\n",
      "Epoch 53, Train Loss: 0.1822\n",
      "Epoch 54, Train Loss: 0.1808\n",
      "Epoch 55, Train Loss: 0.1846\n",
      "Epoch 56, Train Loss: 0.1795\n",
      "Epoch 57, Train Loss: 0.1783\n",
      "Epoch 58, Train Loss: 0.1802\n",
      "Epoch 59, Train Loss: 0.1737\n",
      "Epoch 60, Train Loss: 0.1753\n",
      "Epoch 61, Train Loss: 0.1775\n",
      "Epoch 62, Train Loss: 0.1742\n",
      "Epoch 63, Train Loss: 0.1756\n",
      "Epoch 64, Train Loss: 0.1730\n",
      "Epoch 65, Train Loss: 0.1720\n",
      "Epoch 66, Train Loss: 0.1723\n",
      "Epoch 67, Train Loss: 0.1713\n",
      "Epoch 68, Train Loss: 0.1709\n",
      "Epoch 69, Train Loss: 0.1691\n",
      "Epoch 70, Train Loss: 0.1685\n",
      "Epoch 71, Train Loss: 0.1675\n",
      "Epoch 72, Train Loss: 0.1665\n",
      "Epoch 73, Train Loss: 0.1706\n",
      "Epoch 74, Train Loss: 0.1648\n",
      "Epoch 75, Train Loss: 0.1652\n",
      "Epoch 76, Train Loss: 0.1641\n",
      "Epoch 77, Train Loss: 0.1652\n",
      "Epoch 78, Train Loss: 0.1584\n",
      "Epoch 79, Train Loss: 0.1647\n",
      "Epoch 80, Train Loss: 0.1699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:11<01:11, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, Train Loss: 0.1630\n",
      "Validation Accuracy for n=9: 0.9481\n",
      "Test Accuracy for n=9: 0.9515\n",
      "Epoch 1, Train Loss: 0.7022\n",
      "Epoch 2, Train Loss: 0.6956\n",
      "Epoch 3, Train Loss: 0.6957\n",
      "Epoch 4, Train Loss: 0.6940\n",
      "Epoch 5, Train Loss: 0.6937\n",
      "Epoch 6, Train Loss: 0.6939\n",
      "Epoch 7, Train Loss: 0.6940\n",
      "Epoch 8, Train Loss: 0.6940\n",
      "Epoch 9, Train Loss: 0.6931\n",
      "Epoch 10, Train Loss: 0.6927\n",
      "Epoch 11, Train Loss: 0.6920\n",
      "Epoch 12, Train Loss: 0.6935\n",
      "Epoch 13, Train Loss: 0.6918\n",
      "Epoch 14, Train Loss: 0.6912\n",
      "Epoch 15, Train Loss: 0.6924\n",
      "Epoch 16, Train Loss: 0.6924\n",
      "Epoch 17, Train Loss: 0.6918\n",
      "Epoch 18, Train Loss: 0.6910\n",
      "Epoch 19, Train Loss: 0.6919\n",
      "Epoch 20, Train Loss: 0.6911\n",
      "Epoch 21, Train Loss: 0.6914\n",
      "Epoch 22, Train Loss: 0.6910\n",
      "Epoch 23, Train Loss: 0.6907\n",
      "Epoch 24, Train Loss: 0.6911\n",
      "Epoch 25, Train Loss: 0.6912\n",
      "Epoch 26, Train Loss: 0.6900\n",
      "Epoch 27, Train Loss: 0.6892\n",
      "Epoch 28, Train Loss: 0.6891\n",
      "Epoch 29, Train Loss: 0.6898\n",
      "Epoch 30, Train Loss: 0.6894\n",
      "Epoch 31, Train Loss: 0.6892\n",
      "Epoch 32, Train Loss: 0.6891\n",
      "Epoch 33, Train Loss: 0.6879\n",
      "Epoch 34, Train Loss: 0.6870\n",
      "Epoch 35, Train Loss: 0.6870\n",
      "Epoch 36, Train Loss: 0.6871\n",
      "Epoch 37, Train Loss: 0.6868\n",
      "Epoch 38, Train Loss: 0.6832\n",
      "Epoch 39, Train Loss: 0.6821\n",
      "Epoch 40, Train Loss: 0.6774\n",
      "Epoch 41, Train Loss: 0.6744\n",
      "Epoch 42, Train Loss: 0.6586\n",
      "Epoch 43, Train Loss: 0.6390\n",
      "Epoch 44, Train Loss: 0.6176\n",
      "Epoch 45, Train Loss: 0.5871\n",
      "Epoch 46, Train Loss: 0.5604\n",
      "Epoch 47, Train Loss: 0.5243\n",
      "Epoch 48, Train Loss: 0.4967\n",
      "Epoch 49, Train Loss: 0.4631\n",
      "Epoch 50, Train Loss: 0.4352\n",
      "Epoch 51, Train Loss: 0.4148\n",
      "Epoch 52, Train Loss: 0.4010\n",
      "Epoch 53, Train Loss: 0.3816\n",
      "Epoch 54, Train Loss: 0.3547\n",
      "Epoch 55, Train Loss: 0.3361\n",
      "Epoch 56, Train Loss: 0.3207\n",
      "Epoch 57, Train Loss: 0.3080\n",
      "Epoch 58, Train Loss: 0.2954\n",
      "Epoch 59, Train Loss: 0.2847\n",
      "Epoch 60, Train Loss: 0.2742\n",
      "Epoch 61, Train Loss: 0.2605\n",
      "Epoch 62, Train Loss: 0.2568\n",
      "Epoch 63, Train Loss: 0.2492\n",
      "Epoch 64, Train Loss: 0.2401\n",
      "Epoch 65, Train Loss: 0.2353\n",
      "Epoch 66, Train Loss: 0.2312\n",
      "Epoch 67, Train Loss: 0.2242\n",
      "Epoch 68, Train Loss: 0.2213\n",
      "Epoch 69, Train Loss: 0.2189\n",
      "Epoch 70, Train Loss: 0.2108\n",
      "Epoch 71, Train Loss: 0.2117\n",
      "Epoch 72, Train Loss: 0.2093\n",
      "Epoch 73, Train Loss: 0.2026\n",
      "Epoch 74, Train Loss: 0.1978\n",
      "Epoch 75, Train Loss: 0.2010\n",
      "Epoch 76, Train Loss: 0.2001\n",
      "Epoch 77, Train Loss: 0.1935\n",
      "Epoch 78, Train Loss: 0.1952\n",
      "Epoch 79, Train Loss: 0.1898\n",
      "Epoch 80, Train Loss: 0.1825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:27<01:10, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, Train Loss: 0.1812\n",
      "Validation Accuracy for n=12: 0.9197\n",
      "Test Accuracy for n=12: 0.9186\n",
      "Epoch 1, Train Loss: 0.6989\n",
      "Epoch 2, Train Loss: 0.6949\n",
      "Epoch 3, Train Loss: 0.6959\n",
      "Epoch 4, Train Loss: 0.6939\n",
      "Epoch 5, Train Loss: 0.6944\n",
      "Epoch 6, Train Loss: 0.6938\n",
      "Epoch 7, Train Loss: 0.6938\n",
      "Epoch 8, Train Loss: 0.6936\n",
      "Epoch 9, Train Loss: 0.6935\n",
      "Epoch 10, Train Loss: 0.6921\n",
      "Epoch 11, Train Loss: 0.6923\n",
      "Epoch 12, Train Loss: 0.6929\n",
      "Epoch 13, Train Loss: 0.6921\n",
      "Epoch 14, Train Loss: 0.6924\n",
      "Epoch 15, Train Loss: 0.6921\n",
      "Epoch 16, Train Loss: 0.6924\n",
      "Epoch 17, Train Loss: 0.6921\n",
      "Epoch 18, Train Loss: 0.6924\n",
      "Epoch 19, Train Loss: 0.6921\n",
      "Epoch 20, Train Loss: 0.6923\n",
      "Epoch 21, Train Loss: 0.6915\n",
      "Epoch 22, Train Loss: 0.6915\n",
      "Epoch 23, Train Loss: 0.6911\n",
      "Epoch 24, Train Loss: 0.6908\n",
      "Epoch 25, Train Loss: 0.6909\n",
      "Epoch 26, Train Loss: 0.6906\n",
      "Epoch 27, Train Loss: 0.6898\n",
      "Epoch 28, Train Loss: 0.6904\n",
      "Epoch 29, Train Loss: 0.6897\n",
      "Epoch 30, Train Loss: 0.6896\n",
      "Epoch 31, Train Loss: 0.6873\n",
      "Epoch 32, Train Loss: 0.6833\n",
      "Epoch 33, Train Loss: 0.6753\n",
      "Epoch 34, Train Loss: 0.6597\n",
      "Epoch 35, Train Loss: 0.6390\n",
      "Epoch 36, Train Loss: 0.6065\n",
      "Epoch 37, Train Loss: 0.5774\n",
      "Epoch 38, Train Loss: 0.5322\n",
      "Epoch 39, Train Loss: 0.4975\n",
      "Epoch 40, Train Loss: 0.4665\n",
      "Epoch 41, Train Loss: 0.4518\n",
      "Epoch 42, Train Loss: 0.4330\n",
      "Epoch 43, Train Loss: 0.4225\n",
      "Epoch 44, Train Loss: 0.4098\n",
      "Epoch 45, Train Loss: 0.3990\n",
      "Epoch 46, Train Loss: 0.3865\n",
      "Epoch 47, Train Loss: 0.3787\n",
      "Epoch 48, Train Loss: 0.3708\n",
      "Epoch 49, Train Loss: 0.3625\n",
      "Epoch 50, Train Loss: 0.3576\n",
      "Epoch 51, Train Loss: 0.3458\n",
      "Epoch 52, Train Loss: 0.3387\n",
      "Epoch 53, Train Loss: 0.3286\n",
      "Epoch 54, Train Loss: 0.3196\n",
      "Epoch 55, Train Loss: 0.3168\n",
      "Epoch 56, Train Loss: 0.3124\n",
      "Epoch 57, Train Loss: 0.3102\n",
      "Epoch 58, Train Loss: 0.3114\n",
      "Epoch 59, Train Loss: 0.3045\n",
      "Epoch 60, Train Loss: 0.3027\n",
      "Epoch 61, Train Loss: 0.3019\n",
      "Epoch 62, Train Loss: 0.2964\n",
      "Epoch 63, Train Loss: 0.2903\n",
      "Epoch 64, Train Loss: 0.2895\n",
      "Epoch 65, Train Loss: 0.2932\n",
      "Epoch 66, Train Loss: 0.2865\n",
      "Epoch 67, Train Loss: 0.2850\n",
      "Epoch 68, Train Loss: 0.2837\n",
      "Epoch 69, Train Loss: 0.2769\n",
      "Epoch 70, Train Loss: 0.2761\n",
      "Epoch 71, Train Loss: 0.2752\n",
      "Epoch 72, Train Loss: 0.2744\n",
      "Epoch 73, Train Loss: 0.2722\n",
      "Epoch 74, Train Loss: 0.2714\n",
      "Epoch 75, Train Loss: 0.2685\n",
      "Epoch 76, Train Loss: 0.2682\n",
      "Epoch 77, Train Loss: 0.2649\n",
      "Epoch 78, Train Loss: 0.2636\n",
      "Epoch 79, Train Loss: 0.2626\n",
      "Epoch 80, Train Loss: 0.2592\n",
      "Epoch 81, Train Loss: 0.2560\n",
      "Validation Accuracy for n=15: 0.8744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:47<01:07, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=15: 0.8660\n",
      "Epoch 1, Train Loss: 0.6980\n",
      "Epoch 2, Train Loss: 0.6960\n",
      "Epoch 3, Train Loss: 0.6948\n",
      "Epoch 4, Train Loss: 0.6944\n",
      "Epoch 5, Train Loss: 0.6941\n",
      "Epoch 6, Train Loss: 0.6941\n",
      "Epoch 7, Train Loss: 0.6942\n",
      "Epoch 8, Train Loss: 0.6936\n",
      "Epoch 9, Train Loss: 0.6938\n",
      "Epoch 10, Train Loss: 0.6933\n",
      "Epoch 11, Train Loss: 0.6931\n",
      "Epoch 12, Train Loss: 0.6936\n",
      "Epoch 13, Train Loss: 0.6939\n",
      "Epoch 14, Train Loss: 0.6934\n",
      "Epoch 15, Train Loss: 0.6935\n",
      "Epoch 16, Train Loss: 0.6941\n",
      "Epoch 17, Train Loss: 0.6931\n",
      "Epoch 18, Train Loss: 0.6926\n",
      "Epoch 19, Train Loss: 0.6935\n",
      "Epoch 20, Train Loss: 0.6928\n",
      "Epoch 21, Train Loss: 0.6932\n",
      "Epoch 22, Train Loss: 0.6935\n",
      "Epoch 23, Train Loss: 0.6926\n",
      "Epoch 24, Train Loss: 0.6935\n",
      "Epoch 25, Train Loss: 0.6928\n",
      "Epoch 26, Train Loss: 0.6936\n",
      "Epoch 27, Train Loss: 0.6934\n",
      "Epoch 28, Train Loss: 0.6931\n",
      "Epoch 29, Train Loss: 0.6936\n",
      "Epoch 30, Train Loss: 0.6931\n",
      "Epoch 31, Train Loss: 0.6926\n",
      "Epoch 32, Train Loss: 0.6934\n",
      "Epoch 33, Train Loss: 0.6930\n",
      "Epoch 34, Train Loss: 0.6922\n",
      "Epoch 35, Train Loss: 0.6928\n",
      "Epoch 36, Train Loss: 0.6922\n",
      "Epoch 37, Train Loss: 0.6919\n",
      "Epoch 38, Train Loss: 0.6922\n",
      "Epoch 39, Train Loss: 0.6914\n",
      "Epoch 40, Train Loss: 0.6919\n",
      "Epoch 41, Train Loss: 0.6917\n",
      "Epoch 42, Train Loss: 0.6909\n",
      "Epoch 43, Train Loss: 0.6911\n",
      "Epoch 44, Train Loss: 0.6900\n",
      "Epoch 45, Train Loss: 0.6897\n",
      "Epoch 46, Train Loss: 0.6906\n",
      "Epoch 47, Train Loss: 0.6886\n",
      "Epoch 48, Train Loss: 0.6887\n",
      "Epoch 49, Train Loss: 0.6882\n",
      "Epoch 50, Train Loss: 0.6887\n",
      "Epoch 51, Train Loss: 0.6841\n",
      "Epoch 52, Train Loss: 0.6883\n",
      "Epoch 53, Train Loss: 0.6884\n",
      "Epoch 54, Train Loss: 0.6872\n",
      "Epoch 55, Train Loss: 0.6868\n",
      "Epoch 56, Train Loss: 0.6861\n",
      "Epoch 57, Train Loss: 0.6845\n",
      "Epoch 58, Train Loss: 0.6825\n",
      "Epoch 59, Train Loss: 0.6876\n",
      "Epoch 60, Train Loss: 0.6838\n",
      "Epoch 61, Train Loss: 0.6816\n",
      "Epoch 62, Train Loss: 0.6817\n",
      "Epoch 63, Train Loss: 0.6822\n",
      "Epoch 64, Train Loss: 0.6798\n",
      "Epoch 65, Train Loss: 0.6796\n",
      "Epoch 66, Train Loss: 0.6784\n",
      "Epoch 67, Train Loss: 0.6733\n",
      "Epoch 68, Train Loss: 0.6798\n",
      "Epoch 69, Train Loss: 0.6763\n",
      "Epoch 70, Train Loss: 0.6721\n",
      "Epoch 71, Train Loss: 0.6736\n",
      "Epoch 72, Train Loss: 0.6705\n",
      "Epoch 73, Train Loss: 0.6720\n",
      "Epoch 74, Train Loss: 0.6709\n",
      "Epoch 75, Train Loss: 0.6693\n",
      "Epoch 76, Train Loss: 0.6657\n",
      "Epoch 77, Train Loss: 0.6608\n",
      "Epoch 78, Train Loss: 0.6613\n",
      "Epoch 79, Train Loss: 0.6637\n",
      "Epoch 80, Train Loss: 0.6613\n",
      "Epoch 81, Train Loss: 0.6573\n",
      "Validation Accuracy for n=18: 0.4880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [01:11<00:59, 19.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=18: 0.5055\n",
      "Epoch 1, Train Loss: 0.6978\n",
      "Epoch 2, Train Loss: 0.6955\n",
      "Epoch 3, Train Loss: 0.6942\n",
      "Epoch 4, Train Loss: 0.6936\n",
      "Epoch 5, Train Loss: 0.6937\n",
      "Epoch 6, Train Loss: 0.6939\n",
      "Epoch 7, Train Loss: 0.6932\n",
      "Epoch 8, Train Loss: 0.6934\n",
      "Epoch 9, Train Loss: 0.6936\n",
      "Epoch 10, Train Loss: 0.6938\n",
      "Epoch 11, Train Loss: 0.6933\n",
      "Epoch 12, Train Loss: 0.6932\n",
      "Epoch 13, Train Loss: 0.6933\n",
      "Epoch 14, Train Loss: 0.6937\n",
      "Epoch 15, Train Loss: 0.6933\n",
      "Epoch 16, Train Loss: 0.6932\n",
      "Epoch 17, Train Loss: 0.6934\n",
      "Epoch 18, Train Loss: 0.6933\n",
      "Epoch 19, Train Loss: 0.6933\n",
      "Epoch 20, Train Loss: 0.6932\n",
      "Epoch 21, Train Loss: 0.6932\n",
      "Epoch 22, Train Loss: 0.6933\n",
      "Epoch 23, Train Loss: 0.6933\n",
      "Epoch 24, Train Loss: 0.6933\n",
      "Epoch 25, Train Loss: 0.6932\n",
      "Epoch 26, Train Loss: 0.6932\n",
      "Epoch 27, Train Loss: 0.6934\n",
      "Epoch 28, Train Loss: 0.6933\n",
      "Epoch 29, Train Loss: 0.6933\n",
      "Epoch 30, Train Loss: 0.6933\n",
      "Epoch 31, Train Loss: 0.6933\n",
      "Epoch 32, Train Loss: 0.6932\n",
      "Epoch 33, Train Loss: 0.6932\n",
      "Epoch 34, Train Loss: 0.6932\n",
      "Epoch 35, Train Loss: 0.6933\n",
      "Epoch 36, Train Loss: 0.6932\n",
      "Epoch 37, Train Loss: 0.6936\n",
      "Epoch 38, Train Loss: 0.6933\n",
      "Epoch 39, Train Loss: 0.6932\n",
      "Epoch 40, Train Loss: 0.6932\n",
      "Epoch 41, Train Loss: 0.6936\n",
      "Epoch 42, Train Loss: 0.6933\n",
      "Epoch 43, Train Loss: 0.6932\n",
      "Epoch 44, Train Loss: 0.6932\n",
      "Epoch 45, Train Loss: 0.6932\n",
      "Epoch 46, Train Loss: 0.6931\n",
      "Epoch 47, Train Loss: 0.6933\n",
      "Epoch 48, Train Loss: 0.6932\n",
      "Epoch 49, Train Loss: 0.6932\n",
      "Epoch 50, Train Loss: 0.6932\n",
      "Epoch 51, Train Loss: 0.6933\n",
      "Epoch 52, Train Loss: 0.6932\n",
      "Epoch 53, Train Loss: 0.6932\n",
      "Epoch 54, Train Loss: 0.6934\n",
      "Epoch 55, Train Loss: 0.6932\n",
      "Epoch 56, Train Loss: 0.6932\n",
      "Epoch 57, Train Loss: 0.6932\n",
      "Epoch 58, Train Loss: 0.6932\n",
      "Epoch 59, Train Loss: 0.6932\n",
      "Epoch 60, Train Loss: 0.6932\n",
      "Epoch 61, Train Loss: 0.6932\n",
      "Epoch 62, Train Loss: 0.6932\n",
      "Epoch 63, Train Loss: 0.6932\n",
      "Epoch 64, Train Loss: 0.6932\n",
      "Epoch 65, Train Loss: 0.6932\n",
      "Epoch 66, Train Loss: 0.6932\n",
      "Epoch 67, Train Loss: 0.6932\n",
      "Epoch 68, Train Loss: 0.6932\n",
      "Epoch 69, Train Loss: 0.6932\n",
      "Epoch 70, Train Loss: 0.6932\n",
      "Epoch 71, Train Loss: 0.6932\n",
      "Epoch 72, Train Loss: 0.6932\n",
      "Epoch 73, Train Loss: 0.6932\n",
      "Epoch 74, Train Loss: 0.6932\n",
      "Epoch 75, Train Loss: 0.6932\n",
      "Epoch 76, Train Loss: 0.6932\n",
      "Epoch 77, Train Loss: 0.6932\n",
      "Epoch 78, Train Loss: 0.6932\n",
      "Epoch 79, Train Loss: 0.6933\n",
      "Epoch 80, Train Loss: 0.6932\n",
      "Epoch 81, Train Loss: 0.6932\n",
      "Validation Accuracy for n=24: 0.4987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [01:44<00:49, 24.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=24: 0.5031\n",
      "Epoch 1, Train Loss: 0.6989\n",
      "Epoch 2, Train Loss: 0.6937\n",
      "Epoch 3, Train Loss: 0.6944\n",
      "Epoch 4, Train Loss: 0.6938\n",
      "Epoch 5, Train Loss: 0.6938\n",
      "Epoch 6, Train Loss: 0.6938\n",
      "Epoch 7, Train Loss: 0.6937\n",
      "Epoch 8, Train Loss: 0.6935\n",
      "Epoch 9, Train Loss: 0.6934\n",
      "Epoch 10, Train Loss: 0.6934\n",
      "Epoch 11, Train Loss: 0.6934\n",
      "Epoch 12, Train Loss: 0.6932\n",
      "Epoch 13, Train Loss: 0.6936\n",
      "Epoch 14, Train Loss: 0.6933\n",
      "Epoch 15, Train Loss: 0.6932\n",
      "Epoch 16, Train Loss: 0.6931\n",
      "Epoch 17, Train Loss: 0.6933\n",
      "Epoch 18, Train Loss: 0.6930\n",
      "Epoch 19, Train Loss: 0.6930\n",
      "Epoch 20, Train Loss: 0.6929\n",
      "Epoch 21, Train Loss: 0.6925\n",
      "Epoch 22, Train Loss: 0.6925\n",
      "Epoch 23, Train Loss: 0.6925\n",
      "Epoch 24, Train Loss: 0.6923\n",
      "Epoch 25, Train Loss: 0.6923\n",
      "Epoch 26, Train Loss: 0.6918\n",
      "Epoch 27, Train Loss: 0.6915\n",
      "Epoch 28, Train Loss: 0.6908\n",
      "Epoch 29, Train Loss: 0.6905\n",
      "Epoch 30, Train Loss: 0.6897\n",
      "Epoch 31, Train Loss: 0.6896\n",
      "Epoch 32, Train Loss: 0.6891\n",
      "Epoch 33, Train Loss: 0.6875\n",
      "Epoch 34, Train Loss: 0.6884\n",
      "Epoch 35, Train Loss: 0.6867\n",
      "Epoch 36, Train Loss: 0.6862\n",
      "Epoch 37, Train Loss: 0.6857\n",
      "Epoch 38, Train Loss: 0.6843\n",
      "Epoch 39, Train Loss: 0.6829\n",
      "Epoch 40, Train Loss: 0.6829\n",
      "Epoch 41, Train Loss: 0.6825\n",
      "Epoch 42, Train Loss: 0.6809\n",
      "Epoch 43, Train Loss: 0.6798\n",
      "Epoch 44, Train Loss: 0.6794\n",
      "Epoch 45, Train Loss: 0.6770\n",
      "Epoch 46, Train Loss: 0.6767\n",
      "Epoch 47, Train Loss: 0.6756\n",
      "Epoch 48, Train Loss: 0.6752\n",
      "Epoch 49, Train Loss: 0.6735\n",
      "Epoch 50, Train Loss: 0.6728\n",
      "Epoch 51, Train Loss: 0.6724\n",
      "Epoch 52, Train Loss: 0.6706\n",
      "Epoch 53, Train Loss: 0.6695\n",
      "Epoch 54, Train Loss: 0.6684\n",
      "Epoch 55, Train Loss: 0.6661\n",
      "Epoch 56, Train Loss: 0.6653\n",
      "Epoch 57, Train Loss: 0.6645\n",
      "Epoch 58, Train Loss: 0.6618\n",
      "Epoch 59, Train Loss: 0.6620\n",
      "Epoch 60, Train Loss: 0.6579\n",
      "Epoch 61, Train Loss: 0.6595\n",
      "Epoch 62, Train Loss: 0.6574\n",
      "Epoch 63, Train Loss: 0.6551\n",
      "Epoch 64, Train Loss: 0.6539\n",
      "Epoch 65, Train Loss: 0.6513\n",
      "Epoch 66, Train Loss: 0.6502\n",
      "Epoch 67, Train Loss: 0.6478\n",
      "Epoch 68, Train Loss: 0.6457\n",
      "Epoch 69, Train Loss: 0.6432\n",
      "Epoch 70, Train Loss: 0.6425\n",
      "Epoch 71, Train Loss: 0.6410\n",
      "Epoch 72, Train Loss: 0.6390\n",
      "Epoch 73, Train Loss: 0.6360\n",
      "Epoch 74, Train Loss: 0.6357\n",
      "Epoch 75, Train Loss: 0.6332\n",
      "Epoch 76, Train Loss: 0.6305\n",
      "Epoch 77, Train Loss: 0.6286\n",
      "Epoch 78, Train Loss: 0.6265\n",
      "Epoch 79, Train Loss: 0.6244\n",
      "Epoch 80, Train Loss: 0.6237\n",
      "Epoch 81, Train Loss: 0.6199\n",
      "Validation Accuracy for n=30: 0.4949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [02:27<00:30, 30.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=30: 0.4998\n",
      "Epoch 1, Train Loss: 0.6969\n",
      "Epoch 2, Train Loss: 0.6941\n",
      "Epoch 3, Train Loss: 0.6935\n",
      "Epoch 4, Train Loss: 0.6936\n",
      "Epoch 5, Train Loss: 0.6935\n",
      "Epoch 6, Train Loss: 0.6936\n",
      "Epoch 7, Train Loss: 0.6934\n",
      "Epoch 8, Train Loss: 0.6934\n",
      "Epoch 9, Train Loss: 0.6933\n",
      "Epoch 10, Train Loss: 0.6933\n",
      "Epoch 11, Train Loss: 0.6933\n",
      "Epoch 12, Train Loss: 0.6933\n",
      "Epoch 13, Train Loss: 0.6932\n",
      "Epoch 14, Train Loss: 0.6933\n",
      "Epoch 15, Train Loss: 0.6934\n",
      "Epoch 16, Train Loss: 0.6933\n",
      "Epoch 17, Train Loss: 0.6932\n",
      "Epoch 18, Train Loss: 0.6934\n",
      "Epoch 19, Train Loss: 0.6932\n",
      "Epoch 20, Train Loss: 0.6933\n",
      "Epoch 21, Train Loss: 0.6932\n",
      "Epoch 22, Train Loss: 0.6932\n",
      "Epoch 23, Train Loss: 0.6933\n",
      "Epoch 24, Train Loss: 0.6932\n",
      "Epoch 25, Train Loss: 0.6932\n",
      "Epoch 26, Train Loss: 0.6932\n",
      "Epoch 27, Train Loss: 0.6933\n",
      "Epoch 28, Train Loss: 0.6933\n",
      "Epoch 29, Train Loss: 0.6932\n",
      "Epoch 30, Train Loss: 0.6932\n",
      "Epoch 31, Train Loss: 0.6932\n",
      "Epoch 32, Train Loss: 0.6933\n",
      "Epoch 33, Train Loss: 0.6932\n",
      "Epoch 34, Train Loss: 0.6932\n",
      "Epoch 35, Train Loss: 0.6932\n",
      "Epoch 36, Train Loss: 0.6932\n",
      "Epoch 37, Train Loss: 0.6932\n",
      "Epoch 38, Train Loss: 0.6932\n",
      "Epoch 39, Train Loss: 0.6932\n",
      "Epoch 40, Train Loss: 0.6932\n",
      "Epoch 41, Train Loss: 0.6933\n",
      "Epoch 42, Train Loss: 0.6934\n",
      "Epoch 43, Train Loss: 0.6932\n",
      "Epoch 44, Train Loss: 0.6933\n",
      "Epoch 45, Train Loss: 0.6932\n",
      "Epoch 46, Train Loss: 0.6932\n",
      "Epoch 47, Train Loss: 0.6932\n",
      "Epoch 48, Train Loss: 0.6932\n",
      "Epoch 49, Train Loss: 0.6932\n",
      "Epoch 50, Train Loss: 0.6932\n",
      "Epoch 51, Train Loss: 0.6934\n",
      "Epoch 52, Train Loss: 0.6933\n",
      "Epoch 53, Train Loss: 0.6932\n",
      "Epoch 54, Train Loss: 0.6932\n",
      "Epoch 55, Train Loss: 0.6932\n",
      "Epoch 56, Train Loss: 0.6932\n",
      "Epoch 57, Train Loss: 0.6932\n",
      "Epoch 58, Train Loss: 0.6932\n",
      "Epoch 59, Train Loss: 0.6932\n",
      "Epoch 60, Train Loss: 0.6932\n",
      "Epoch 61, Train Loss: 0.6932\n",
      "Epoch 62, Train Loss: 0.6932\n",
      "Epoch 63, Train Loss: 0.6932\n",
      "Epoch 64, Train Loss: 0.6932\n",
      "Epoch 65, Train Loss: 0.6932\n",
      "Epoch 66, Train Loss: 0.6932\n",
      "Epoch 67, Train Loss: 0.6932\n",
      "Epoch 68, Train Loss: 0.6932\n",
      "Epoch 69, Train Loss: 0.6933\n",
      "Epoch 70, Train Loss: 0.6932\n",
      "Epoch 71, Train Loss: 0.6932\n",
      "Epoch 72, Train Loss: 0.6932\n",
      "Epoch 73, Train Loss: 0.6932\n",
      "Epoch 74, Train Loss: 0.6932\n",
      "Epoch 75, Train Loss: 0.6932\n",
      "Epoch 76, Train Loss: 0.6932\n",
      "Epoch 77, Train Loss: 0.6932\n",
      "Epoch 78, Train Loss: 0.6932\n",
      "Epoch 79, Train Loss: 0.6932\n",
      "Epoch 80, Train Loss: 0.6932\n",
      "Epoch 81, Train Loss: 0.6932\n",
      "Validation Accuracy for n=45: 0.4994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [03:38<00:00, 31.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=45: 0.4976\n",
      "Accuracies across different n values: [(9, 0.9514814814814815), (12, 0.9186111111111112), (15, 0.866), (18, 0.505462962962963), (24, 0.5030555555555556), (30, 0.49977777777777777), (45, 0.49762962962962964)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run for each n value and collect accuracies\n",
    "results = []\n",
    "# learning_rate: 0.008705054471236994\n",
    "# batch_size: 17\n",
    "# layer1_units: 92\n",
    "# layer2_units: 52\n",
    "# prior_mu: 0.46796382181206203\n",
    "# prior_sigma: 0.4384207640007753\n",
    "# epochs: 100\n",
    "# Acc: 0.9348\n",
    "\n",
    "# learning_rate: 0.003992368422485689\n",
    "# batch_size: 121\n",
    "# layer1_units: 73\n",
    "# layer2_units: 54\n",
    "# prior_mu: 0.5103986684212475\n",
    "# prior_sigma: 0.29950393862614066\n",
    "# epochs: 81\n",
    "# Accuracies across different n values: [(9, 0.9514814814814815), (12, 0.9186111111111112), (15, 0.866), (18, 0.505462962962963), (24, 0.5030555555555556), (30, 0.49977777777777777), (45, 0.49762962962962964)]\n",
    "\n",
    "hyper_params9 = {\n",
    "    'prior_mu': 0.5103986684212475,\n",
    "    'prior_sigma': 0.29950393862614066,\n",
    "    'layer1_units': 73,\n",
    "    'layer2_units': 54,\n",
    "    'batch_size': 121,\n",
    "    'learning_rate': 0.003992368422485689,\n",
    "    'epochs': 81\n",
    "}\n",
    "possible_n_vals = [9, 12, 15, 18, 24, 30, 45]\n",
    "for n in tqdm(possible_n_vals):\n",
    "    accuracy = run_bayesian_nn(n, hyper_params9)\n",
    "    results.append((n, accuracy))\n",
    "\n",
    "print(\"Accuracies across different n values:\", results)\n",
    "\n",
    "# Threshold grid for each n value\n",
    "thresh_grid = {\n",
    "    '9': 0.95,  \n",
    "    '12': 0.925,\n",
    "    '15': 0.90,\n",
    "    '18': 0.875,\n",
    "    '24': 0.80,\n",
    "    '30': 0.75,\n",
    "    '45': 0.70\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
