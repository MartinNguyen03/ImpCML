{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchbnn as bnn  # torchbnn library for BNN layers\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BayesianNN(nn.Module):\n",
    "    def __init__(self, input_dim, prior_mu, prior_sigma, layer1_units, layer2_units):\n",
    "        super(BayesianNN, self).__init__()\n",
    "        # Define prior parameters\n",
    "        prior_mu = 0.0  # Mean of the prior distribution\n",
    "        prior_sigma = 0.1  # Standard deviation of the prior distribution\n",
    "        \n",
    "        # Initialize Bayesian layers with the specified priors\n",
    "        self.fc1 = bnn.BayesLinear(prior_mu, prior_sigma, in_features=input_dim, out_features=layer1_units)\n",
    "        self.fc2 = bnn.BayesLinear(prior_mu, prior_sigma, in_features=layer1_units, out_features=layer2_units)\n",
    "        self.fc3 = bnn.BayesLinear(prior_mu, prior_sigma, in_features=layer2_units, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bayesian_nn(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)  # Shape: (batch_size, 1)\n",
    "        \n",
    "        # Ensure y_batch has the same shape as y_pred\n",
    "        y_batch = y_batch.unsqueeze(1)  # Convert y_batch shape to (batch_size, 1) if needed\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)  # Now both are of shape (batch_size, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation function\n",
    "# def evaluate_bayesian_nn(model, loader):\n",
    "#     model.eval()\n",
    "#     all_preds = []\n",
    "#     with torch.no_grad():\n",
    "#         for X_batch, _ in loader:\n",
    "#             y_pred = model(X_batch)\n",
    "#             print(\"y_pred shape: \", y_pred.shape)\n",
    "#             if y_pred.dim() > 1:  # Check if y_pred is not a scalar\n",
    "#                 all_preds.extend(y_pred.round().squeeze().cpu().numpy())\n",
    "#             else:\n",
    "#                 all_preds.append(y_pred.round().cpu().numpy())  # For scalar, append directly\n",
    "\n",
    "#     return np.array(all_preds)\n",
    "def evaluate_bayesian_nn(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            # Round predictions and move to CPU for compatibility\n",
    "            y_pred = y_pred.round().cpu().numpy()\n",
    "\n",
    "            # If y_pred is a scalar, wrap it in a list, otherwise ensure it's a list-like object\n",
    "            if np.isscalar(y_pred):\n",
    "                y_pred = [y_pred]  # Wrap scalar in a list\n",
    "            elif len(y_pred.shape) == 1:  # If it's already a 1D array, no need to squeeze\n",
    "                y_pred = y_pred.tolist()  # Convert it to a list directly\n",
    "\n",
    "            # Extend the list of predictions\n",
    "            all_preds.extend(y_pred)\n",
    "\n",
    "    return np.array(all_preds)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run BNN for each value of n\n",
    "def run_bayesian_nn(n, hyper_params):\n",
    "    # Load data\n",
    "    X = np.load(f'Datasets/kryptonite-{n}-X.npy')\n",
    "    y = np.load(f'Datasets/kryptonite-{n}-y.npy')\n",
    "    \n",
    "    # Split data into training, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Convert data to PyTorch tensors and create DataLoaders\n",
    "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32).clone().detach(), torch.tensor(y_train, dtype=torch.float32).clone().detach()\n",
    "    X_val, y_val = torch.tensor(X_val, dtype=torch.float32).clone().detach(), torch.tensor(y_val, dtype=torch.float32).clone().detach()\n",
    "    X_test, y_test = torch.tensor(X_test, dtype=torch.float32).clone().detach(), torch.tensor(y_test, dtype=torch.float32).clone().detach()\n",
    "    \n",
    "    # Convert data to PyTorch tensors and create DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=hyper_params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=hyper_params['batch_size'])\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = BayesianNN(input_dim=input_dim, prior_mu=hyper_params['prior_mu'], prior_sigma=hyper_params['prior_sigma'], layer1_units=hyper_params['layer1_units'], layer2_units=hyper_params['layer2_units'])\n",
    "\n",
    "    \n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hyper_params['learning_rate'])\n",
    "    \n",
    "    # Train the model and track training loss\n",
    "    training_losses = []\n",
    "    for epoch in range(hyper_params['epochs']):\n",
    "        train_loss = train_bayesian_nn(model, train_loader, criterion, optimizer)\n",
    "        training_losses.append(train_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validate the model\n",
    "    y_val_pred = evaluate_bayesian_nn(model, val_loader)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"Validation Accuracy for n={n}: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Test the model\n",
    "    y_test_pred = evaluate_bayesian_nn(model, test_loader)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"Test Accuracy for n={n}: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 45.2703\n",
      "Function value obtained: -0.4954\n",
      "Current minimum: -0.4954\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 3.1798\n",
      "Function value obtained: -0.4987\n",
      "Current minimum: -0.4987\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 25.9858\n",
      "Function value obtained: -0.5896\n",
      "Current minimum: -0.5896\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 46.7780\n",
      "Function value obtained: -0.9357\n",
      "Current minimum: -0.9357\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 22.9944\n",
      "Function value obtained: -0.5517\n",
      "Current minimum: -0.9357\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 186.2069\n",
      "Function value obtained: -0.5009\n",
      "Current minimum: -0.9357\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 34.8078\n",
      "Function value obtained: -0.9448\n",
      "Current minimum: -0.9448\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 13.3042\n",
      "Function value obtained: -0.9467\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 33.6050\n",
      "Function value obtained: -0.5124\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 29.6442\n",
      "Function value obtained: -0.4991\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 28.2203\n",
      "Function value obtained: -0.9333\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.0110\n",
      "Function value obtained: -0.4946\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.1894\n",
      "Function value obtained: -0.4889\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 149.6608\n",
      "Function value obtained: -0.5033\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 31.4747\n",
      "Function value obtained: -0.9367\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 10.7813\n",
      "Function value obtained: -0.5035\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "Iteration No: 17 ended. Search finished for the next optimal point.\n",
      "Time taken: 27.3250\n",
      "Function value obtained: -0.9428\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 18 started. Searching for the next optimal point.\n",
      "Iteration No: 18 ended. Search finished for the next optimal point.\n",
      "Time taken: 20.7043\n",
      "Function value obtained: -0.9391\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 19 started. Searching for the next optimal point.\n",
      "Iteration No: 19 ended. Search finished for the next optimal point.\n",
      "Time taken: 30.2469\n",
      "Function value obtained: -0.9393\n",
      "Current minimum: -0.9467\n",
      "Iteration No: 20 started. Searching for the next optimal point.\n",
      "Iteration No: 20 ended. Search finished for the next optimal point.\n",
      "Time taken: 15.2950\n",
      "Function value obtained: -0.9474\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 51.8106\n",
      "Function value obtained: -0.9387\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 19.3265\n",
      "Function value obtained: -0.5048\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 18.3100\n",
      "Function value obtained: -0.9461\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 48.2447\n",
      "Function value obtained: -0.9444\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 39.3027\n",
      "Function value obtained: -0.4963\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 49.1896\n",
      "Function value obtained: -0.9237\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 28.1799\n",
      "Function value obtained: -0.9448\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 22.6809\n",
      "Function value obtained: -0.9369\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 28.2021\n",
      "Function value obtained: -0.5009\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 136.7941\n",
      "Function value obtained: -0.9369\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 31 started. Searching for the next optimal point.\n",
      "Iteration No: 31 ended. Search finished for the next optimal point.\n",
      "Time taken: 17.1607\n",
      "Function value obtained: -0.9396\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 32 started. Searching for the next optimal point.\n",
      "Iteration No: 32 ended. Search finished for the next optimal point.\n",
      "Time taken: 107.2140\n",
      "Function value obtained: -0.8533\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 33 started. Searching for the next optimal point.\n",
      "Iteration No: 33 ended. Search finished for the next optimal point.\n",
      "Time taken: 4.4381\n",
      "Function value obtained: -0.8909\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 34 started. Searching for the next optimal point.\n",
      "Iteration No: 34 ended. Search finished for the next optimal point.\n",
      "Time taken: 23.4330\n",
      "Function value obtained: -0.9352\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 35 started. Searching for the next optimal point.\n",
      "Iteration No: 35 ended. Search finished for the next optimal point.\n",
      "Time taken: 20.9804\n",
      "Function value obtained: -0.9409\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 36 started. Searching for the next optimal point.\n",
      "Iteration No: 36 ended. Search finished for the next optimal point.\n",
      "Time taken: 34.7749\n",
      "Function value obtained: -0.9356\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 37 started. Searching for the next optimal point.\n",
      "Iteration No: 37 ended. Search finished for the next optimal point.\n",
      "Time taken: 35.0728\n",
      "Function value obtained: -0.9361\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 38 started. Searching for the next optimal point.\n",
      "Iteration No: 38 ended. Search finished for the next optimal point.\n",
      "Time taken: 33.5791\n",
      "Function value obtained: -0.9257\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 39 started. Searching for the next optimal point.\n",
      "Iteration No: 39 ended. Search finished for the next optimal point.\n",
      "Time taken: 22.8665\n",
      "Function value obtained: -0.9393\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 40 started. Searching for the next optimal point.\n",
      "Iteration No: 40 ended. Search finished for the next optimal point.\n",
      "Time taken: 33.1200\n",
      "Function value obtained: -0.9309\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 41 started. Searching for the next optimal point.\n",
      "Iteration No: 41 ended. Search finished for the next optimal point.\n",
      "Time taken: 40.1554\n",
      "Function value obtained: -0.9081\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 42 started. Searching for the next optimal point.\n",
      "Iteration No: 42 ended. Search finished for the next optimal point.\n",
      "Time taken: 4.1893\n",
      "Function value obtained: -0.4972\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 43 started. Searching for the next optimal point.\n",
      "Iteration No: 43 ended. Search finished for the next optimal point.\n",
      "Time taken: 6.5796\n",
      "Function value obtained: -0.8578\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 44 started. Searching for the next optimal point.\n",
      "Iteration No: 44 ended. Search finished for the next optimal point.\n",
      "Time taken: 23.2279\n",
      "Function value obtained: -0.9443\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 45 started. Searching for the next optimal point.\n",
      "Iteration No: 45 ended. Search finished for the next optimal point.\n",
      "Time taken: 25.0765\n",
      "Function value obtained: -0.9450\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 46 started. Searching for the next optimal point.\n",
      "Iteration No: 46 ended. Search finished for the next optimal point.\n",
      "Time taken: 18.9683\n",
      "Function value obtained: -0.9419\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 47 started. Searching for the next optimal point.\n",
      "Iteration No: 47 ended. Search finished for the next optimal point.\n",
      "Time taken: 24.0303\n",
      "Function value obtained: -0.9424\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 48 started. Searching for the next optimal point.\n",
      "Iteration No: 48 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.9828\n",
      "Function value obtained: -0.9415\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 49 started. Searching for the next optimal point.\n",
      "Iteration No: 49 ended. Search finished for the next optimal point.\n",
      "Time taken: 33.9795\n",
      "Function value obtained: -0.9311\n",
      "Current minimum: -0.9474\n",
      "Iteration No: 50 started. Searching for the next optimal point.\n",
      "Iteration No: 50 ended. Search finished for the next optimal point.\n",
      "Time taken: 15.5962\n",
      "Function value obtained: -0.9456\n",
      "Current minimum: -0.9474\n",
      "Best hyperparameters found:\n",
      "learning_rate: 0.003992368422485689\n",
      "batch_size: 121\n",
      "layer1_units: 73\n",
      "layer2_units: 54\n",
      "prior_mu: 0.5103986684212475\n",
      "prior_sigma: 0.29950393862614066\n",
      "epochs: 81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "space = [\n",
    "    Real(1e-5, 1e-1, name='learning_rate'),  # Learning rate\n",
    "    Integer(16, 128, name='batch_size'),     # Batch size\n",
    "    Integer(32, 128, name='layer1_units'),   # Number of units in layer 1\n",
    "    Integer(16, 64, name='layer2_units'),    # Number of units in layer 2\n",
    "    Real(0.0, 1.0, name='prior_mu'),         # Prior mean\n",
    "    Real(0.01, 0.5, name='prior_sigma'),     # Prior sigma\n",
    "    Integer(5, 200, name='epochs')         # Number of epochs\n",
    "]\n",
    "\n",
    "\n",
    "# Objective function to optimize using Gaussian Process\n",
    "@use_named_args(space)\n",
    "def objective_function(learning_rate, batch_size, layer1_units, layer2_units, prior_mu, prior_sigma, epochs, n=9):\n",
    "    # Load the data for the specific dataset\n",
    "    X = np.load(f'Datasets/kryptonite-{n}-X.npy')\n",
    "    y = np.load(f'Datasets/kryptonite-{n}-y.npy')\n",
    "    \n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)\n",
    "    X_val, _, y_val, _ = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Convert to torch tensors and create DataLoader instances\n",
    "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    batch_size = int(max(16, batch_size))\n",
    "    # DataLoader for training and validation\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    # Initialize the model with the given hyperparameters\n",
    "    model = BayesianNN(input_dim=X_train.shape[1], \n",
    "                       prior_mu=prior_mu, prior_sigma=prior_sigma, \n",
    "                       layer1_units=layer1_units, layer2_units=layer2_units)\n",
    "    \n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training the model for a fixed number of epochs\n",
    "    for epoch in range(epochs):  # Use 10 epochs for each optimization trial\n",
    "        train_bayesian_nn(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    y_val_pred = evaluate_bayesian_nn(model, val_loader)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Return the negative validation accuracy to minimize (since gp_minimize tries to minimize the objective)\n",
    "    return -val_accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Run Bayesian Optimization with Gaussian Process\n",
    "results = gp_minimize(objective_function, space, n_calls=50, random_state=42, verbose=True)\n",
    "\n",
    "# Print the best found hyperparameters\n",
    "print(\"Best hyperparameters found:\")\n",
    "for param, value in zip(space, results.x):\n",
    "    print(f\"{param.name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 563.5791\n",
      "Function value obtained: 2.3806\n",
      "Current minimum: 2.3806\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 30.3193\n",
      "Function value obtained: 2.3886\n",
      "Current minimum: 2.3806\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 214.6472\n",
      "Function value obtained: 2.3930\n",
      "Current minimum: 2.3806\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 353.8662\n",
      "Function value obtained: 1.1727\n",
      "Current minimum: 1.1727\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 267.4685\n",
      "Function value obtained: 2.3948\n",
      "Current minimum: 1.1727\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 1458.1117\n",
      "Function value obtained: 2.4100\n",
      "Current minimum: 1.1727\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 405.8285\n",
      "Function value obtained: 1.9508\n",
      "Current minimum: 1.1727\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 155.7095\n",
      "Function value obtained: 1.5399\n",
      "Current minimum: 1.1727\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 398.1669\n",
      "Function value obtained: 2.3189\n",
      "Current minimum: 1.1727\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 211.6857\n",
      "Function value obtained: 2.4096\n",
      "Current minimum: 1.1727\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 2309.5533\n",
      "Function value obtained: 2.3889\n",
      "Current minimum: 1.1727\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 12.9189\n",
      "Function value obtained: 2.3865\n",
      "Current minimum: 1.1727\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 440.3969\n",
      "Function value obtained: 1.1591\n",
      "Current minimum: 1.1591\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 387.3167\n",
      "Function value obtained: 1.1742\n",
      "Current minimum: 1.1591\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 62.3913\n",
      "Function value obtained: 2.3964\n",
      "Current minimum: 1.1591\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 13.3823\n",
      "Function value obtained: 2.4049\n",
      "Current minimum: 1.1591\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "Iteration No: 17 ended. Search finished for the next optimal point.\n",
      "Time taken: 4667.3993\n",
      "Function value obtained: 2.3855\n",
      "Current minimum: 1.1591\n",
      "Iteration No: 18 started. Searching for the next optimal point.\n",
      "Iteration No: 18 ended. Search finished for the next optimal point.\n",
      "Time taken: 462.6480\n",
      "Function value obtained: 1.1185\n",
      "Current minimum: 1.1185\n",
      "Iteration No: 19 started. Searching for the next optimal point.\n",
      "Iteration No: 19 ended. Search finished for the next optimal point.\n",
      "Time taken: 18.3320\n",
      "Function value obtained: 2.4181\n",
      "Current minimum: 1.1185\n",
      "Iteration No: 20 started. Searching for the next optimal point.\n",
      "Iteration No: 20 ended. Search finished for the next optimal point.\n",
      "Time taken: 974.9974\n",
      "Function value obtained: 1.1188\n",
      "Current minimum: 1.1185\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 564.8149\n",
      "Function value obtained: 1.2335\n",
      "Current minimum: 1.1185\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 725.5552\n",
      "Function value obtained: 1.1932\n",
      "Current minimum: 1.1185\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 498.3921\n",
      "Function value obtained: 1.1759\n",
      "Current minimum: 1.1185\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 550.1845\n",
      "Function value obtained: 1.1604\n",
      "Current minimum: 1.1185\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 717.7794\n",
      "Function value obtained: 0.8406\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 607.2111\n",
      "Function value obtained: 1.2453\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 651.8550\n",
      "Function value obtained: 1.2817\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 17.9736\n",
      "Function value obtained: 2.4006\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 1030.3521\n",
      "Function value obtained: 1.5095\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 539.6479\n",
      "Function value obtained: 1.9693\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 31 started. Searching for the next optimal point.\n",
      "Iteration No: 31 ended. Search finished for the next optimal point.\n",
      "Time taken: 6048.5983\n",
      "Function value obtained: 2.3987\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 32 started. Searching for the next optimal point.\n",
      "Iteration No: 32 ended. Search finished for the next optimal point.\n",
      "Time taken: 26.8407\n",
      "Function value obtained: 2.4003\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 33 started. Searching for the next optimal point.\n",
      "Iteration No: 33 ended. Search finished for the next optimal point.\n",
      "Time taken: 2283.9472\n",
      "Function value obtained: 2.3871\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 34 started. Searching for the next optimal point.\n",
      "Iteration No: 34 ended. Search finished for the next optimal point.\n",
      "Time taken: 24.8191\n",
      "Function value obtained: 2.3491\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 35 started. Searching for the next optimal point.\n",
      "Iteration No: 35 ended. Search finished for the next optimal point.\n",
      "Time taken: 60.3115\n",
      "Function value obtained: 2.4078\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 36 started. Searching for the next optimal point.\n",
      "Iteration No: 36 ended. Search finished for the next optimal point.\n",
      "Time taken: 796.0821\n",
      "Function value obtained: 1.6973\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 37 started. Searching for the next optimal point.\n",
      "Iteration No: 37 ended. Search finished for the next optimal point.\n",
      "Time taken: 2451.7396\n",
      "Function value obtained: 1.2332\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 38 started. Searching for the next optimal point.\n",
      "Iteration No: 38 ended. Search finished for the next optimal point.\n",
      "Time taken: 373.3345\n",
      "Function value obtained: 1.1487\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 39 started. Searching for the next optimal point.\n",
      "Iteration No: 39 ended. Search finished for the next optimal point.\n",
      "Time taken: 3041.8468\n",
      "Function value obtained: 1.2484\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 40 started. Searching for the next optimal point.\n",
      "Iteration No: 40 ended. Search finished for the next optimal point.\n",
      "Time taken: 474.5972\n",
      "Function value obtained: 1.2049\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 41 started. Searching for the next optimal point.\n",
      "Iteration No: 41 ended. Search finished for the next optimal point.\n",
      "Time taken: 1769.8419\n",
      "Function value obtained: 1.4581\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 42 started. Searching for the next optimal point.\n",
      "Iteration No: 42 ended. Search finished for the next optimal point.\n",
      "Time taken: 429.5138\n",
      "Function value obtained: 1.1520\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 43 started. Searching for the next optimal point.\n",
      "Iteration No: 43 ended. Search finished for the next optimal point.\n",
      "Time taken: 422.6259\n",
      "Function value obtained: 1.1912\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 44 started. Searching for the next optimal point.\n",
      "Iteration No: 44 ended. Search finished for the next optimal point.\n",
      "Time taken: 492.2081\n",
      "Function value obtained: 1.1689\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 45 started. Searching for the next optimal point.\n",
      "Iteration No: 45 ended. Search finished for the next optimal point.\n",
      "Time taken: 323.0492\n",
      "Function value obtained: 1.5177\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 46 started. Searching for the next optimal point.\n",
      "Iteration No: 46 ended. Search finished for the next optimal point.\n",
      "Time taken: 486.1143\n",
      "Function value obtained: 1.3182\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 47 started. Searching for the next optimal point.\n",
      "Iteration No: 47 ended. Search finished for the next optimal point.\n",
      "Time taken: 504.4171\n",
      "Function value obtained: 1.2371\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 48 started. Searching for the next optimal point.\n",
      "Iteration No: 48 ended. Search finished for the next optimal point.\n",
      "Time taken: 351.7791\n",
      "Function value obtained: 1.1895\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 49 started. Searching for the next optimal point.\n",
      "Iteration No: 49 ended. Search finished for the next optimal point.\n",
      "Time taken: 369.9491\n",
      "Function value obtained: 1.1923\n",
      "Current minimum: 0.8406\n",
      "Iteration No: 50 started. Searching for the next optimal point.\n",
      "Iteration No: 50 ended. Search finished for the next optimal point.\n",
      "Time taken: 513.5927\n",
      "Function value obtained: 1.2005\n",
      "Current minimum: 0.8406\n",
      "Best hyperparameters found:\n",
      "learning_rate: 0.004155371586453587\n",
      "batch_size: 99\n",
      "layer1_units: 42\n",
      "layer2_units: 38\n",
      "prior_mu: 0.6144697119581359\n",
      "prior_sigma: 0.5\n",
      "epochs: 166\n"
     ]
    }
   ],
   "source": [
    "# Define the search space for hyperparameters\n",
    "thresholds = {\n",
    "    '9': 0.95,\n",
    "    '12': 0.925,\n",
    "    '15': 0.90,\n",
    "    '18': 0.875,\n",
    "    '24': 0.80,\n",
    "    '30': 0.75,\n",
    "    '45': 0.70\n",
    "}\n",
    "\n",
    "# These thresholds are the target accuracies for each dataset size, they are increased\n",
    "# by 0.025 as the algorithm is expected to perform better than the original thresholds\n",
    "correctThresholds = {\n",
    "    '9': 0.975,\n",
    "    '12': 0.95,\n",
    "    '15': 0.925,\n",
    "    '18': 0.90,\n",
    "    '24': 0.825,\n",
    "    '30': 0.775,\n",
    "    '45': 0.725\n",
    "}\n",
    "space = [\n",
    "    Real(1e-5, 1e-1, name='learning_rate'),  # Learning rate\n",
    "    Integer(16, 128, name='batch_size'),     # Batch size\n",
    "    Integer(32, 128, name='layer1_units'),   # Number of units in layer 1\n",
    "    Integer(16, 64, name='layer2_units'),    # Number of units in layer 2\n",
    "    Real(0.0, 1.0, name='prior_mu'),         # Prior mean\n",
    "    Real(0.01, 0.5, name='prior_sigma'),     # Prior sigma\n",
    "    Integer(5, 200, name='epochs')         # Number of epochs\n",
    "]\n",
    "\n",
    "# Objective function to optimize using Gaussian Process\n",
    "@use_named_args(space)\n",
    "def objective_function_all(learning_rate, batch_size, layer1_units, layer2_units, prior_mu, prior_sigma, epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Loop over different n values\n",
    "    for n_str, threshold in thresholds.items():\n",
    "        n = int(n_str)  # Convert n to integer\n",
    "        \n",
    "        # Load the data for the specific dataset\n",
    "        X = np.load(f'Datasets/kryptonite-{n}-X.npy')\n",
    "        y = np.load(f'Datasets/kryptonite-{n}-y.npy')\n",
    "        \n",
    "        # Split the dataset into training, validation, and test sets\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)\n",
    "        X_val, _, y_val, _ = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Convert to torch tensors and create DataLoader instances\n",
    "        X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "        X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "        \n",
    "        batch_size = int(max(5, batch_size))\n",
    "        # DataLoader for training and validation\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "        \n",
    "        # Initialize the model with the given hyperparameters\n",
    "        model = BayesianNN(input_dim=X_train.shape[1], \n",
    "                           prior_mu=prior_mu, prior_sigma=prior_sigma, \n",
    "                           layer1_units=layer1_units, layer2_units=layer2_units)\n",
    "        \n",
    "        criterion = nn.BCELoss()  # Binary Cross-Entropy loss\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Training the model for a fixed number of epochs\n",
    "        for epoch in range(epochs):  # Use specified epochs for each optimization trial\n",
    "            train_bayesian_nn(model, train_loader, criterion, optimizer)\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        y_val_pred = evaluate_bayesian_nn(model, val_loader)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        \n",
    "        # Calculate the loss for this n: the absolute deviation from the target threshold\n",
    "        loss_n = abs(val_accuracy - threshold)\n",
    "        total_loss += loss_n  # Add the loss for this n to the total loss\n",
    "    \n",
    "    # Return the total loss (to be minimized)\n",
    "    return total_loss\n",
    "\n",
    "# Run Bayesian Optimization with Gaussian Process\n",
    "results = gp_minimize(objective_function_all, space, n_calls=50, random_state=42, verbose=True)\n",
    "\n",
    "# Print the best found hyperparameters\n",
    "print(\"Best hyperparameters found:\")\n",
    "for param, value in zip(space, results.x):\n",
    "    print(f\"{param.name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6973\n",
      "Epoch 2, Train Loss: 0.6950\n",
      "Epoch 3, Train Loss: 0.6942\n",
      "Epoch 4, Train Loss: 0.6935\n",
      "Epoch 5, Train Loss: 0.6941\n",
      "Epoch 6, Train Loss: 0.6935\n",
      "Epoch 7, Train Loss: 0.6931\n",
      "Epoch 8, Train Loss: 0.6933\n",
      "Epoch 9, Train Loss: 0.6933\n",
      "Epoch 10, Train Loss: 0.6933\n",
      "Epoch 11, Train Loss: 0.6923\n",
      "Epoch 12, Train Loss: 0.6918\n",
      "Epoch 13, Train Loss: 0.6908\n",
      "Epoch 14, Train Loss: 0.6838\n",
      "Epoch 15, Train Loss: 0.6641\n",
      "Epoch 16, Train Loss: 0.6269\n",
      "Epoch 17, Train Loss: 0.5586\n",
      "Epoch 18, Train Loss: 0.4769\n",
      "Epoch 19, Train Loss: 0.4160\n",
      "Epoch 20, Train Loss: 0.3816\n",
      "Epoch 21, Train Loss: 0.3367\n",
      "Epoch 22, Train Loss: 0.2997\n",
      "Epoch 23, Train Loss: 0.2769\n",
      "Epoch 24, Train Loss: 0.2579\n",
      "Epoch 25, Train Loss: 0.2489\n",
      "Epoch 26, Train Loss: 0.2314\n",
      "Epoch 27, Train Loss: 0.2259\n",
      "Epoch 28, Train Loss: 0.2147\n",
      "Epoch 29, Train Loss: 0.2164\n",
      "Epoch 30, Train Loss: 0.2088\n",
      "Epoch 31, Train Loss: 0.2120\n",
      "Epoch 32, Train Loss: 0.2078\n",
      "Epoch 33, Train Loss: 0.2049\n",
      "Epoch 34, Train Loss: 0.2006\n",
      "Epoch 35, Train Loss: 0.1974\n",
      "Epoch 36, Train Loss: 0.2005\n",
      "Epoch 37, Train Loss: 0.2010\n",
      "Epoch 38, Train Loss: 0.1960\n",
      "Epoch 39, Train Loss: 0.1958\n",
      "Epoch 40, Train Loss: 0.1946\n",
      "Epoch 41, Train Loss: 0.1924\n",
      "Epoch 42, Train Loss: 0.1941\n",
      "Epoch 43, Train Loss: 0.1938\n",
      "Epoch 44, Train Loss: 0.1888\n",
      "Epoch 45, Train Loss: 0.1909\n",
      "Epoch 46, Train Loss: 0.1853\n",
      "Epoch 47, Train Loss: 0.1870\n",
      "Epoch 48, Train Loss: 0.1832\n",
      "Epoch 49, Train Loss: 0.1868\n",
      "Epoch 50, Train Loss: 0.1853\n",
      "Epoch 51, Train Loss: 0.1846\n",
      "Epoch 52, Train Loss: 0.1837\n",
      "Epoch 53, Train Loss: 0.1877\n",
      "Epoch 54, Train Loss: 0.1806\n",
      "Epoch 55, Train Loss: 0.1829\n",
      "Epoch 56, Train Loss: 0.1832\n",
      "Epoch 57, Train Loss: 0.1842\n",
      "Epoch 58, Train Loss: 0.1776\n",
      "Epoch 59, Train Loss: 0.1784\n",
      "Epoch 60, Train Loss: 0.1771\n",
      "Epoch 61, Train Loss: 0.1768\n",
      "Epoch 62, Train Loss: 0.1769\n",
      "Epoch 63, Train Loss: 0.1783\n",
      "Epoch 64, Train Loss: 0.1755\n",
      "Epoch 65, Train Loss: 0.1749\n",
      "Epoch 66, Train Loss: 0.1742\n",
      "Epoch 67, Train Loss: 0.1738\n",
      "Epoch 68, Train Loss: 0.1741\n",
      "Epoch 69, Train Loss: 0.1729\n",
      "Epoch 70, Train Loss: 0.1723\n",
      "Epoch 71, Train Loss: 0.1715\n",
      "Epoch 72, Train Loss: 0.1720\n",
      "Epoch 73, Train Loss: 0.1705\n",
      "Epoch 74, Train Loss: 0.1680\n",
      "Epoch 75, Train Loss: 0.1723\n",
      "Epoch 76, Train Loss: 0.1672\n",
      "Epoch 77, Train Loss: 0.1672\n",
      "Epoch 78, Train Loss: 0.1678\n",
      "Epoch 79, Train Loss: 0.1662\n",
      "Epoch 80, Train Loss: 0.1668\n",
      "Epoch 81, Train Loss: 0.1670\n",
      "Epoch 82, Train Loss: 0.1679\n",
      "Epoch 83, Train Loss: 0.1651\n",
      "Epoch 84, Train Loss: 0.1646\n",
      "Epoch 85, Train Loss: 0.1625\n",
      "Epoch 86, Train Loss: 0.1648\n",
      "Epoch 87, Train Loss: 0.1650\n",
      "Epoch 88, Train Loss: 0.1614\n",
      "Epoch 89, Train Loss: 0.1643\n",
      "Epoch 90, Train Loss: 0.1613\n",
      "Epoch 91, Train Loss: 0.1623\n",
      "Epoch 92, Train Loss: 0.1590\n",
      "Epoch 93, Train Loss: 0.1592\n",
      "Epoch 94, Train Loss: 0.1595\n",
      "Epoch 95, Train Loss: 0.1599\n",
      "Epoch 96, Train Loss: 0.1623\n",
      "Epoch 97, Train Loss: 0.1588\n",
      "Epoch 98, Train Loss: 0.1584\n",
      "Epoch 99, Train Loss: 0.1568\n",
      "Epoch 100, Train Loss: 0.1577\n",
      "Epoch 101, Train Loss: 0.1557\n",
      "Epoch 102, Train Loss: 0.1574\n",
      "Epoch 103, Train Loss: 0.1579\n",
      "Epoch 104, Train Loss: 0.1581\n",
      "Epoch 105, Train Loss: 0.1550\n",
      "Epoch 106, Train Loss: 0.1546\n",
      "Epoch 107, Train Loss: 0.1562\n",
      "Epoch 108, Train Loss: 0.1551\n",
      "Epoch 109, Train Loss: 0.1537\n",
      "Epoch 110, Train Loss: 0.1546\n",
      "Epoch 111, Train Loss: 0.1525\n",
      "Epoch 112, Train Loss: 0.1527\n",
      "Epoch 113, Train Loss: 0.1526\n",
      "Epoch 114, Train Loss: 0.1519\n",
      "Epoch 115, Train Loss: 0.1528\n",
      "Epoch 116, Train Loss: 0.1510\n",
      "Epoch 117, Train Loss: 0.1516\n",
      "Epoch 118, Train Loss: 0.1503\n",
      "Epoch 119, Train Loss: 0.1520\n",
      "Epoch 120, Train Loss: 0.1491\n",
      "Epoch 121, Train Loss: 0.1485\n",
      "Epoch 122, Train Loss: 0.1481\n",
      "Epoch 123, Train Loss: 0.1481\n",
      "Epoch 124, Train Loss: 0.1467\n",
      "Epoch 125, Train Loss: 0.1489\n",
      "Epoch 126, Train Loss: 0.1469\n",
      "Epoch 127, Train Loss: 0.1495\n",
      "Epoch 128, Train Loss: 0.1473\n",
      "Epoch 129, Train Loss: 0.1443\n",
      "Epoch 130, Train Loss: 0.1448\n",
      "Epoch 131, Train Loss: 0.1454\n",
      "Epoch 132, Train Loss: 0.1428\n",
      "Epoch 133, Train Loss: 0.1455\n",
      "Epoch 134, Train Loss: 0.1428\n",
      "Epoch 135, Train Loss: 0.1433\n",
      "Epoch 136, Train Loss: 0.1465\n",
      "Epoch 137, Train Loss: 0.1418\n",
      "Epoch 138, Train Loss: 0.1449\n",
      "Epoch 139, Train Loss: 0.1421\n",
      "Epoch 140, Train Loss: 0.1432\n",
      "Epoch 141, Train Loss: 0.1414\n",
      "Epoch 142, Train Loss: 0.1418\n",
      "Epoch 143, Train Loss: 0.1412\n",
      "Epoch 144, Train Loss: 0.1403\n",
      "Epoch 145, Train Loss: 0.1387\n",
      "Epoch 146, Train Loss: 0.1384\n",
      "Epoch 147, Train Loss: 0.1426\n",
      "Epoch 148, Train Loss: 0.1385\n",
      "Epoch 149, Train Loss: 0.1399\n",
      "Epoch 150, Train Loss: 0.1371\n",
      "Epoch 151, Train Loss: 0.1372\n",
      "Epoch 152, Train Loss: 0.1362\n",
      "Epoch 153, Train Loss: 0.1375\n",
      "Epoch 154, Train Loss: 0.1356\n",
      "Epoch 155, Train Loss: 0.1367\n",
      "Epoch 156, Train Loss: 0.1359\n",
      "Epoch 157, Train Loss: 0.1355\n",
      "Epoch 158, Train Loss: 0.1353\n",
      "Epoch 159, Train Loss: 0.1347\n",
      "Epoch 160, Train Loss: 0.1377\n",
      "Epoch 161, Train Loss: 0.1350\n",
      "Epoch 162, Train Loss: 0.1353\n",
      "Epoch 163, Train Loss: 0.1343\n",
      "Epoch 164, Train Loss: 0.1334\n",
      "Epoch 165, Train Loss: 0.1317\n",
      "Epoch 166, Train Loss: 0.1328\n",
      "Validation Accuracy for n=9: 0.9409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:40<04:02, 40.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=9: 0.9417\n",
      "Epoch 1, Train Loss: 0.6975\n",
      "Epoch 2, Train Loss: 0.6959\n",
      "Epoch 3, Train Loss: 0.6950\n",
      "Epoch 4, Train Loss: 0.6939\n",
      "Epoch 5, Train Loss: 0.6934\n",
      "Epoch 6, Train Loss: 0.6938\n",
      "Epoch 7, Train Loss: 0.6934\n",
      "Epoch 8, Train Loss: 0.6943\n",
      "Epoch 9, Train Loss: 0.6932\n",
      "Epoch 10, Train Loss: 0.6936\n",
      "Epoch 11, Train Loss: 0.6929\n",
      "Epoch 12, Train Loss: 0.6930\n",
      "Epoch 13, Train Loss: 0.6931\n",
      "Epoch 14, Train Loss: 0.6930\n",
      "Epoch 15, Train Loss: 0.6920\n",
      "Epoch 16, Train Loss: 0.6922\n",
      "Epoch 17, Train Loss: 0.6926\n",
      "Epoch 18, Train Loss: 0.6909\n",
      "Epoch 19, Train Loss: 0.6922\n",
      "Epoch 20, Train Loss: 0.6906\n",
      "Epoch 21, Train Loss: 0.6915\n",
      "Epoch 22, Train Loss: 0.6897\n",
      "Epoch 23, Train Loss: 0.6877\n",
      "Epoch 24, Train Loss: 0.6853\n",
      "Epoch 25, Train Loss: 0.6787\n",
      "Epoch 26, Train Loss: 0.6699\n",
      "Epoch 27, Train Loss: 0.6404\n",
      "Epoch 28, Train Loss: 0.5939\n",
      "Epoch 29, Train Loss: 0.5310\n",
      "Epoch 30, Train Loss: 0.4564\n",
      "Epoch 31, Train Loss: 0.4076\n",
      "Epoch 32, Train Loss: 0.3825\n",
      "Epoch 33, Train Loss: 0.3729\n",
      "Epoch 34, Train Loss: 0.3542\n",
      "Epoch 35, Train Loss: 0.3402\n",
      "Epoch 36, Train Loss: 0.3285\n",
      "Epoch 37, Train Loss: 0.3200\n",
      "Epoch 38, Train Loss: 0.3173\n",
      "Epoch 39, Train Loss: 0.3080\n",
      "Epoch 40, Train Loss: 0.2981\n",
      "Epoch 41, Train Loss: 0.3003\n",
      "Epoch 42, Train Loss: 0.2914\n",
      "Epoch 43, Train Loss: 0.2867\n",
      "Epoch 44, Train Loss: 0.2817\n",
      "Epoch 45, Train Loss: 0.2766\n",
      "Epoch 46, Train Loss: 0.2726\n",
      "Epoch 47, Train Loss: 0.2678\n",
      "Epoch 48, Train Loss: 0.2579\n",
      "Epoch 49, Train Loss: 0.2551\n",
      "Epoch 50, Train Loss: 0.2487\n",
      "Epoch 51, Train Loss: 0.2475\n",
      "Epoch 52, Train Loss: 0.2484\n",
      "Epoch 53, Train Loss: 0.2428\n",
      "Epoch 54, Train Loss: 0.2387\n",
      "Epoch 55, Train Loss: 0.2368\n",
      "Epoch 56, Train Loss: 0.2355\n",
      "Epoch 57, Train Loss: 0.2326\n",
      "Epoch 58, Train Loss: 0.2307\n",
      "Epoch 59, Train Loss: 0.2284\n",
      "Epoch 60, Train Loss: 0.2287\n",
      "Epoch 61, Train Loss: 0.2220\n",
      "Epoch 62, Train Loss: 0.2238\n",
      "Epoch 63, Train Loss: 0.2215\n",
      "Epoch 64, Train Loss: 0.2200\n",
      "Epoch 65, Train Loss: 0.2182\n",
      "Epoch 66, Train Loss: 0.2199\n",
      "Epoch 67, Train Loss: 0.2177\n",
      "Epoch 68, Train Loss: 0.2191\n",
      "Epoch 69, Train Loss: 0.2199\n",
      "Epoch 70, Train Loss: 0.2190\n",
      "Epoch 71, Train Loss: 0.2147\n",
      "Epoch 72, Train Loss: 0.2144\n",
      "Epoch 73, Train Loss: 0.2131\n",
      "Epoch 74, Train Loss: 0.2100\n",
      "Epoch 75, Train Loss: 0.2108\n",
      "Epoch 76, Train Loss: 0.2091\n",
      "Epoch 77, Train Loss: 0.2112\n",
      "Epoch 78, Train Loss: 0.2081\n",
      "Epoch 79, Train Loss: 0.2086\n",
      "Epoch 80, Train Loss: 0.2095\n",
      "Epoch 81, Train Loss: 0.2059\n",
      "Epoch 82, Train Loss: 0.2030\n",
      "Epoch 83, Train Loss: 0.2057\n",
      "Epoch 84, Train Loss: 0.2040\n",
      "Epoch 85, Train Loss: 0.2015\n",
      "Epoch 86, Train Loss: 0.2034\n",
      "Epoch 87, Train Loss: 0.2026\n",
      "Epoch 88, Train Loss: 0.1996\n",
      "Epoch 89, Train Loss: 0.2012\n",
      "Epoch 90, Train Loss: 0.1984\n",
      "Epoch 91, Train Loss: 0.1968\n",
      "Epoch 92, Train Loss: 0.1961\n",
      "Epoch 93, Train Loss: 0.1951\n",
      "Epoch 94, Train Loss: 0.1960\n",
      "Epoch 95, Train Loss: 0.1945\n",
      "Epoch 96, Train Loss: 0.1949\n",
      "Epoch 97, Train Loss: 0.1935\n",
      "Epoch 98, Train Loss: 0.1940\n",
      "Epoch 99, Train Loss: 0.1913\n",
      "Epoch 100, Train Loss: 0.1891\n",
      "Epoch 101, Train Loss: 0.1890\n",
      "Epoch 102, Train Loss: 0.1886\n",
      "Epoch 103, Train Loss: 0.1891\n",
      "Epoch 104, Train Loss: 0.1856\n",
      "Epoch 105, Train Loss: 0.1869\n",
      "Epoch 106, Train Loss: 0.1819\n",
      "Epoch 107, Train Loss: 0.1830\n",
      "Epoch 108, Train Loss: 0.1831\n",
      "Epoch 109, Train Loss: 0.1826\n",
      "Epoch 110, Train Loss: 0.1799\n",
      "Epoch 111, Train Loss: 0.1795\n",
      "Epoch 112, Train Loss: 0.1804\n",
      "Epoch 113, Train Loss: 0.1802\n",
      "Epoch 114, Train Loss: 0.1785\n",
      "Epoch 115, Train Loss: 0.1772\n",
      "Epoch 116, Train Loss: 0.1770\n",
      "Epoch 117, Train Loss: 0.1734\n",
      "Epoch 118, Train Loss: 0.1755\n",
      "Epoch 119, Train Loss: 0.1709\n",
      "Epoch 120, Train Loss: 0.1717\n",
      "Epoch 121, Train Loss: 0.1741\n",
      "Epoch 122, Train Loss: 0.1708\n",
      "Epoch 123, Train Loss: 0.1688\n",
      "Epoch 124, Train Loss: 0.1715\n",
      "Epoch 125, Train Loss: 0.1715\n",
      "Epoch 126, Train Loss: 0.1694\n",
      "Epoch 127, Train Loss: 0.1667\n",
      "Epoch 128, Train Loss: 0.1652\n",
      "Epoch 129, Train Loss: 0.1690\n",
      "Epoch 130, Train Loss: 0.1678\n",
      "Epoch 131, Train Loss: 0.1647\n",
      "Epoch 132, Train Loss: 0.1650\n",
      "Epoch 133, Train Loss: 0.1669\n",
      "Epoch 134, Train Loss: 0.1639\n",
      "Epoch 135, Train Loss: 0.1609\n",
      "Epoch 136, Train Loss: 0.1621\n",
      "Epoch 137, Train Loss: 0.1636\n",
      "Epoch 138, Train Loss: 0.1601\n",
      "Epoch 139, Train Loss: 0.1586\n",
      "Epoch 140, Train Loss: 0.1612\n",
      "Epoch 141, Train Loss: 0.1579\n",
      "Epoch 142, Train Loss: 0.1568\n",
      "Epoch 143, Train Loss: 0.1592\n",
      "Epoch 144, Train Loss: 0.1579\n",
      "Epoch 145, Train Loss: 0.1578\n",
      "Epoch 146, Train Loss: 0.1550\n",
      "Epoch 147, Train Loss: 0.1573\n",
      "Epoch 148, Train Loss: 0.1564\n",
      "Epoch 149, Train Loss: 0.1542\n",
      "Epoch 150, Train Loss: 0.1535\n",
      "Epoch 151, Train Loss: 0.1537\n",
      "Epoch 152, Train Loss: 0.1535\n",
      "Epoch 153, Train Loss: 0.1518\n",
      "Epoch 154, Train Loss: 0.1523\n",
      "Epoch 155, Train Loss: 0.1507\n",
      "Epoch 156, Train Loss: 0.1499\n",
      "Epoch 157, Train Loss: 0.1505\n",
      "Epoch 158, Train Loss: 0.1489\n",
      "Epoch 159, Train Loss: 0.1492\n",
      "Epoch 160, Train Loss: 0.1500\n",
      "Epoch 161, Train Loss: 0.1480\n",
      "Epoch 162, Train Loss: 0.1479\n",
      "Epoch 163, Train Loss: 0.1454\n",
      "Epoch 164, Train Loss: 0.1483\n",
      "Epoch 165, Train Loss: 0.1451\n",
      "Epoch 166, Train Loss: 0.1457\n",
      "Validation Accuracy for n=12: 0.9197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [01:37<04:10, 50.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=12: 0.9208\n",
      "Epoch 1, Train Loss: 0.6968\n",
      "Epoch 2, Train Loss: 0.6954\n",
      "Epoch 3, Train Loss: 0.6944\n",
      "Epoch 4, Train Loss: 0.6938\n",
      "Epoch 5, Train Loss: 0.6937\n",
      "Epoch 6, Train Loss: 0.6932\n",
      "Epoch 7, Train Loss: 0.6937\n",
      "Epoch 8, Train Loss: 0.6931\n",
      "Epoch 9, Train Loss: 0.6924\n",
      "Epoch 10, Train Loss: 0.6933\n",
      "Epoch 11, Train Loss: 0.6927\n",
      "Epoch 12, Train Loss: 0.6918\n",
      "Epoch 13, Train Loss: 0.6928\n",
      "Epoch 14, Train Loss: 0.6919\n",
      "Epoch 15, Train Loss: 0.6921\n",
      "Epoch 16, Train Loss: 0.6924\n",
      "Epoch 17, Train Loss: 0.6915\n",
      "Epoch 18, Train Loss: 0.6915\n",
      "Epoch 19, Train Loss: 0.6919\n",
      "Epoch 20, Train Loss: 0.6916\n",
      "Epoch 21, Train Loss: 0.6907\n",
      "Epoch 22, Train Loss: 0.6906\n",
      "Epoch 23, Train Loss: 0.6902\n",
      "Epoch 24, Train Loss: 0.6908\n",
      "Epoch 25, Train Loss: 0.6897\n",
      "Epoch 26, Train Loss: 0.6898\n",
      "Epoch 27, Train Loss: 0.6898\n",
      "Epoch 28, Train Loss: 0.6890\n",
      "Epoch 29, Train Loss: 0.6892\n",
      "Epoch 30, Train Loss: 0.6891\n",
      "Epoch 31, Train Loss: 0.6889\n",
      "Epoch 32, Train Loss: 0.6878\n",
      "Epoch 33, Train Loss: 0.6884\n",
      "Epoch 34, Train Loss: 0.6878\n",
      "Epoch 35, Train Loss: 0.6877\n",
      "Epoch 36, Train Loss: 0.6872\n",
      "Epoch 37, Train Loss: 0.6865\n",
      "Epoch 38, Train Loss: 0.6859\n",
      "Epoch 39, Train Loss: 0.6860\n",
      "Epoch 40, Train Loss: 0.6863\n",
      "Epoch 41, Train Loss: 0.6852\n",
      "Epoch 42, Train Loss: 0.6841\n",
      "Epoch 43, Train Loss: 0.6843\n",
      "Epoch 44, Train Loss: 0.6847\n",
      "Epoch 45, Train Loss: 0.6822\n",
      "Epoch 46, Train Loss: 0.6826\n",
      "Epoch 47, Train Loss: 0.6819\n",
      "Epoch 48, Train Loss: 0.6822\n",
      "Epoch 49, Train Loss: 0.6804\n",
      "Epoch 50, Train Loss: 0.6820\n",
      "Epoch 51, Train Loss: 0.6798\n",
      "Epoch 52, Train Loss: 0.6793\n",
      "Epoch 53, Train Loss: 0.6804\n",
      "Epoch 54, Train Loss: 0.6793\n",
      "Epoch 55, Train Loss: 0.6780\n",
      "Epoch 56, Train Loss: 0.6777\n",
      "Epoch 57, Train Loss: 0.6767\n",
      "Epoch 58, Train Loss: 0.6760\n",
      "Epoch 59, Train Loss: 0.6747\n",
      "Epoch 60, Train Loss: 0.6757\n",
      "Epoch 61, Train Loss: 0.6721\n",
      "Epoch 62, Train Loss: 0.6706\n",
      "Epoch 63, Train Loss: 0.6704\n",
      "Epoch 64, Train Loss: 0.6713\n",
      "Epoch 65, Train Loss: 0.6716\n",
      "Epoch 66, Train Loss: 0.6696\n",
      "Epoch 67, Train Loss: 0.6671\n",
      "Epoch 68, Train Loss: 0.6664\n",
      "Epoch 69, Train Loss: 0.6654\n",
      "Epoch 70, Train Loss: 0.6637\n",
      "Epoch 71, Train Loss: 0.6625\n",
      "Epoch 72, Train Loss: 0.6612\n",
      "Epoch 73, Train Loss: 0.6602\n",
      "Epoch 74, Train Loss: 0.6579\n",
      "Epoch 75, Train Loss: 0.6573\n",
      "Epoch 76, Train Loss: 0.6552\n",
      "Epoch 77, Train Loss: 0.6524\n",
      "Epoch 78, Train Loss: 0.6529\n",
      "Epoch 79, Train Loss: 0.6497\n",
      "Epoch 80, Train Loss: 0.6501\n",
      "Epoch 81, Train Loss: 0.6480\n",
      "Epoch 82, Train Loss: 0.6447\n",
      "Epoch 83, Train Loss: 0.6443\n",
      "Epoch 84, Train Loss: 0.6426\n",
      "Epoch 85, Train Loss: 0.6410\n",
      "Epoch 86, Train Loss: 0.6375\n",
      "Epoch 87, Train Loss: 0.6380\n",
      "Epoch 88, Train Loss: 0.6360\n",
      "Epoch 89, Train Loss: 0.6321\n",
      "Epoch 90, Train Loss: 0.6320\n",
      "Epoch 91, Train Loss: 0.6306\n",
      "Epoch 92, Train Loss: 0.6273\n",
      "Epoch 93, Train Loss: 0.6256\n",
      "Epoch 94, Train Loss: 0.6249\n",
      "Epoch 95, Train Loss: 0.6233\n",
      "Epoch 96, Train Loss: 0.6184\n",
      "Epoch 97, Train Loss: 0.6212\n",
      "Epoch 98, Train Loss: 0.6157\n",
      "Epoch 99, Train Loss: 0.6141\n",
      "Epoch 100, Train Loss: 0.6113\n",
      "Epoch 101, Train Loss: 0.6098\n",
      "Epoch 102, Train Loss: 0.6104\n",
      "Epoch 103, Train Loss: 0.6066\n",
      "Epoch 104, Train Loss: 0.6041\n",
      "Epoch 105, Train Loss: 0.6035\n",
      "Epoch 106, Train Loss: 0.6011\n",
      "Epoch 107, Train Loss: 0.6003\n",
      "Epoch 108, Train Loss: 0.5979\n",
      "Epoch 109, Train Loss: 0.5967\n",
      "Epoch 110, Train Loss: 0.5954\n",
      "Epoch 111, Train Loss: 0.5934\n",
      "Epoch 112, Train Loss: 0.5907\n",
      "Epoch 113, Train Loss: 0.5899\n",
      "Epoch 114, Train Loss: 0.5881\n",
      "Epoch 115, Train Loss: 0.5865\n",
      "Epoch 116, Train Loss: 0.5826\n",
      "Epoch 117, Train Loss: 0.5812\n",
      "Epoch 118, Train Loss: 0.5794\n",
      "Epoch 119, Train Loss: 0.5774\n",
      "Epoch 120, Train Loss: 0.5760\n",
      "Epoch 121, Train Loss: 0.5763\n",
      "Epoch 122, Train Loss: 0.5731\n",
      "Epoch 123, Train Loss: 0.5700\n",
      "Epoch 124, Train Loss: 0.5695\n",
      "Epoch 125, Train Loss: 0.5677\n",
      "Epoch 126, Train Loss: 0.5676\n",
      "Epoch 127, Train Loss: 0.5669\n",
      "Epoch 128, Train Loss: 0.5626\n",
      "Epoch 129, Train Loss: 0.5599\n",
      "Epoch 130, Train Loss: 0.5578\n",
      "Epoch 131, Train Loss: 0.5586\n",
      "Epoch 132, Train Loss: 0.5602\n",
      "Epoch 133, Train Loss: 0.5546\n",
      "Epoch 134, Train Loss: 0.5553\n",
      "Epoch 135, Train Loss: 0.5534\n",
      "Epoch 136, Train Loss: 0.5488\n",
      "Epoch 137, Train Loss: 0.5486\n",
      "Epoch 138, Train Loss: 0.5490\n",
      "Epoch 139, Train Loss: 0.5435\n",
      "Epoch 140, Train Loss: 0.5457\n",
      "Epoch 141, Train Loss: 0.5430\n",
      "Epoch 142, Train Loss: 0.5386\n",
      "Epoch 143, Train Loss: 0.5385\n",
      "Epoch 144, Train Loss: 0.5352\n",
      "Epoch 145, Train Loss: 0.5337\n",
      "Epoch 146, Train Loss: 0.5299\n",
      "Epoch 147, Train Loss: 0.5278\n",
      "Epoch 148, Train Loss: 0.5293\n",
      "Epoch 149, Train Loss: 0.5262\n",
      "Epoch 150, Train Loss: 0.5243\n",
      "Epoch 151, Train Loss: 0.5210\n",
      "Epoch 152, Train Loss: 0.5203\n",
      "Epoch 153, Train Loss: 0.5187\n",
      "Epoch 154, Train Loss: 0.5150\n",
      "Epoch 155, Train Loss: 0.5117\n",
      "Epoch 156, Train Loss: 0.5127\n",
      "Epoch 157, Train Loss: 0.5089\n",
      "Epoch 158, Train Loss: 0.5086\n",
      "Epoch 159, Train Loss: 0.5044\n",
      "Epoch 160, Train Loss: 0.5025\n",
      "Epoch 161, Train Loss: 0.5035\n",
      "Epoch 162, Train Loss: 0.5008\n",
      "Epoch 163, Train Loss: 0.4990\n",
      "Epoch 164, Train Loss: 0.4968\n",
      "Epoch 165, Train Loss: 0.4943\n",
      "Epoch 166, Train Loss: 0.4921\n",
      "Validation Accuracy for n=15: 0.5888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [02:46<03:56, 59.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=15: 0.5886\n",
      "Epoch 1, Train Loss: 0.6970\n",
      "Epoch 2, Train Loss: 0.6948\n",
      "Epoch 3, Train Loss: 0.6945\n",
      "Epoch 4, Train Loss: 0.6941\n",
      "Epoch 5, Train Loss: 0.6941\n",
      "Epoch 6, Train Loss: 0.6938\n",
      "Epoch 7, Train Loss: 0.6937\n",
      "Epoch 8, Train Loss: 0.6935\n",
      "Epoch 9, Train Loss: 0.6936\n",
      "Epoch 10, Train Loss: 0.6930\n",
      "Epoch 11, Train Loss: 0.6932\n",
      "Epoch 12, Train Loss: 0.6934\n",
      "Epoch 13, Train Loss: 0.6932\n",
      "Epoch 14, Train Loss: 0.6931\n",
      "Epoch 15, Train Loss: 0.6928\n",
      "Epoch 16, Train Loss: 0.6931\n",
      "Epoch 17, Train Loss: 0.6931\n",
      "Epoch 18, Train Loss: 0.6924\n",
      "Epoch 19, Train Loss: 0.6922\n",
      "Epoch 20, Train Loss: 0.6927\n",
      "Epoch 21, Train Loss: 0.6921\n",
      "Epoch 22, Train Loss: 0.6925\n",
      "Epoch 23, Train Loss: 0.6918\n",
      "Epoch 24, Train Loss: 0.6918\n",
      "Epoch 25, Train Loss: 0.6917\n",
      "Epoch 26, Train Loss: 0.6910\n",
      "Epoch 27, Train Loss: 0.6911\n",
      "Epoch 28, Train Loss: 0.6900\n",
      "Epoch 29, Train Loss: 0.6907\n",
      "Epoch 30, Train Loss: 0.6898\n",
      "Epoch 31, Train Loss: 0.6892\n",
      "Epoch 32, Train Loss: 0.6886\n",
      "Epoch 33, Train Loss: 0.6888\n",
      "Epoch 34, Train Loss: 0.6884\n",
      "Epoch 35, Train Loss: 0.6883\n",
      "Epoch 36, Train Loss: 0.6881\n",
      "Epoch 37, Train Loss: 0.6874\n",
      "Epoch 38, Train Loss: 0.6873\n",
      "Epoch 39, Train Loss: 0.6862\n",
      "Epoch 40, Train Loss: 0.6854\n",
      "Epoch 41, Train Loss: 0.6852\n",
      "Epoch 42, Train Loss: 0.6848\n",
      "Epoch 43, Train Loss: 0.6842\n",
      "Epoch 44, Train Loss: 0.6829\n",
      "Epoch 45, Train Loss: 0.6837\n",
      "Epoch 46, Train Loss: 0.6816\n",
      "Epoch 47, Train Loss: 0.6810\n",
      "Epoch 48, Train Loss: 0.6804\n",
      "Epoch 49, Train Loss: 0.6788\n",
      "Epoch 50, Train Loss: 0.6783\n",
      "Epoch 51, Train Loss: 0.6778\n",
      "Epoch 52, Train Loss: 0.6757\n",
      "Epoch 53, Train Loss: 0.6756\n",
      "Epoch 54, Train Loss: 0.6751\n",
      "Epoch 55, Train Loss: 0.6732\n",
      "Epoch 56, Train Loss: 0.6713\n",
      "Epoch 57, Train Loss: 0.6703\n",
      "Epoch 58, Train Loss: 0.6690\n",
      "Epoch 59, Train Loss: 0.6685\n",
      "Epoch 60, Train Loss: 0.6652\n",
      "Epoch 61, Train Loss: 0.6639\n",
      "Epoch 62, Train Loss: 0.6648\n",
      "Epoch 63, Train Loss: 0.6625\n",
      "Epoch 64, Train Loss: 0.6614\n",
      "Epoch 65, Train Loss: 0.6604\n",
      "Epoch 66, Train Loss: 0.6573\n",
      "Epoch 67, Train Loss: 0.6581\n",
      "Epoch 68, Train Loss: 0.6574\n",
      "Epoch 69, Train Loss: 0.6543\n",
      "Epoch 70, Train Loss: 0.6536\n",
      "Epoch 71, Train Loss: 0.6523\n",
      "Epoch 72, Train Loss: 0.6519\n",
      "Epoch 73, Train Loss: 0.6497\n",
      "Epoch 74, Train Loss: 0.6466\n",
      "Epoch 75, Train Loss: 0.6468\n",
      "Epoch 76, Train Loss: 0.6455\n",
      "Epoch 77, Train Loss: 0.6436\n",
      "Epoch 78, Train Loss: 0.6438\n",
      "Epoch 79, Train Loss: 0.6412\n",
      "Epoch 80, Train Loss: 0.6403\n",
      "Epoch 81, Train Loss: 0.6373\n",
      "Epoch 82, Train Loss: 0.6357\n",
      "Epoch 83, Train Loss: 0.6343\n",
      "Epoch 84, Train Loss: 0.6337\n",
      "Epoch 85, Train Loss: 0.6308\n",
      "Epoch 86, Train Loss: 0.6286\n",
      "Epoch 87, Train Loss: 0.6289\n",
      "Epoch 88, Train Loss: 0.6269\n",
      "Epoch 89, Train Loss: 0.6267\n",
      "Epoch 90, Train Loss: 0.6222\n",
      "Epoch 91, Train Loss: 0.6218\n",
      "Epoch 92, Train Loss: 0.6186\n",
      "Epoch 93, Train Loss: 0.6194\n",
      "Epoch 94, Train Loss: 0.6182\n",
      "Epoch 95, Train Loss: 0.6152\n",
      "Epoch 96, Train Loss: 0.6148\n",
      "Epoch 97, Train Loss: 0.6135\n",
      "Epoch 98, Train Loss: 0.6105\n",
      "Epoch 99, Train Loss: 0.6118\n",
      "Epoch 100, Train Loss: 0.6081\n",
      "Epoch 101, Train Loss: 0.6095\n",
      "Epoch 102, Train Loss: 0.6058\n",
      "Epoch 103, Train Loss: 0.6046\n",
      "Epoch 104, Train Loss: 0.6031\n",
      "Epoch 105, Train Loss: 0.6013\n",
      "Epoch 106, Train Loss: 0.5972\n",
      "Epoch 107, Train Loss: 0.5980\n",
      "Epoch 108, Train Loss: 0.5966\n",
      "Epoch 109, Train Loss: 0.5952\n",
      "Epoch 110, Train Loss: 0.5926\n",
      "Epoch 111, Train Loss: 0.5941\n",
      "Epoch 112, Train Loss: 0.5924\n",
      "Epoch 113, Train Loss: 0.5886\n",
      "Epoch 114, Train Loss: 0.5899\n",
      "Epoch 115, Train Loss: 0.5880\n",
      "Epoch 116, Train Loss: 0.5853\n",
      "Epoch 117, Train Loss: 0.5833\n",
      "Epoch 118, Train Loss: 0.5847\n",
      "Epoch 119, Train Loss: 0.5827\n",
      "Epoch 120, Train Loss: 0.5802\n",
      "Epoch 121, Train Loss: 0.5811\n",
      "Epoch 122, Train Loss: 0.5810\n",
      "Epoch 123, Train Loss: 0.5796\n",
      "Epoch 124, Train Loss: 0.5772\n",
      "Epoch 125, Train Loss: 0.5762\n",
      "Epoch 126, Train Loss: 0.5729\n",
      "Epoch 127, Train Loss: 0.5718\n",
      "Epoch 128, Train Loss: 0.5719\n",
      "Epoch 129, Train Loss: 0.5720\n",
      "Epoch 130, Train Loss: 0.5711\n",
      "Epoch 131, Train Loss: 0.5706\n",
      "Epoch 132, Train Loss: 0.5678\n",
      "Epoch 133, Train Loss: 0.5683\n",
      "Epoch 134, Train Loss: 0.5664\n",
      "Epoch 135, Train Loss: 0.5662\n",
      "Epoch 136, Train Loss: 0.5649\n",
      "Epoch 137, Train Loss: 0.5645\n",
      "Epoch 138, Train Loss: 0.5628\n",
      "Epoch 139, Train Loss: 0.5616\n",
      "Epoch 140, Train Loss: 0.5619\n",
      "Epoch 141, Train Loss: 0.5603\n",
      "Epoch 142, Train Loss: 0.5592\n",
      "Epoch 143, Train Loss: 0.5592\n",
      "Epoch 144, Train Loss: 0.5581\n",
      "Epoch 145, Train Loss: 0.5578\n",
      "Epoch 146, Train Loss: 0.5570\n",
      "Epoch 147, Train Loss: 0.5575\n",
      "Epoch 148, Train Loss: 0.5549\n",
      "Epoch 149, Train Loss: 0.5567\n",
      "Epoch 150, Train Loss: 0.5523\n",
      "Epoch 151, Train Loss: 0.5526\n",
      "Epoch 152, Train Loss: 0.5546\n",
      "Epoch 153, Train Loss: 0.5514\n",
      "Epoch 154, Train Loss: 0.5491\n",
      "Epoch 155, Train Loss: 0.5498\n",
      "Epoch 156, Train Loss: 0.5488\n",
      "Epoch 157, Train Loss: 0.5477\n",
      "Epoch 158, Train Loss: 0.5487\n",
      "Epoch 159, Train Loss: 0.5460\n",
      "Epoch 160, Train Loss: 0.5483\n",
      "Epoch 161, Train Loss: 0.5482\n",
      "Epoch 162, Train Loss: 0.5465\n",
      "Epoch 163, Train Loss: 0.5444\n",
      "Epoch 164, Train Loss: 0.5448\n",
      "Epoch 165, Train Loss: 0.5437\n",
      "Epoch 166, Train Loss: 0.5432\n",
      "Validation Accuracy for n=18: 0.5048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [04:09<03:25, 68.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=18: 0.4992\n",
      "Epoch 1, Train Loss: 0.6976\n",
      "Epoch 2, Train Loss: 0.6944\n",
      "Epoch 3, Train Loss: 0.6944\n",
      "Epoch 4, Train Loss: 0.6934\n",
      "Epoch 5, Train Loss: 0.6940\n",
      "Epoch 6, Train Loss: 0.6936\n",
      "Epoch 7, Train Loss: 0.6932\n",
      "Epoch 8, Train Loss: 0.6933\n",
      "Epoch 9, Train Loss: 0.6935\n",
      "Epoch 10, Train Loss: 0.6934\n",
      "Epoch 11, Train Loss: 0.6930\n",
      "Epoch 12, Train Loss: 0.6932\n",
      "Epoch 13, Train Loss: 0.6930\n",
      "Epoch 14, Train Loss: 0.6931\n",
      "Epoch 15, Train Loss: 0.6928\n",
      "Epoch 16, Train Loss: 0.6925\n",
      "Epoch 17, Train Loss: 0.6926\n",
      "Epoch 18, Train Loss: 0.6922\n",
      "Epoch 19, Train Loss: 0.6915\n",
      "Epoch 20, Train Loss: 0.6916\n",
      "Epoch 21, Train Loss: 0.6918\n",
      "Epoch 22, Train Loss: 0.6918\n",
      "Epoch 23, Train Loss: 0.6907\n",
      "Epoch 24, Train Loss: 0.6908\n",
      "Epoch 25, Train Loss: 0.6900\n",
      "Epoch 26, Train Loss: 0.6901\n",
      "Epoch 27, Train Loss: 0.6896\n",
      "Epoch 28, Train Loss: 0.6888\n",
      "Epoch 29, Train Loss: 0.6886\n",
      "Epoch 30, Train Loss: 0.6887\n",
      "Epoch 31, Train Loss: 0.6883\n",
      "Epoch 32, Train Loss: 0.6880\n",
      "Epoch 33, Train Loss: 0.6871\n",
      "Epoch 34, Train Loss: 0.6874\n",
      "Epoch 35, Train Loss: 0.6867\n",
      "Epoch 36, Train Loss: 0.6867\n",
      "Epoch 37, Train Loss: 0.6855\n",
      "Epoch 38, Train Loss: 0.6853\n",
      "Epoch 39, Train Loss: 0.6849\n",
      "Epoch 40, Train Loss: 0.6831\n",
      "Epoch 41, Train Loss: 0.6825\n",
      "Epoch 42, Train Loss: 0.6820\n",
      "Epoch 43, Train Loss: 0.6816\n",
      "Epoch 44, Train Loss: 0.6805\n",
      "Epoch 45, Train Loss: 0.6809\n",
      "Epoch 46, Train Loss: 0.6782\n",
      "Epoch 47, Train Loss: 0.6779\n",
      "Epoch 48, Train Loss: 0.6782\n",
      "Epoch 49, Train Loss: 0.6756\n",
      "Epoch 50, Train Loss: 0.6748\n",
      "Epoch 51, Train Loss: 0.6732\n",
      "Epoch 52, Train Loss: 0.6728\n",
      "Epoch 53, Train Loss: 0.6718\n",
      "Epoch 54, Train Loss: 0.6699\n",
      "Epoch 55, Train Loss: 0.6703\n",
      "Epoch 56, Train Loss: 0.6687\n",
      "Epoch 57, Train Loss: 0.6671\n",
      "Epoch 58, Train Loss: 0.6657\n",
      "Epoch 59, Train Loss: 0.6650\n",
      "Epoch 60, Train Loss: 0.6633\n",
      "Epoch 61, Train Loss: 0.6647\n",
      "Epoch 62, Train Loss: 0.6624\n",
      "Epoch 63, Train Loss: 0.6615\n",
      "Epoch 64, Train Loss: 0.6601\n",
      "Epoch 65, Train Loss: 0.6598\n",
      "Epoch 66, Train Loss: 0.6577\n",
      "Epoch 67, Train Loss: 0.6562\n",
      "Epoch 68, Train Loss: 0.6550\n",
      "Epoch 69, Train Loss: 0.6551\n",
      "Epoch 70, Train Loss: 0.6545\n",
      "Epoch 71, Train Loss: 0.6518\n",
      "Epoch 72, Train Loss: 0.6510\n",
      "Epoch 73, Train Loss: 0.6508\n",
      "Epoch 74, Train Loss: 0.6485\n",
      "Epoch 75, Train Loss: 0.6473\n",
      "Epoch 76, Train Loss: 0.6471\n",
      "Epoch 77, Train Loss: 0.6450\n",
      "Epoch 78, Train Loss: 0.6452\n",
      "Epoch 79, Train Loss: 0.6429\n",
      "Epoch 80, Train Loss: 0.6418\n",
      "Epoch 81, Train Loss: 0.6413\n",
      "Epoch 82, Train Loss: 0.6408\n",
      "Epoch 83, Train Loss: 0.6401\n",
      "Epoch 84, Train Loss: 0.6384\n",
      "Epoch 85, Train Loss: 0.6364\n",
      "Epoch 86, Train Loss: 0.6354\n",
      "Epoch 87, Train Loss: 0.6351\n",
      "Epoch 88, Train Loss: 0.6337\n",
      "Epoch 89, Train Loss: 0.6322\n",
      "Epoch 90, Train Loss: 0.6320\n",
      "Epoch 91, Train Loss: 0.6305\n",
      "Epoch 92, Train Loss: 0.6299\n",
      "Epoch 93, Train Loss: 0.6288\n",
      "Epoch 94, Train Loss: 0.6289\n",
      "Epoch 95, Train Loss: 0.6266\n",
      "Epoch 96, Train Loss: 0.6259\n",
      "Epoch 97, Train Loss: 0.6263\n",
      "Epoch 98, Train Loss: 0.6240\n",
      "Epoch 99, Train Loss: 0.6240\n",
      "Epoch 100, Train Loss: 0.6207\n",
      "Epoch 101, Train Loss: 0.6194\n",
      "Epoch 102, Train Loss: 0.6196\n",
      "Epoch 103, Train Loss: 0.6182\n",
      "Epoch 104, Train Loss: 0.6178\n",
      "Epoch 105, Train Loss: 0.6186\n",
      "Epoch 106, Train Loss: 0.6181\n",
      "Epoch 107, Train Loss: 0.6162\n",
      "Epoch 108, Train Loss: 0.6154\n",
      "Epoch 109, Train Loss: 0.6140\n",
      "Epoch 110, Train Loss: 0.6134\n",
      "Epoch 111, Train Loss: 0.6130\n",
      "Epoch 112, Train Loss: 0.6120\n",
      "Epoch 113, Train Loss: 0.6109\n",
      "Epoch 114, Train Loss: 0.6098\n",
      "Epoch 115, Train Loss: 0.6091\n",
      "Epoch 116, Train Loss: 0.6079\n",
      "Epoch 117, Train Loss: 0.6065\n",
      "Epoch 118, Train Loss: 0.6065\n",
      "Epoch 119, Train Loss: 0.6065\n",
      "Epoch 120, Train Loss: 0.6042\n",
      "Epoch 121, Train Loss: 0.6049\n",
      "Epoch 122, Train Loss: 0.6035\n",
      "Epoch 123, Train Loss: 0.6024\n",
      "Epoch 124, Train Loss: 0.6032\n",
      "Epoch 125, Train Loss: 0.6017\n",
      "Epoch 126, Train Loss: 0.5991\n",
      "Epoch 127, Train Loss: 0.5999\n",
      "Epoch 128, Train Loss: 0.5998\n",
      "Epoch 129, Train Loss: 0.5986\n",
      "Epoch 130, Train Loss: 0.5990\n",
      "Epoch 131, Train Loss: 0.5959\n",
      "Epoch 132, Train Loss: 0.5966\n",
      "Epoch 133, Train Loss: 0.5965\n",
      "Epoch 134, Train Loss: 0.5952\n",
      "Epoch 135, Train Loss: 0.5932\n",
      "Epoch 136, Train Loss: 0.5936\n",
      "Epoch 137, Train Loss: 0.5939\n",
      "Epoch 138, Train Loss: 0.5922\n",
      "Epoch 139, Train Loss: 0.5918\n",
      "Epoch 140, Train Loss: 0.5927\n",
      "Epoch 141, Train Loss: 0.5900\n",
      "Epoch 142, Train Loss: 0.5906\n",
      "Epoch 143, Train Loss: 0.5895\n",
      "Epoch 144, Train Loss: 0.5897\n",
      "Epoch 145, Train Loss: 0.5874\n",
      "Epoch 146, Train Loss: 0.5870\n",
      "Epoch 147, Train Loss: 0.5866\n",
      "Epoch 148, Train Loss: 0.5888\n",
      "Epoch 149, Train Loss: 0.5868\n",
      "Epoch 150, Train Loss: 0.5862\n",
      "Epoch 151, Train Loss: 0.5838\n",
      "Epoch 152, Train Loss: 0.5845\n",
      "Epoch 153, Train Loss: 0.5841\n",
      "Epoch 154, Train Loss: 0.5828\n",
      "Epoch 155, Train Loss: 0.5826\n",
      "Epoch 156, Train Loss: 0.5818\n",
      "Epoch 157, Train Loss: 0.5820\n",
      "Epoch 158, Train Loss: 0.5822\n",
      "Epoch 159, Train Loss: 0.5812\n",
      "Epoch 160, Train Loss: 0.5815\n",
      "Epoch 161, Train Loss: 0.5807\n",
      "Epoch 162, Train Loss: 0.5802\n",
      "Epoch 163, Train Loss: 0.5797\n",
      "Epoch 164, Train Loss: 0.5783\n",
      "Epoch 165, Train Loss: 0.5786\n",
      "Epoch 166, Train Loss: 0.5773\n",
      "Validation Accuracy for n=24: 0.5024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [06:06<02:51, 85.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=24: 0.5003\n",
      "Epoch 1, Train Loss: 0.6964\n",
      "Epoch 2, Train Loss: 0.6945\n",
      "Epoch 3, Train Loss: 0.6937\n",
      "Epoch 4, Train Loss: 0.6938\n",
      "Epoch 5, Train Loss: 0.6934\n",
      "Epoch 6, Train Loss: 0.6934\n",
      "Epoch 7, Train Loss: 0.6933\n",
      "Epoch 8, Train Loss: 0.6932\n",
      "Epoch 9, Train Loss: 0.6934\n",
      "Epoch 10, Train Loss: 0.6935\n",
      "Epoch 11, Train Loss: 0.6933\n",
      "Epoch 12, Train Loss: 0.6933\n",
      "Epoch 13, Train Loss: 0.6933\n",
      "Epoch 14, Train Loss: 0.6931\n",
      "Epoch 15, Train Loss: 0.6933\n",
      "Epoch 16, Train Loss: 0.6932\n",
      "Epoch 17, Train Loss: 0.6931\n",
      "Epoch 18, Train Loss: 0.6930\n",
      "Epoch 19, Train Loss: 0.6930\n",
      "Epoch 20, Train Loss: 0.6928\n",
      "Epoch 21, Train Loss: 0.6931\n",
      "Epoch 22, Train Loss: 0.6928\n",
      "Epoch 23, Train Loss: 0.6931\n",
      "Epoch 24, Train Loss: 0.6922\n",
      "Epoch 25, Train Loss: 0.6925\n",
      "Epoch 26, Train Loss: 0.6924\n",
      "Epoch 27, Train Loss: 0.6918\n",
      "Epoch 28, Train Loss: 0.6912\n",
      "Epoch 29, Train Loss: 0.6916\n",
      "Epoch 30, Train Loss: 0.6912\n",
      "Epoch 31, Train Loss: 0.6908\n",
      "Epoch 32, Train Loss: 0.6899\n",
      "Epoch 33, Train Loss: 0.6899\n",
      "Epoch 34, Train Loss: 0.6889\n",
      "Epoch 35, Train Loss: 0.6886\n",
      "Epoch 36, Train Loss: 0.6877\n",
      "Epoch 37, Train Loss: 0.6875\n",
      "Epoch 38, Train Loss: 0.6874\n",
      "Epoch 39, Train Loss: 0.6858\n",
      "Epoch 40, Train Loss: 0.6858\n",
      "Epoch 41, Train Loss: 0.6841\n",
      "Epoch 42, Train Loss: 0.6835\n",
      "Epoch 43, Train Loss: 0.6827\n",
      "Epoch 44, Train Loss: 0.6808\n",
      "Epoch 45, Train Loss: 0.6813\n",
      "Epoch 46, Train Loss: 0.6804\n",
      "Epoch 47, Train Loss: 0.6790\n",
      "Epoch 48, Train Loss: 0.6789\n",
      "Epoch 49, Train Loss: 0.6774\n",
      "Epoch 50, Train Loss: 0.6768\n",
      "Epoch 51, Train Loss: 0.6750\n",
      "Epoch 52, Train Loss: 0.6741\n",
      "Epoch 53, Train Loss: 0.6738\n",
      "Epoch 54, Train Loss: 0.6725\n",
      "Epoch 55, Train Loss: 0.6723\n",
      "Epoch 56, Train Loss: 0.6716\n",
      "Epoch 57, Train Loss: 0.6706\n",
      "Epoch 58, Train Loss: 0.6699\n",
      "Epoch 59, Train Loss: 0.6687\n",
      "Epoch 60, Train Loss: 0.6677\n",
      "Epoch 61, Train Loss: 0.6669\n",
      "Epoch 62, Train Loss: 0.6668\n",
      "Epoch 63, Train Loss: 0.6659\n",
      "Epoch 64, Train Loss: 0.6637\n",
      "Epoch 65, Train Loss: 0.6630\n",
      "Epoch 66, Train Loss: 0.6621\n",
      "Epoch 67, Train Loss: 0.6606\n",
      "Epoch 68, Train Loss: 0.6597\n",
      "Epoch 69, Train Loss: 0.6581\n",
      "Epoch 70, Train Loss: 0.6581\n",
      "Epoch 71, Train Loss: 0.6556\n",
      "Epoch 72, Train Loss: 0.6552\n",
      "Epoch 73, Train Loss: 0.6546\n",
      "Epoch 74, Train Loss: 0.6545\n",
      "Epoch 75, Train Loss: 0.6528\n",
      "Epoch 76, Train Loss: 0.6512\n",
      "Epoch 77, Train Loss: 0.6498\n",
      "Epoch 78, Train Loss: 0.6505\n",
      "Epoch 79, Train Loss: 0.6482\n",
      "Epoch 80, Train Loss: 0.6488\n",
      "Epoch 81, Train Loss: 0.6475\n",
      "Epoch 82, Train Loss: 0.6451\n",
      "Epoch 83, Train Loss: 0.6449\n",
      "Epoch 84, Train Loss: 0.6452\n",
      "Epoch 85, Train Loss: 0.6429\n",
      "Epoch 86, Train Loss: 0.6439\n",
      "Epoch 87, Train Loss: 0.6423\n",
      "Epoch 88, Train Loss: 0.6405\n",
      "Epoch 89, Train Loss: 0.6410\n",
      "Epoch 90, Train Loss: 0.6403\n",
      "Epoch 91, Train Loss: 0.6383\n",
      "Epoch 92, Train Loss: 0.6369\n",
      "Epoch 93, Train Loss: 0.6368\n",
      "Epoch 94, Train Loss: 0.6359\n",
      "Epoch 95, Train Loss: 0.6353\n",
      "Epoch 96, Train Loss: 0.6340\n",
      "Epoch 97, Train Loss: 0.6322\n",
      "Epoch 98, Train Loss: 0.6319\n",
      "Epoch 99, Train Loss: 0.6307\n",
      "Epoch 100, Train Loss: 0.6304\n",
      "Epoch 101, Train Loss: 0.6296\n",
      "Epoch 102, Train Loss: 0.6278\n",
      "Epoch 103, Train Loss: 0.6283\n",
      "Epoch 104, Train Loss: 0.6273\n",
      "Epoch 105, Train Loss: 0.6255\n",
      "Epoch 106, Train Loss: 0.6251\n",
      "Epoch 107, Train Loss: 0.6244\n",
      "Epoch 108, Train Loss: 0.6242\n",
      "Epoch 109, Train Loss: 0.6246\n",
      "Epoch 110, Train Loss: 0.6233\n",
      "Epoch 111, Train Loss: 0.6208\n",
      "Epoch 112, Train Loss: 0.6209\n",
      "Epoch 113, Train Loss: 0.6206\n",
      "Epoch 114, Train Loss: 0.6188\n",
      "Epoch 115, Train Loss: 0.6187\n",
      "Epoch 116, Train Loss: 0.6170\n",
      "Epoch 117, Train Loss: 0.6176\n",
      "Epoch 118, Train Loss: 0.6172\n",
      "Epoch 119, Train Loss: 0.6167\n",
      "Epoch 120, Train Loss: 0.6164\n",
      "Epoch 121, Train Loss: 0.6170\n",
      "Epoch 122, Train Loss: 0.6155\n",
      "Epoch 123, Train Loss: 0.6133\n",
      "Epoch 124, Train Loss: 0.6135\n",
      "Epoch 125, Train Loss: 0.6123\n",
      "Epoch 126, Train Loss: 0.6127\n",
      "Epoch 127, Train Loss: 0.6123\n",
      "Epoch 128, Train Loss: 0.6117\n",
      "Epoch 129, Train Loss: 0.6094\n",
      "Epoch 130, Train Loss: 0.6104\n",
      "Epoch 131, Train Loss: 0.6088\n",
      "Epoch 132, Train Loss: 0.6107\n",
      "Epoch 133, Train Loss: 0.6105\n",
      "Epoch 134, Train Loss: 0.6075\n",
      "Epoch 135, Train Loss: 0.6091\n",
      "Epoch 136, Train Loss: 0.6067\n",
      "Epoch 137, Train Loss: 0.6071\n",
      "Epoch 138, Train Loss: 0.6070\n",
      "Epoch 139, Train Loss: 0.6043\n",
      "Epoch 140, Train Loss: 0.6051\n",
      "Epoch 141, Train Loss: 0.6050\n",
      "Epoch 142, Train Loss: 0.6042\n",
      "Epoch 143, Train Loss: 0.6021\n",
      "Epoch 144, Train Loss: 0.6034\n",
      "Epoch 145, Train Loss: 0.6039\n",
      "Epoch 146, Train Loss: 0.6020\n",
      "Epoch 147, Train Loss: 0.6013\n",
      "Epoch 148, Train Loss: 0.6014\n",
      "Epoch 149, Train Loss: 0.6004\n",
      "Epoch 150, Train Loss: 0.6006\n",
      "Epoch 151, Train Loss: 0.6009\n",
      "Epoch 152, Train Loss: 0.6010\n",
      "Epoch 153, Train Loss: 0.5982\n",
      "Epoch 154, Train Loss: 0.5993\n",
      "Epoch 155, Train Loss: 0.5991\n",
      "Epoch 156, Train Loss: 0.5985\n",
      "Epoch 157, Train Loss: 0.5977\n",
      "Epoch 158, Train Loss: 0.5970\n",
      "Epoch 159, Train Loss: 0.5964\n",
      "Epoch 160, Train Loss: 0.5955\n",
      "Epoch 161, Train Loss: 0.5954\n",
      "Epoch 162, Train Loss: 0.5950\n",
      "Epoch 163, Train Loss: 0.5962\n",
      "Epoch 164, Train Loss: 0.5942\n",
      "Epoch 165, Train Loss: 0.5941\n",
      "Epoch 166, Train Loss: 0.5934\n",
      "Validation Accuracy for n=30: 0.4986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [08:25<01:43, 103.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=30: 0.5007\n",
      "Epoch 1, Train Loss: 0.6955\n",
      "Epoch 2, Train Loss: 0.6939\n",
      "Epoch 3, Train Loss: 0.6938\n",
      "Epoch 4, Train Loss: 0.6935\n",
      "Epoch 5, Train Loss: 0.6934\n",
      "Epoch 6, Train Loss: 0.6932\n",
      "Epoch 7, Train Loss: 0.6934\n",
      "Epoch 8, Train Loss: 0.6933\n",
      "Epoch 9, Train Loss: 0.6930\n",
      "Epoch 10, Train Loss: 0.6932\n",
      "Epoch 11, Train Loss: 0.6930\n",
      "Epoch 12, Train Loss: 0.6932\n",
      "Epoch 13, Train Loss: 0.6929\n",
      "Epoch 14, Train Loss: 0.6931\n",
      "Epoch 15, Train Loss: 0.6927\n",
      "Epoch 16, Train Loss: 0.6925\n",
      "Epoch 17, Train Loss: 0.6921\n",
      "Epoch 18, Train Loss: 0.6921\n",
      "Epoch 19, Train Loss: 0.6917\n",
      "Epoch 20, Train Loss: 0.6918\n",
      "Epoch 21, Train Loss: 0.6912\n",
      "Epoch 22, Train Loss: 0.6912\n",
      "Epoch 23, Train Loss: 0.6907\n",
      "Epoch 24, Train Loss: 0.6910\n",
      "Epoch 25, Train Loss: 0.6906\n",
      "Epoch 26, Train Loss: 0.6906\n",
      "Epoch 27, Train Loss: 0.6900\n",
      "Epoch 28, Train Loss: 0.6902\n",
      "Epoch 29, Train Loss: 0.6896\n",
      "Epoch 30, Train Loss: 0.6893\n",
      "Epoch 31, Train Loss: 0.6889\n",
      "Epoch 32, Train Loss: 0.6884\n",
      "Epoch 33, Train Loss: 0.6886\n",
      "Epoch 34, Train Loss: 0.6876\n",
      "Epoch 35, Train Loss: 0.6876\n",
      "Epoch 36, Train Loss: 0.6872\n",
      "Epoch 37, Train Loss: 0.6863\n",
      "Epoch 38, Train Loss: 0.6860\n",
      "Epoch 39, Train Loss: 0.6858\n",
      "Epoch 40, Train Loss: 0.6849\n",
      "Epoch 41, Train Loss: 0.6847\n",
      "Epoch 42, Train Loss: 0.6843\n",
      "Epoch 43, Train Loss: 0.6829\n",
      "Epoch 44, Train Loss: 0.6822\n",
      "Epoch 45, Train Loss: 0.6821\n",
      "Epoch 46, Train Loss: 0.6807\n",
      "Epoch 47, Train Loss: 0.6803\n",
      "Epoch 48, Train Loss: 0.6795\n",
      "Epoch 49, Train Loss: 0.6783\n",
      "Epoch 50, Train Loss: 0.6779\n",
      "Epoch 51, Train Loss: 0.6769\n",
      "Epoch 52, Train Loss: 0.6764\n",
      "Epoch 53, Train Loss: 0.6761\n",
      "Epoch 54, Train Loss: 0.6754\n",
      "Epoch 55, Train Loss: 0.6748\n",
      "Epoch 56, Train Loss: 0.6743\n",
      "Epoch 57, Train Loss: 0.6729\n",
      "Epoch 58, Train Loss: 0.6737\n",
      "Epoch 59, Train Loss: 0.6720\n",
      "Epoch 60, Train Loss: 0.6720\n",
      "Epoch 61, Train Loss: 0.6707\n",
      "Epoch 62, Train Loss: 0.6704\n",
      "Epoch 63, Train Loss: 0.6687\n",
      "Epoch 64, Train Loss: 0.6696\n",
      "Epoch 65, Train Loss: 0.6686\n",
      "Epoch 66, Train Loss: 0.6680\n",
      "Epoch 67, Train Loss: 0.6681\n",
      "Epoch 68, Train Loss: 0.6670\n",
      "Epoch 69, Train Loss: 0.6662\n",
      "Epoch 70, Train Loss: 0.6657\n",
      "Epoch 71, Train Loss: 0.6651\n",
      "Epoch 72, Train Loss: 0.6643\n",
      "Epoch 73, Train Loss: 0.6640\n",
      "Epoch 74, Train Loss: 0.6641\n",
      "Epoch 75, Train Loss: 0.6630\n",
      "Epoch 76, Train Loss: 0.6622\n",
      "Epoch 77, Train Loss: 0.6621\n",
      "Epoch 78, Train Loss: 0.6626\n",
      "Epoch 79, Train Loss: 0.6616\n",
      "Epoch 80, Train Loss: 0.6603\n",
      "Epoch 81, Train Loss: 0.6608\n",
      "Epoch 82, Train Loss: 0.6601\n",
      "Epoch 83, Train Loss: 0.6594\n",
      "Epoch 84, Train Loss: 0.6588\n",
      "Epoch 85, Train Loss: 0.6588\n",
      "Epoch 86, Train Loss: 0.6576\n",
      "Epoch 87, Train Loss: 0.6564\n",
      "Epoch 88, Train Loss: 0.6571\n",
      "Epoch 89, Train Loss: 0.6563\n",
      "Epoch 90, Train Loss: 0.6556\n",
      "Epoch 91, Train Loss: 0.6557\n",
      "Epoch 92, Train Loss: 0.6550\n",
      "Epoch 93, Train Loss: 0.6546\n",
      "Epoch 94, Train Loss: 0.6539\n",
      "Epoch 95, Train Loss: 0.6542\n",
      "Epoch 96, Train Loss: 0.6535\n",
      "Epoch 97, Train Loss: 0.6524\n",
      "Epoch 98, Train Loss: 0.6532\n",
      "Epoch 99, Train Loss: 0.6512\n",
      "Epoch 100, Train Loss: 0.6508\n",
      "Epoch 101, Train Loss: 0.6517\n",
      "Epoch 102, Train Loss: 0.6511\n",
      "Epoch 103, Train Loss: 0.6510\n",
      "Epoch 104, Train Loss: 0.6506\n",
      "Epoch 105, Train Loss: 0.6492\n",
      "Epoch 106, Train Loss: 0.6492\n",
      "Epoch 107, Train Loss: 0.6490\n",
      "Epoch 108, Train Loss: 0.6485\n",
      "Epoch 109, Train Loss: 0.6482\n",
      "Epoch 110, Train Loss: 0.6483\n",
      "Epoch 111, Train Loss: 0.6476\n",
      "Epoch 112, Train Loss: 0.6467\n",
      "Epoch 113, Train Loss: 0.6466\n",
      "Epoch 114, Train Loss: 0.6457\n",
      "Epoch 115, Train Loss: 0.6458\n",
      "Epoch 116, Train Loss: 0.6459\n",
      "Epoch 117, Train Loss: 0.6455\n",
      "Epoch 118, Train Loss: 0.6441\n",
      "Epoch 119, Train Loss: 0.6456\n",
      "Epoch 120, Train Loss: 0.6446\n",
      "Epoch 121, Train Loss: 0.6439\n",
      "Epoch 122, Train Loss: 0.6434\n",
      "Epoch 123, Train Loss: 0.6440\n",
      "Epoch 124, Train Loss: 0.6435\n",
      "Epoch 125, Train Loss: 0.6413\n",
      "Epoch 126, Train Loss: 0.6433\n",
      "Epoch 127, Train Loss: 0.6416\n",
      "Epoch 128, Train Loss: 0.6414\n",
      "Epoch 129, Train Loss: 0.6418\n",
      "Epoch 130, Train Loss: 0.6409\n",
      "Epoch 131, Train Loss: 0.6409\n",
      "Epoch 132, Train Loss: 0.6405\n",
      "Epoch 133, Train Loss: 0.6394\n",
      "Epoch 134, Train Loss: 0.6408\n",
      "Epoch 135, Train Loss: 0.6397\n",
      "Epoch 136, Train Loss: 0.6391\n",
      "Epoch 137, Train Loss: 0.6394\n",
      "Epoch 138, Train Loss: 0.6388\n",
      "Epoch 139, Train Loss: 0.6389\n",
      "Epoch 140, Train Loss: 0.6382\n",
      "Epoch 141, Train Loss: 0.6382\n",
      "Epoch 142, Train Loss: 0.6383\n",
      "Epoch 143, Train Loss: 0.6383\n",
      "Epoch 144, Train Loss: 0.6391\n",
      "Epoch 145, Train Loss: 0.6368\n",
      "Epoch 146, Train Loss: 0.6369\n",
      "Epoch 147, Train Loss: 0.6366\n",
      "Epoch 148, Train Loss: 0.6376\n",
      "Epoch 149, Train Loss: 0.6367\n",
      "Epoch 150, Train Loss: 0.6355\n",
      "Epoch 151, Train Loss: 0.6366\n",
      "Epoch 152, Train Loss: 0.6361\n",
      "Epoch 153, Train Loss: 0.6351\n",
      "Epoch 154, Train Loss: 0.6346\n",
      "Epoch 155, Train Loss: 0.6338\n",
      "Epoch 156, Train Loss: 0.6351\n",
      "Epoch 157, Train Loss: 0.6335\n",
      "Epoch 158, Train Loss: 0.6343\n",
      "Epoch 159, Train Loss: 0.6343\n",
      "Epoch 160, Train Loss: 0.6331\n",
      "Epoch 161, Train Loss: 0.6333\n",
      "Epoch 162, Train Loss: 0.6332\n",
      "Epoch 163, Train Loss: 0.6342\n",
      "Epoch 164, Train Loss: 0.6330\n",
      "Epoch 165, Train Loss: 0.6328\n",
      "Epoch 166, Train Loss: 0.6315\n",
      "Validation Accuracy for n=45: 0.4973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [11:32<00:00, 98.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for n=45: 0.5007\n",
      "Accuracies across different n values: [(9, 0.9416666666666667), (12, 0.9208333333333333), (15, 0.5885555555555556), (18, 0.49916666666666665), (24, 0.5003472222222223), (30, 0.5007222222222222), (45, 0.5007407407407407)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run for each n value and collect accuracies\n",
    "results = []\n",
    "# learning_rate: 0.008705054471236994\n",
    "# batch_size: 17\n",
    "# layer1_units: 92\n",
    "# layer2_units: 52\n",
    "# prior_mu: 0.46796382181206203\n",
    "# prior_sigma: 0.4384207640007753\n",
    "# epochs: 100\n",
    "# Acc: 0.9348\n",
    "\n",
    "# learning_rate: 0.003992368422485689\n",
    "# batch_size: 121\n",
    "# layer1_units: 73\n",
    "# layer2_units: 54\n",
    "# prior_mu: 0.5103986684212475\n",
    "# prior_sigma: 0.29950393862614066\n",
    "# epochs: 81\n",
    "# Accuracies across different n values: [(9, 0.9514814814814815), (12, 0.9186111111111112), (15, 0.866), (18, 0.505462962962963), (24, 0.5030555555555556), (30, 0.49977777777777777), (45, 0.49762962962962964)]\n",
    "\n",
    "# learning_rate: 0.004155371586453587\n",
    "# batch_size: 99\n",
    "# layer1_units: 42\n",
    "# layer2_units: 38\n",
    "# prior_mu: 0.6144697119581359\n",
    "# prior_sigma: 0.5\n",
    "# epochs: 166\n",
    "# Accuracies across different n values: [(9, 0.9416666666666667), (12, 0.9208333333333333), (15, 0.5885555555555556), (18, 0.49916666666666665), (24, 0.5003472222222223), (30, 0.5007222222222222), (45, 0.5007407407407407)]\n",
    "\n",
    "hyper_params = {\n",
    "    'prior_mu': 0.6144697119581359,\n",
    "    'prior_sigma': 0.5,\n",
    "    'layer1_units': 42,\n",
    "    'layer2_units': 38,\n",
    "    'batch_size': 99,\n",
    "    'learning_rate': 0.004155371586453587,\n",
    "    'epochs': 166\n",
    "}\n",
    "hyper_params9 = {\n",
    "    'prior_mu': 0.5103986684212475,\n",
    "    'prior_sigma': 0.29950393862614066,\n",
    "    'layer1_units': 73,\n",
    "    'layer2_units': 54,\n",
    "    'batch_size': 121,\n",
    "    'learning_rate': 0.003992368422485689,\n",
    "    'epochs': 81\n",
    "}\n",
    "possible_n_vals = [9, 12, 15, 18, 24, 30, 45]\n",
    "for n in tqdm(possible_n_vals):\n",
    "    accuracy = run_bayesian_nn(n, hyper_params)\n",
    "    results.append((n, accuracy))\n",
    "\n",
    "print(\"Accuracies across different n values:\", results)\n",
    "\n",
    "# Threshold grid for each n value\n",
    "thresh_grid = {\n",
    "    '9': 0.95,  \n",
    "    '12': 0.925,\n",
    "    '15': 0.90,\n",
    "    '18': 0.875,\n",
    "    '24': 0.80,\n",
    "    '30': 0.75,\n",
    "    '45': 0.70\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
